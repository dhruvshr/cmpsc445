{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMPSC 445 - M7 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# defining preprocessing transformations on mnist dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# loading mnist training data\n",
    "mnist_training_data = datasets.MNIST(\n",
    "    './data/mnist', \n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# loading mnist test data\n",
    "mnist_testing_data = datasets.MNIST(\n",
    "    './data/mnist', \n",
    "    train=False, \n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model\n",
    "\n",
    "Involves:\n",
    "- Initialization\n",
    "- Forward Propagation\n",
    "- Training Function\n",
    "- Testing Function\n",
    "- Model Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    \"\"\"  \n",
    "    Neural network model for classification tasks.\n",
    "\n",
    "    This model consists of two hidden layers with ReLU activation \n",
    "    and an output layer that uses log softmax for multi-class classification.\n",
    "\n",
    "    Attributes:\n",
    "        fc1 (nn.Linear): The first fully connected hidden layer (input: 784, output: 128).\n",
    "        fc2 (nn.Linear): The second fully connected hidden layer (input: 128, output: 64).\n",
    "        final_layer (nn.Linear): The final layer for output (input: 64, output: 10).\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # hidden layers with ReLU activations\n",
    "        # first fully connected hidden layer (input: 784, output: 128)\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        # second fully connected hidden layer (input: 128, output: 64)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        # final layer (output: 10 for classification, assuming 10 classes)\n",
    "        self.final_layer = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten input image (assuming input is of shape [batch_size, 1, 28, 28])\n",
    "        x = x.view(-1, 28*28)\n",
    "\n",
    "        # pass through hidden layers with ReLU activations\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "\n",
    "        # pass through final layer\n",
    "        output = self.final_layer(x)\n",
    "        # applying log softmax to the output (for NLL loss)\n",
    "        output = nn.functional.log_softmax(output, dim=1)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def training(model, train_dataloader, optimizer, print_freq=10):\n",
    "        model.train()   # set model to training mode\n",
    "\n",
    "        train_loss = 0\n",
    "\n",
    "        for batch_index, (data, target) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()   # zero the gradients\n",
    "            output = model(data)    # forward pass through the model\n",
    "\n",
    "            # calculate loss using negative log-likelihood\n",
    "            loss = nn.functional.nll_loss(output, target)\n",
    "\n",
    "            # backpropagation\n",
    "            loss.backward() \n",
    "\n",
    "            # update model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # sum up loss\n",
    "            train_loss += loss.item() * data.shape[0]\n",
    "\n",
    "            # print current training loss at specified intervals\n",
    "            if not (batch_index % print_freq):\n",
    "                print(\n",
    "                    f\"Train Batch {batch_index}/{len(train_dataloader)} Loss: {loss.item():.4f}\"\n",
    "                )\n",
    "\n",
    "        # return average training loss\n",
    "        return train_loss / len(train_dataloader.dataset)\n",
    "    \n",
    "    def testing(model, test_dataloader):\n",
    "        model.eval()    # set model to training mode\n",
    "\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        # no need to compute gradients during evaluation\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_dataloader:\n",
    "                # forward pass through the model\n",
    "                output = model(data)\n",
    "\n",
    "                # calculate loss\n",
    "                test_loss += nn.functional.nll_loss(output, target, reduction='sum').item()\n",
    "\n",
    "                # get predictions by taking argmax of the output (the class with the highest score)\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        # average loss over test dataset\n",
    "        test_loss /= len(test_dataloader.dataset)\n",
    "\n",
    "        # calculate accuracy\n",
    "        test_accuracy = correct / len(test_dataloader.dataset)\n",
    "\n",
    "        return test_loss, test_accuracy\n",
    "    \n",
    "    def train_model(model, train_dataloader, test_dataloader, optimizer, num_epochs):\n",
    "        for i in range(num_epochs):\n",
    "            # train model for one epoch\n",
    "            train_loss = NeuralNet.training(model, train_dataloader, optimizer)\n",
    "\n",
    "            # test model on validation set\n",
    "            test_loss, test_accuracy = NeuralNet.testing(model, test_dataloader)\n",
    "\n",
    "            # print stats\n",
    "            print(\n",
    "                f\"Epoch: {i+1} | Train Loss: {train_loss:.5f} |\",\n",
    "                f\"Test Loss: {test_loss:.5f} | Test Accuracy: {test_accuracy:.5f}\"\n",
    "            )\n",
    "\n",
    "# model = NeuralNet()\n",
    "# print(f\"Model: {model}\")\n",
    "# print(f\"Parameter Sum: {sum([torch.prod(torch.tensor(i.shape)) for i in model.parameters()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model\n",
    "\n",
    "Results Summary:\n",
    "- Model improved steadily, with both training and test losses decreasing and test accuracy rising consistently.\n",
    "The model is generalizing well to the test data, evidenced by a strong performance on the test set.\n",
    "- The absence of signs of overfitting (i.e., test accuracy keeps improving without a significant rise in test loss) suggested a well-balanced model.\n",
    "- The results indicated that the model would be on track and likely to perform well in real-world scenarios.\n",
    "\n",
    "Model Output Training Statistics:\n",
    "| Epoch | Train Loss | Test Loss | Test Accuracy |\n",
    "| :----: | :---------: | :-------- | :------------: |\n",
    "| 1     | 0.32063    | 0.15165   | 0.95537       |\n",
    "| 2     | 0.13151    | 0.09308   | 0.97242       |\n",
    "| 3     | 0.09001    | 0.06999   | 0.97922       |\n",
    "| 4     | 0.06998    | 0.05546   | 0.98257       |\n",
    "| 5     | 0.05499    | 0.03792   | 0.98860       |\n",
    "| 6     | 0.04414    | 0.03082   | 0.99087       |\n",
    "| 7     | 0.03761    | 0.02451   | 0.99242       |\n",
    "| 8     | 0.02950    | 0.02248   | 0.99293       |\n",
    "| 9     | 0.02404    | 0.01487   | 0.99523       |\n",
    "| 10    | 0.02402    | 0.02228   | 0.99182       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader for training data\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    mnist_training_data,\n",
    "    batch_size = 128, \n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "# dataloader for testing data\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    mnist_training_data,\n",
    "    batch_size = 128, \n",
    "    shuffle = False\n",
    ")\n",
    "\n",
    "# running model train function\n",
    "model = NeuralNet()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "NeuralNet.train_model(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    optimizer,\n",
    "    num_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining preprocessing transformations on cifar-10 dataset\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# loading cifar-10 training data\n",
    "cifar10_training_data = datasets.MNIST(\n",
    "    './data/cifar10', \n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# loading cifar-10 test data\n",
    "cifar10_testing_data = datasets.MNIST(\n",
    "    './data/cifar10', \n",
    "    train=False, \n",
    "    transform=transform\n",
    ")``"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
