{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMPSC 445 - M6 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "dataset = pd.read_csv(url, names=names)\n",
    "\n",
    "X = dataset.values[:,0:4].astype(float)\n",
    "Y = dataset.values[:,4].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition Training and Testing Data\n",
    "Training size = 40% of Iris dataset instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Model\n",
    "\n",
    "- Multilayer Neural Network Model\n",
    "- Learning Rate: 0.001\n",
    "- 3 hidden neurons in 1 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_nuerons = 3\n",
    "alpha = 0.001\n",
    "\n",
    "clf = MLPClassifier(\n",
    "    solver = 'sgd', \n",
    "    activation = 'logistic',                 \n",
    "    learning_rate_init = alpha, \n",
    "    learning_rate = 'constant', \n",
    "    max_iter = 1000, \n",
    "    verbose = True,\n",
    "    hidden_layer_sizes = (hidden_nuerons,)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.14659956\n",
      "Iteration 2, loss = 1.14639974\n",
      "Iteration 3, loss = 1.14611531\n",
      "Iteration 4, loss = 1.14575535\n",
      "Iteration 5, loss = 1.14532811\n",
      "Iteration 6, loss = 1.14484112\n",
      "Iteration 7, loss = 1.14430121\n",
      "Iteration 8, loss = 1.14371460\n",
      "Iteration 9, loss = 1.14308690\n",
      "Iteration 10, loss = 1.14242322\n",
      "Iteration 11, loss = 1.14172815\n",
      "Iteration 12, loss = 1.14100583\n",
      "Iteration 13, loss = 1.14026001\n",
      "Iteration 14, loss = 1.13949406\n",
      "Iteration 15, loss = 1.13871099\n",
      "Iteration 16, loss = 1.13791353\n",
      "Iteration 17, loss = 1.13710411\n",
      "Iteration 18, loss = 1.13628491\n",
      "Iteration 19, loss = 1.13545788\n",
      "Iteration 20, loss = 1.13462475\n",
      "Iteration 21, loss = 1.13378708\n",
      "Iteration 22, loss = 1.13294624\n",
      "Iteration 23, loss = 1.13210347\n",
      "Iteration 24, loss = 1.13125984\n",
      "Iteration 25, loss = 1.13041631\n",
      "Iteration 26, loss = 1.12957373\n",
      "Iteration 27, loss = 1.12873285\n",
      "Iteration 28, loss = 1.12789430\n",
      "Iteration 29, loss = 1.12705867\n",
      "Iteration 30, loss = 1.12622645\n",
      "Iteration 31, loss = 1.12539806\n",
      "Iteration 32, loss = 1.12457388\n",
      "Iteration 33, loss = 1.12375421\n",
      "Iteration 34, loss = 1.12293933\n",
      "Iteration 35, loss = 1.12212946\n",
      "Iteration 36, loss = 1.12132478\n",
      "Iteration 37, loss = 1.12052544\n",
      "Iteration 38, loss = 1.11973156\n",
      "Iteration 39, loss = 1.11894324\n",
      "Iteration 40, loss = 1.11816055\n",
      "Iteration 41, loss = 1.11738352\n",
      "Iteration 42, loss = 1.11661220\n",
      "Iteration 43, loss = 1.11584659\n",
      "Iteration 44, loss = 1.11508668\n",
      "Iteration 45, loss = 1.11433246\n",
      "Iteration 46, loss = 1.11358390\n",
      "Iteration 47, loss = 1.11284097\n",
      "Iteration 48, loss = 1.11210361\n",
      "Iteration 49, loss = 1.11137178\n",
      "Iteration 50, loss = 1.11064540\n",
      "Iteration 51, loss = 1.10992442\n",
      "Iteration 52, loss = 1.10920877\n",
      "Iteration 53, loss = 1.10849836\n",
      "Iteration 54, loss = 1.10779312\n",
      "Iteration 55, loss = 1.10709298\n",
      "Iteration 56, loss = 1.10639784\n",
      "Iteration 57, loss = 1.10570763\n",
      "Iteration 58, loss = 1.10502227\n",
      "Iteration 59, loss = 1.10434166\n",
      "Iteration 60, loss = 1.10366572\n",
      "Iteration 61, loss = 1.10299438\n",
      "Iteration 62, loss = 1.10232753\n",
      "Iteration 63, loss = 1.10166511\n",
      "Iteration 64, loss = 1.10100703\n",
      "Iteration 65, loss = 1.10035321\n",
      "Iteration 66, loss = 1.09970358\n",
      "Iteration 67, loss = 1.09905804\n",
      "Iteration 68, loss = 1.09841654\n",
      "Iteration 69, loss = 1.09777899\n",
      "Iteration 70, loss = 1.09714533\n",
      "Iteration 71, loss = 1.09651549\n",
      "Iteration 72, loss = 1.09588939\n",
      "Iteration 73, loss = 1.09526699\n",
      "Iteration 74, loss = 1.09464821\n",
      "Iteration 75, loss = 1.09403301\n",
      "Iteration 76, loss = 1.09342131\n",
      "Iteration 77, loss = 1.09281308\n",
      "Iteration 78, loss = 1.09220826\n",
      "Iteration 79, loss = 1.09160680\n",
      "Iteration 80, loss = 1.09100867\n",
      "Iteration 81, loss = 1.09041381\n",
      "Iteration 82, loss = 1.08982220\n",
      "Iteration 83, loss = 1.08923379\n",
      "Iteration 84, loss = 1.08864855\n",
      "Iteration 85, loss = 1.08806646\n",
      "Iteration 86, loss = 1.08748748\n",
      "Iteration 87, loss = 1.08691160\n",
      "Iteration 88, loss = 1.08633878\n",
      "Iteration 89, loss = 1.08576903\n",
      "Iteration 90, loss = 1.08520231\n",
      "Iteration 91, loss = 1.08463861\n",
      "Iteration 92, loss = 1.08407793\n",
      "Iteration 93, loss = 1.08352025\n",
      "Iteration 94, loss = 1.08296558\n",
      "Iteration 95, loss = 1.08241390\n",
      "Iteration 96, loss = 1.08186522\n",
      "Iteration 97, loss = 1.08131954\n",
      "Iteration 98, loss = 1.08077686\n",
      "Iteration 99, loss = 1.08023718\n",
      "Iteration 100, loss = 1.07970052\n",
      "Iteration 101, loss = 1.07916689\n",
      "Iteration 102, loss = 1.07863628\n",
      "Iteration 103, loss = 1.07810873\n",
      "Iteration 104, loss = 1.07758423\n",
      "Iteration 105, loss = 1.07706281\n",
      "Iteration 106, loss = 1.07654447\n",
      "Iteration 107, loss = 1.07602925\n",
      "Iteration 108, loss = 1.07551716\n",
      "Iteration 109, loss = 1.07500821\n",
      "Iteration 110, loss = 1.07450242\n",
      "Iteration 111, loss = 1.07399983\n",
      "Iteration 112, loss = 1.07350044\n",
      "Iteration 113, loss = 1.07300428\n",
      "Iteration 114, loss = 1.07251136\n",
      "Iteration 115, loss = 1.07202172\n",
      "Iteration 116, loss = 1.07153537\n",
      "Iteration 117, loss = 1.07105234\n",
      "Iteration 118, loss = 1.07057264\n",
      "Iteration 119, loss = 1.07009629\n",
      "Iteration 120, loss = 1.06962332\n",
      "Iteration 121, loss = 1.06915374\n",
      "Iteration 122, loss = 1.06868758\n",
      "Iteration 123, loss = 1.06822484\n",
      "Iteration 124, loss = 1.06776555\n",
      "Iteration 125, loss = 1.06730972\n",
      "Iteration 126, loss = 1.06685737\n",
      "Iteration 127, loss = 1.06640851\n",
      "Iteration 128, loss = 1.06596315\n",
      "Iteration 129, loss = 1.06552130\n",
      "Iteration 130, loss = 1.06508298\n",
      "Iteration 131, loss = 1.06464818\n",
      "Iteration 132, loss = 1.06421691\n",
      "Iteration 133, loss = 1.06378919\n",
      "Iteration 134, loss = 1.06336500\n",
      "Iteration 135, loss = 1.06294436\n",
      "Iteration 136, loss = 1.06252726\n",
      "Iteration 137, loss = 1.06211371\n",
      "Iteration 138, loss = 1.06170368\n",
      "Iteration 139, loss = 1.06129719\n",
      "Iteration 140, loss = 1.06089422\n",
      "Iteration 141, loss = 1.06049476\n",
      "Iteration 142, loss = 1.06009880\n",
      "Iteration 143, loss = 1.05970633\n",
      "Iteration 144, loss = 1.05931733\n",
      "Iteration 145, loss = 1.05893178\n",
      "Iteration 146, loss = 1.05854968\n",
      "Iteration 147, loss = 1.05817099\n",
      "Iteration 148, loss = 1.05779570\n",
      "Iteration 149, loss = 1.05742379\n",
      "Iteration 150, loss = 1.05705522\n",
      "Iteration 151, loss = 1.05668997\n",
      "Iteration 152, loss = 1.05632802\n",
      "Iteration 153, loss = 1.05596933\n",
      "Iteration 154, loss = 1.05561387\n",
      "Iteration 155, loss = 1.05526162\n",
      "Iteration 156, loss = 1.05491253\n",
      "Iteration 157, loss = 1.05456657\n",
      "Iteration 158, loss = 1.05422371\n",
      "Iteration 159, loss = 1.05388391\n",
      "Iteration 160, loss = 1.05354714\n",
      "Iteration 161, loss = 1.05321334\n",
      "Iteration 162, loss = 1.05288249\n",
      "Iteration 163, loss = 1.05255454\n",
      "Iteration 164, loss = 1.05222944\n",
      "Iteration 165, loss = 1.05190717\n",
      "Iteration 166, loss = 1.05158766\n",
      "Iteration 167, loss = 1.05127089\n",
      "Iteration 168, loss = 1.05095680\n",
      "Iteration 169, loss = 1.05064535\n",
      "Iteration 170, loss = 1.05033649\n",
      "Iteration 171, loss = 1.05003018\n",
      "Iteration 172, loss = 1.04972638\n",
      "Iteration 173, loss = 1.04942502\n",
      "Iteration 174, loss = 1.04912608\n",
      "Iteration 175, loss = 1.04882949\n",
      "Iteration 176, loss = 1.04853522\n",
      "Iteration 177, loss = 1.04824321\n",
      "Iteration 178, loss = 1.04795342\n",
      "Iteration 179, loss = 1.04766581\n",
      "Iteration 180, loss = 1.04738031\n",
      "Iteration 181, loss = 1.04709689\n",
      "Iteration 182, loss = 1.04681550\n",
      "Iteration 183, loss = 1.04653609\n",
      "Iteration 184, loss = 1.04625861\n",
      "Iteration 185, loss = 1.04598302\n",
      "Iteration 186, loss = 1.04570927\n",
      "Iteration 187, loss = 1.04543732\n",
      "Iteration 188, loss = 1.04516712\n",
      "Iteration 189, loss = 1.04489861\n",
      "Iteration 190, loss = 1.04463177\n",
      "Iteration 191, loss = 1.04436654\n",
      "Iteration 192, loss = 1.04410288\n",
      "Iteration 193, loss = 1.04384074\n",
      "Iteration 194, loss = 1.04358009\n",
      "Iteration 195, loss = 1.04332087\n",
      "Iteration 196, loss = 1.04306305\n",
      "Iteration 197, loss = 1.04280659\n",
      "Iteration 198, loss = 1.04255143\n",
      "Iteration 199, loss = 1.04229755\n",
      "Iteration 200, loss = 1.04204490\n",
      "Iteration 201, loss = 1.04179344\n",
      "Iteration 202, loss = 1.04154314\n",
      "Iteration 203, loss = 1.04129395\n",
      "Iteration 204, loss = 1.04104583\n",
      "Iteration 205, loss = 1.04079876\n",
      "Iteration 206, loss = 1.04055268\n",
      "Iteration 207, loss = 1.04030757\n",
      "Iteration 208, loss = 1.04006340\n",
      "Iteration 209, loss = 1.03982011\n",
      "Iteration 210, loss = 1.03957769\n",
      "Iteration 211, loss = 1.03933609\n",
      "Iteration 212, loss = 1.03909529\n",
      "Iteration 213, loss = 1.03885525\n",
      "Iteration 214, loss = 1.03861594\n",
      "Iteration 215, loss = 1.03837733\n",
      "Iteration 216, loss = 1.03813938\n",
      "Iteration 217, loss = 1.03790207\n",
      "Iteration 218, loss = 1.03766537\n",
      "Iteration 219, loss = 1.03742925\n",
      "Iteration 220, loss = 1.03719368\n",
      "Iteration 221, loss = 1.03695863\n",
      "Iteration 222, loss = 1.03672407\n",
      "Iteration 223, loss = 1.03648999\n",
      "Iteration 224, loss = 1.03625635\n",
      "Iteration 225, loss = 1.03602312\n",
      "Iteration 226, loss = 1.03579029\n",
      "Iteration 227, loss = 1.03555783\n",
      "Iteration 228, loss = 1.03532571\n",
      "Iteration 229, loss = 1.03509391\n",
      "Iteration 230, loss = 1.03486241\n",
      "Iteration 231, loss = 1.03463119\n",
      "Iteration 232, loss = 1.03440022\n",
      "Iteration 233, loss = 1.03416949\n",
      "Iteration 234, loss = 1.03393896\n",
      "Iteration 235, loss = 1.03370863\n",
      "Iteration 236, loss = 1.03347848\n",
      "Iteration 237, loss = 1.03324847\n",
      "Iteration 238, loss = 1.03301860\n",
      "Iteration 239, loss = 1.03278885\n",
      "Iteration 240, loss = 1.03255919\n",
      "Iteration 241, loss = 1.03232962\n",
      "Iteration 242, loss = 1.03210011\n",
      "Iteration 243, loss = 1.03187064\n",
      "Iteration 244, loss = 1.03164121\n",
      "Iteration 245, loss = 1.03141179\n",
      "Iteration 246, loss = 1.03118237\n",
      "Iteration 247, loss = 1.03095294\n",
      "Iteration 248, loss = 1.03072348\n",
      "Iteration 249, loss = 1.03049397\n",
      "Iteration 250, loss = 1.03026441\n",
      "Iteration 251, loss = 1.03003477\n",
      "Iteration 252, loss = 1.02980505\n",
      "Iteration 253, loss = 1.02957523\n",
      "Iteration 254, loss = 1.02934530\n",
      "Iteration 255, loss = 1.02911525\n",
      "Iteration 256, loss = 1.02888506\n",
      "Iteration 257, loss = 1.02865473\n",
      "Iteration 258, loss = 1.02842425\n",
      "Iteration 259, loss = 1.02819359\n",
      "Iteration 260, loss = 1.02796276\n",
      "Iteration 261, loss = 1.02773174\n",
      "Iteration 262, loss = 1.02750052\n",
      "Iteration 263, loss = 1.02726909\n",
      "Iteration 264, loss = 1.02703744\n",
      "Iteration 265, loss = 1.02680557\n",
      "Iteration 266, loss = 1.02657346\n",
      "Iteration 267, loss = 1.02634111\n",
      "Iteration 268, loss = 1.02610850\n",
      "Iteration 269, loss = 1.02587563\n",
      "Iteration 270, loss = 1.02564249\n",
      "Iteration 271, loss = 1.02540908\n",
      "Iteration 272, loss = 1.02517538\n",
      "Iteration 273, loss = 1.02494138\n",
      "Iteration 274, loss = 1.02470709\n",
      "Iteration 275, loss = 1.02447249\n",
      "Iteration 276, loss = 1.02423758\n",
      "Iteration 277, loss = 1.02400235\n",
      "Iteration 278, loss = 1.02376679\n",
      "Iteration 279, loss = 1.02353090\n",
      "Iteration 280, loss = 1.02329467\n",
      "Iteration 281, loss = 1.02305810\n",
      "Iteration 282, loss = 1.02282117\n",
      "Iteration 283, loss = 1.02258389\n",
      "Iteration 284, loss = 1.02234626\n",
      "Iteration 285, loss = 1.02210825\n",
      "Iteration 286, loss = 1.02186987\n",
      "Iteration 287, loss = 1.02163112\n",
      "Iteration 288, loss = 1.02139199\n",
      "Iteration 289, loss = 1.02115248\n",
      "Iteration 290, loss = 1.02091257\n",
      "Iteration 291, loss = 1.02067227\n",
      "Iteration 292, loss = 1.02043157\n",
      "Iteration 293, loss = 1.02019047\n",
      "Iteration 294, loss = 1.01994897\n",
      "Iteration 295, loss = 1.01970705\n",
      "Iteration 296, loss = 1.01946472\n",
      "Iteration 297, loss = 1.01922197\n",
      "Iteration 298, loss = 1.01897881\n",
      "Iteration 299, loss = 1.01873522\n",
      "Iteration 300, loss = 1.01849120\n",
      "Iteration 301, loss = 1.01824675\n",
      "Iteration 302, loss = 1.01800187\n",
      "Iteration 303, loss = 1.01775656\n",
      "Iteration 304, loss = 1.01751080\n",
      "Iteration 305, loss = 1.01726460\n",
      "Iteration 306, loss = 1.01701796\n",
      "Iteration 307, loss = 1.01677087\n",
      "Iteration 308, loss = 1.01652333\n",
      "Iteration 309, loss = 1.01627534\n",
      "Iteration 310, loss = 1.01602690\n",
      "Iteration 311, loss = 1.01577800\n",
      "Iteration 312, loss = 1.01552864\n",
      "Iteration 313, loss = 1.01527882\n",
      "Iteration 314, loss = 1.01502854\n",
      "Iteration 315, loss = 1.01477779\n",
      "Iteration 316, loss = 1.01452657\n",
      "Iteration 317, loss = 1.01427489\n",
      "Iteration 318, loss = 1.01402274\n",
      "Iteration 319, loss = 1.01377011\n",
      "Iteration 320, loss = 1.01351701\n",
      "Iteration 321, loss = 1.01326344\n",
      "Iteration 322, loss = 1.01300939\n",
      "Iteration 323, loss = 1.01275486\n",
      "Iteration 324, loss = 1.01249985\n",
      "Iteration 325, loss = 1.01224436\n",
      "Iteration 326, loss = 1.01198839\n",
      "Iteration 327, loss = 1.01173194\n",
      "Iteration 328, loss = 1.01147500\n",
      "Iteration 329, loss = 1.01121758\n",
      "Iteration 330, loss = 1.01095966\n",
      "Iteration 331, loss = 1.01070127\n",
      "Iteration 332, loss = 1.01044238\n",
      "Iteration 333, loss = 1.01018300\n",
      "Iteration 334, loss = 1.00992313\n",
      "Iteration 335, loss = 1.00966277\n",
      "Iteration 336, loss = 1.00940192\n",
      "Iteration 337, loss = 1.00914057\n",
      "Iteration 338, loss = 1.00887873\n",
      "Iteration 339, loss = 1.00861640\n",
      "Iteration 340, loss = 1.00835357\n",
      "Iteration 341, loss = 1.00809024\n",
      "Iteration 342, loss = 1.00782642\n",
      "Iteration 343, loss = 1.00756210\n",
      "Iteration 344, loss = 1.00729728\n",
      "Iteration 345, loss = 1.00703197\n",
      "Iteration 346, loss = 1.00676615\n",
      "Iteration 347, loss = 1.00649984\n",
      "Iteration 348, loss = 1.00623302\n",
      "Iteration 349, loss = 1.00596571\n",
      "Iteration 350, loss = 1.00569790\n",
      "Iteration 351, loss = 1.00542958\n",
      "Iteration 352, loss = 1.00516077\n",
      "Iteration 353, loss = 1.00489145\n",
      "Iteration 354, loss = 1.00462163\n",
      "Iteration 355, loss = 1.00435131\n",
      "Iteration 356, loss = 1.00408049\n",
      "Iteration 357, loss = 1.00380916\n",
      "Iteration 358, loss = 1.00353733\n",
      "Iteration 359, loss = 1.00326501\n",
      "Iteration 360, loss = 1.00299217\n",
      "Iteration 361, loss = 1.00271884\n",
      "Iteration 362, loss = 1.00244500\n",
      "Iteration 363, loss = 1.00217066\n",
      "Iteration 364, loss = 1.00189582\n",
      "Iteration 365, loss = 1.00162047\n",
      "Iteration 366, loss = 1.00134462\n",
      "Iteration 367, loss = 1.00106827\n",
      "Iteration 368, loss = 1.00079142\n",
      "Iteration 369, loss = 1.00051406\n",
      "Iteration 370, loss = 1.00023621\n",
      "Iteration 371, loss = 0.99995784\n",
      "Iteration 372, loss = 0.99967898\n",
      "Iteration 373, loss = 0.99939962\n",
      "Iteration 374, loss = 0.99911975\n",
      "Iteration 375, loss = 0.99883938\n",
      "Iteration 376, loss = 0.99855851\n",
      "Iteration 377, loss = 0.99827714\n",
      "Iteration 378, loss = 0.99799527\n",
      "Iteration 379, loss = 0.99771290\n",
      "Iteration 380, loss = 0.99743003\n",
      "Iteration 381, loss = 0.99714665\n",
      "Iteration 382, loss = 0.99686278\n",
      "Iteration 383, loss = 0.99657841\n",
      "Iteration 384, loss = 0.99629354\n",
      "Iteration 385, loss = 0.99600817\n",
      "Iteration 386, loss = 0.99572230\n",
      "Iteration 387, loss = 0.99543594\n",
      "Iteration 388, loss = 0.99514907\n",
      "Iteration 389, loss = 0.99486171\n",
      "Iteration 390, loss = 0.99457386\n",
      "Iteration 391, loss = 0.99428551\n",
      "Iteration 392, loss = 0.99399666\n",
      "Iteration 393, loss = 0.99370731\n",
      "Iteration 394, loss = 0.99341748\n",
      "Iteration 395, loss = 0.99312714\n",
      "Iteration 396, loss = 0.99283632\n",
      "Iteration 397, loss = 0.99254500\n",
      "Iteration 398, loss = 0.99225319\n",
      "Iteration 399, loss = 0.99196088\n",
      "Iteration 400, loss = 0.99166809\n",
      "Iteration 401, loss = 0.99137480\n",
      "Iteration 402, loss = 0.99108103\n",
      "Iteration 403, loss = 0.99078676\n",
      "Iteration 404, loss = 0.99049201\n",
      "Iteration 405, loss = 0.99019676\n",
      "Iteration 406, loss = 0.98990103\n",
      "Iteration 407, loss = 0.98960481\n",
      "Iteration 408, loss = 0.98930811\n",
      "Iteration 409, loss = 0.98901092\n",
      "Iteration 410, loss = 0.98871325\n",
      "Iteration 411, loss = 0.98841509\n",
      "Iteration 412, loss = 0.98811644\n",
      "Iteration 413, loss = 0.98781732\n",
      "Iteration 414, loss = 0.98751771\n",
      "Iteration 415, loss = 0.98721762\n",
      "Iteration 416, loss = 0.98691706\n",
      "Iteration 417, loss = 0.98661601\n",
      "Iteration 418, loss = 0.98631448\n",
      "Iteration 419, loss = 0.98601247\n",
      "Iteration 420, loss = 0.98570999\n",
      "Iteration 421, loss = 0.98540703\n",
      "Iteration 422, loss = 0.98510360\n",
      "Iteration 423, loss = 0.98479969\n",
      "Iteration 424, loss = 0.98449531\n",
      "Iteration 425, loss = 0.98419045\n",
      "Iteration 426, loss = 0.98388512\n",
      "Iteration 427, loss = 0.98357932\n",
      "Iteration 428, loss = 0.98327305\n",
      "Iteration 429, loss = 0.98296631\n",
      "Iteration 430, loss = 0.98265911\n",
      "Iteration 431, loss = 0.98235143\n",
      "Iteration 432, loss = 0.98204329\n",
      "Iteration 433, loss = 0.98173468\n",
      "Iteration 434, loss = 0.98142561\n",
      "Iteration 435, loss = 0.98111608\n",
      "Iteration 436, loss = 0.98080608\n",
      "Iteration 437, loss = 0.98049562\n",
      "Iteration 438, loss = 0.98018470\n",
      "Iteration 439, loss = 0.97987332\n",
      "Iteration 440, loss = 0.97956149\n",
      "Iteration 441, loss = 0.97924919\n",
      "Iteration 442, loss = 0.97893644\n",
      "Iteration 443, loss = 0.97862323\n",
      "Iteration 444, loss = 0.97830957\n",
      "Iteration 445, loss = 0.97799546\n",
      "Iteration 446, loss = 0.97768089\n",
      "Iteration 447, loss = 0.97736587\n",
      "Iteration 448, loss = 0.97705040\n",
      "Iteration 449, loss = 0.97673449\n",
      "Iteration 450, loss = 0.97641812\n",
      "Iteration 451, loss = 0.97610131\n",
      "Iteration 452, loss = 0.97578406\n",
      "Iteration 453, loss = 0.97546636\n",
      "Iteration 454, loss = 0.97514822\n",
      "Iteration 455, loss = 0.97482963\n",
      "Iteration 456, loss = 0.97451061\n",
      "Iteration 457, loss = 0.97419114\n",
      "Iteration 458, loss = 0.97387124\n",
      "Iteration 459, loss = 0.97355090\n",
      "Iteration 460, loss = 0.97323012\n",
      "Iteration 461, loss = 0.97290891\n",
      "Iteration 462, loss = 0.97258727\n",
      "Iteration 463, loss = 0.97226519\n",
      "Iteration 464, loss = 0.97194269\n",
      "Iteration 465, loss = 0.97161975\n",
      "Iteration 466, loss = 0.97129639\n",
      "Iteration 467, loss = 0.97097260\n",
      "Iteration 468, loss = 0.97064838\n",
      "Iteration 469, loss = 0.97032374\n",
      "Iteration 470, loss = 0.96999867\n",
      "Iteration 471, loss = 0.96967319\n",
      "Iteration 472, loss = 0.96934728\n",
      "Iteration 473, loss = 0.96902096\n",
      "Iteration 474, loss = 0.96869421\n",
      "Iteration 475, loss = 0.96836705\n",
      "Iteration 476, loss = 0.96803948\n",
      "Iteration 477, loss = 0.96771149\n",
      "Iteration 478, loss = 0.96738309\n",
      "Iteration 479, loss = 0.96705427\n",
      "Iteration 480, loss = 0.96672505\n",
      "Iteration 481, loss = 0.96639542\n",
      "Iteration 482, loss = 0.96606539\n",
      "Iteration 483, loss = 0.96573494\n",
      "Iteration 484, loss = 0.96540410\n",
      "Iteration 485, loss = 0.96507285\n",
      "Iteration 486, loss = 0.96474120\n",
      "Iteration 487, loss = 0.96440915\n",
      "Iteration 488, loss = 0.96407670\n",
      "Iteration 489, loss = 0.96374386\n",
      "Iteration 490, loss = 0.96341062\n",
      "Iteration 491, loss = 0.96307698\n",
      "Iteration 492, loss = 0.96274295\n",
      "Iteration 493, loss = 0.96240854\n",
      "Iteration 494, loss = 0.96207373\n",
      "Iteration 495, loss = 0.96173853\n",
      "Iteration 496, loss = 0.96140295\n",
      "Iteration 497, loss = 0.96106699\n",
      "Iteration 498, loss = 0.96073064\n",
      "Iteration 499, loss = 0.96039390\n",
      "Iteration 500, loss = 0.96005679\n",
      "Iteration 501, loss = 0.95971930\n",
      "Iteration 502, loss = 0.95938143\n",
      "Iteration 503, loss = 0.95904319\n",
      "Iteration 504, loss = 0.95870457\n",
      "Iteration 505, loss = 0.95836557\n",
      "Iteration 506, loss = 0.95802621\n",
      "Iteration 507, loss = 0.95768648\n",
      "Iteration 508, loss = 0.95734638\n",
      "Iteration 509, loss = 0.95700591\n",
      "Iteration 510, loss = 0.95666507\n",
      "Iteration 511, loss = 0.95632388\n",
      "Iteration 512, loss = 0.95598232\n",
      "Iteration 513, loss = 0.95564040\n",
      "Iteration 514, loss = 0.95529812\n",
      "Iteration 515, loss = 0.95495549\n",
      "Iteration 516, loss = 0.95461250\n",
      "Iteration 517, loss = 0.95426915\n",
      "Iteration 518, loss = 0.95392546\n",
      "Iteration 519, loss = 0.95358141\n",
      "Iteration 520, loss = 0.95323701\n",
      "Iteration 521, loss = 0.95289227\n",
      "Iteration 522, loss = 0.95254718\n",
      "Iteration 523, loss = 0.95220174\n",
      "Iteration 524, loss = 0.95185597\n",
      "Iteration 525, loss = 0.95150985\n",
      "Iteration 526, loss = 0.95116339\n",
      "Iteration 527, loss = 0.95081660\n",
      "Iteration 528, loss = 0.95046947\n",
      "Iteration 529, loss = 0.95012200\n",
      "Iteration 530, loss = 0.94977420\n",
      "Iteration 531, loss = 0.94942607\n",
      "Iteration 532, loss = 0.94907761\n",
      "Iteration 533, loss = 0.94872883\n",
      "Iteration 534, loss = 0.94837972\n",
      "Iteration 535, loss = 0.94803028\n",
      "Iteration 536, loss = 0.94768052\n",
      "Iteration 537, loss = 0.94733044\n",
      "Iteration 538, loss = 0.94698004\n",
      "Iteration 539, loss = 0.94662932\n",
      "Iteration 540, loss = 0.94627828\n",
      "Iteration 541, loss = 0.94592693\n",
      "Iteration 542, loss = 0.94557527\n",
      "Iteration 543, loss = 0.94522330\n",
      "Iteration 544, loss = 0.94487102\n",
      "Iteration 545, loss = 0.94451843\n",
      "Iteration 546, loss = 0.94416553\n",
      "Iteration 547, loss = 0.94381233\n",
      "Iteration 548, loss = 0.94345883\n",
      "Iteration 549, loss = 0.94310503\n",
      "Iteration 550, loss = 0.94275092\n",
      "Iteration 551, loss = 0.94239652\n",
      "Iteration 552, loss = 0.94204183\n",
      "Iteration 553, loss = 0.94168684\n",
      "Iteration 554, loss = 0.94133156\n",
      "Iteration 555, loss = 0.94097598\n",
      "Iteration 556, loss = 0.94062012\n",
      "Iteration 557, loss = 0.94026397\n",
      "Iteration 558, loss = 0.93990754\n",
      "Iteration 559, loss = 0.93955082\n",
      "Iteration 560, loss = 0.93919382\n",
      "Iteration 561, loss = 0.93883654\n",
      "Iteration 562, loss = 0.93847898\n",
      "Iteration 563, loss = 0.93812114\n",
      "Iteration 564, loss = 0.93776303\n",
      "Iteration 565, loss = 0.93740465\n",
      "Iteration 566, loss = 0.93704599\n",
      "Iteration 567, loss = 0.93668707\n",
      "Iteration 568, loss = 0.93632787\n",
      "Iteration 569, loss = 0.93596841\n",
      "Iteration 570, loss = 0.93560869\n",
      "Iteration 571, loss = 0.93524870\n",
      "Iteration 572, loss = 0.93488845\n",
      "Iteration 573, loss = 0.93452794\n",
      "Iteration 574, loss = 0.93416717\n",
      "Iteration 575, loss = 0.93380615\n",
      "Iteration 576, loss = 0.93344487\n",
      "Iteration 577, loss = 0.93308334\n",
      "Iteration 578, loss = 0.93272156\n",
      "Iteration 579, loss = 0.93235953\n",
      "Iteration 580, loss = 0.93199725\n",
      "Iteration 581, loss = 0.93163473\n",
      "Iteration 582, loss = 0.93127196\n",
      "Iteration 583, loss = 0.93090895\n",
      "Iteration 584, loss = 0.93054570\n",
      "Iteration 585, loss = 0.93018221\n",
      "Iteration 586, loss = 0.92981849\n",
      "Iteration 587, loss = 0.92945453\n",
      "Iteration 588, loss = 0.92909033\n",
      "Iteration 589, loss = 0.92872591\n",
      "Iteration 590, loss = 0.92836125\n",
      "Iteration 591, loss = 0.92799637\n",
      "Iteration 592, loss = 0.92763126\n",
      "Iteration 593, loss = 0.92726592\n",
      "Iteration 594, loss = 0.92690036\n",
      "Iteration 595, loss = 0.92653458\n",
      "Iteration 596, loss = 0.92616858\n",
      "Iteration 597, loss = 0.92580236\n",
      "Iteration 598, loss = 0.92543593\n",
      "Iteration 599, loss = 0.92506928\n",
      "Iteration 600, loss = 0.92470242\n",
      "Iteration 601, loss = 0.92433535\n",
      "Iteration 602, loss = 0.92396807\n",
      "Iteration 603, loss = 0.92360058\n",
      "Iteration 604, loss = 0.92323289\n",
      "Iteration 605, loss = 0.92286499\n",
      "Iteration 606, loss = 0.92249689\n",
      "Iteration 607, loss = 0.92212859\n",
      "Iteration 608, loss = 0.92176009\n",
      "Iteration 609, loss = 0.92139139\n",
      "Iteration 610, loss = 0.92102250\n",
      "Iteration 611, loss = 0.92065341\n",
      "Iteration 612, loss = 0.92028414\n",
      "Iteration 613, loss = 0.91991467\n",
      "Iteration 614, loss = 0.91954501\n",
      "Iteration 615, loss = 0.91917517\n",
      "Iteration 616, loss = 0.91880514\n",
      "Iteration 617, loss = 0.91843493\n",
      "Iteration 618, loss = 0.91806454\n",
      "Iteration 619, loss = 0.91769397\n",
      "Iteration 620, loss = 0.91732322\n",
      "Iteration 621, loss = 0.91695229\n",
      "Iteration 622, loss = 0.91658119\n",
      "Iteration 623, loss = 0.91620992\n",
      "Iteration 624, loss = 0.91583847\n",
      "Iteration 625, loss = 0.91546686\n",
      "Iteration 626, loss = 0.91509508\n",
      "Iteration 627, loss = 0.91472313\n",
      "Iteration 628, loss = 0.91435102\n",
      "Iteration 629, loss = 0.91397874\n",
      "Iteration 630, loss = 0.91360630\n",
      "Iteration 631, loss = 0.91323371\n",
      "Iteration 632, loss = 0.91286096\n",
      "Iteration 633, loss = 0.91248805\n",
      "Iteration 634, loss = 0.91211499\n",
      "Iteration 635, loss = 0.91174177\n",
      "Iteration 636, loss = 0.91136841\n",
      "Iteration 637, loss = 0.91099489\n",
      "Iteration 638, loss = 0.91062123\n",
      "Iteration 639, loss = 0.91024743\n",
      "Iteration 640, loss = 0.90987348\n",
      "Iteration 641, loss = 0.90949938\n",
      "Iteration 642, loss = 0.90912515\n",
      "Iteration 643, loss = 0.90875078\n",
      "Iteration 644, loss = 0.90837627\n",
      "Iteration 645, loss = 0.90800163\n",
      "Iteration 646, loss = 0.90762685\n",
      "Iteration 647, loss = 0.90725194\n",
      "Iteration 648, loss = 0.90687690\n",
      "Iteration 649, loss = 0.90650173\n",
      "Iteration 650, loss = 0.90612644\n",
      "Iteration 651, loss = 0.90575102\n",
      "Iteration 652, loss = 0.90537548\n",
      "Iteration 653, loss = 0.90499981\n",
      "Iteration 654, loss = 0.90462402\n",
      "Iteration 655, loss = 0.90424812\n",
      "Iteration 656, loss = 0.90387210\n",
      "Iteration 657, loss = 0.90349596\n",
      "Iteration 658, loss = 0.90311972\n",
      "Iteration 659, loss = 0.90274336\n",
      "Iteration 660, loss = 0.90236689\n",
      "Iteration 661, loss = 0.90199031\n",
      "Iteration 662, loss = 0.90161362\n",
      "Iteration 663, loss = 0.90123683\n",
      "Iteration 664, loss = 0.90085994\n",
      "Iteration 665, loss = 0.90048294\n",
      "Iteration 666, loss = 0.90010584\n",
      "Iteration 667, loss = 0.89972865\n",
      "Iteration 668, loss = 0.89935136\n",
      "Iteration 669, loss = 0.89897397\n",
      "Iteration 670, loss = 0.89859650\n",
      "Iteration 671, loss = 0.89821893\n",
      "Iteration 672, loss = 0.89784126\n",
      "Iteration 673, loss = 0.89746352\n",
      "Iteration 674, loss = 0.89708568\n",
      "Iteration 675, loss = 0.89670776\n",
      "Iteration 676, loss = 0.89632976\n",
      "Iteration 677, loss = 0.89595167\n",
      "Iteration 678, loss = 0.89557350\n",
      "Iteration 679, loss = 0.89519526\n",
      "Iteration 680, loss = 0.89481694\n",
      "Iteration 681, loss = 0.89443854\n",
      "Iteration 682, loss = 0.89406007\n",
      "Iteration 683, loss = 0.89368153\n",
      "Iteration 684, loss = 0.89330292\n",
      "Iteration 685, loss = 0.89292423\n",
      "Iteration 686, loss = 0.89254549\n",
      "Iteration 687, loss = 0.89216667\n",
      "Iteration 688, loss = 0.89178780\n",
      "Iteration 689, loss = 0.89140886\n",
      "Iteration 690, loss = 0.89102986\n",
      "Iteration 691, loss = 0.89065080\n",
      "Iteration 692, loss = 0.89027168\n",
      "Iteration 693, loss = 0.88989251\n",
      "Iteration 694, loss = 0.88951329\n",
      "Iteration 695, loss = 0.88913401\n",
      "Iteration 696, loss = 0.88875468\n",
      "Iteration 697, loss = 0.88837530\n",
      "Iteration 698, loss = 0.88799588\n",
      "Iteration 699, loss = 0.88761641\n",
      "Iteration 700, loss = 0.88723689\n",
      "Iteration 701, loss = 0.88685733\n",
      "Iteration 702, loss = 0.88647773\n",
      "Iteration 703, loss = 0.88609809\n",
      "Iteration 704, loss = 0.88571842\n",
      "Iteration 705, loss = 0.88533870\n",
      "Iteration 706, loss = 0.88495895\n",
      "Iteration 707, loss = 0.88457917\n",
      "Iteration 708, loss = 0.88419936\n",
      "Iteration 709, loss = 0.88381952\n",
      "Iteration 710, loss = 0.88343965\n",
      "Iteration 711, loss = 0.88305975\n",
      "Iteration 712, loss = 0.88267983\n",
      "Iteration 713, loss = 0.88229988\n",
      "Iteration 714, loss = 0.88191991\n",
      "Iteration 715, loss = 0.88153992\n",
      "Iteration 716, loss = 0.88115992\n",
      "Iteration 717, loss = 0.88077989\n",
      "Iteration 718, loss = 0.88039985\n",
      "Iteration 719, loss = 0.88001980\n",
      "Iteration 720, loss = 0.87963973\n",
      "Iteration 721, loss = 0.87925965\n",
      "Iteration 722, loss = 0.87887956\n",
      "Iteration 723, loss = 0.87849947\n",
      "Iteration 724, loss = 0.87811936\n",
      "Iteration 725, loss = 0.87773926\n",
      "Iteration 726, loss = 0.87735915\n",
      "Iteration 727, loss = 0.87697903\n",
      "Iteration 728, loss = 0.87659892\n",
      "Iteration 729, loss = 0.87621881\n",
      "Iteration 730, loss = 0.87583870\n",
      "Iteration 731, loss = 0.87545860\n",
      "Iteration 732, loss = 0.87507850\n",
      "Iteration 733, loss = 0.87469841\n",
      "Iteration 734, loss = 0.87431833\n",
      "Iteration 735, loss = 0.87393826\n",
      "Iteration 736, loss = 0.87355821\n",
      "Iteration 737, loss = 0.87317816\n",
      "Iteration 738, loss = 0.87279813\n",
      "Iteration 739, loss = 0.87241812\n",
      "Iteration 740, loss = 0.87203813\n",
      "Iteration 741, loss = 0.87165816\n",
      "Iteration 742, loss = 0.87127820\n",
      "Iteration 743, loss = 0.87089827\n",
      "Iteration 744, loss = 0.87051837\n",
      "Iteration 745, loss = 0.87013849\n",
      "Iteration 746, loss = 0.86975864\n",
      "Iteration 747, loss = 0.86937881\n",
      "Iteration 748, loss = 0.86899902\n",
      "Iteration 749, loss = 0.86861926\n",
      "Iteration 750, loss = 0.86823953\n",
      "Iteration 751, loss = 0.86785984\n",
      "Iteration 752, loss = 0.86748018\n",
      "Iteration 753, loss = 0.86710056\n",
      "Iteration 754, loss = 0.86672099\n",
      "Iteration 755, loss = 0.86634145\n",
      "Iteration 756, loss = 0.86596195\n",
      "Iteration 757, loss = 0.86558250\n",
      "Iteration 758, loss = 0.86520309\n",
      "Iteration 759, loss = 0.86482373\n",
      "Iteration 760, loss = 0.86444442\n",
      "Iteration 761, loss = 0.86406516\n",
      "Iteration 762, loss = 0.86368594\n",
      "Iteration 763, loss = 0.86330679\n",
      "Iteration 764, loss = 0.86292768\n",
      "Iteration 765, loss = 0.86254863\n",
      "Iteration 766, loss = 0.86216964\n",
      "Iteration 767, loss = 0.86179070\n",
      "Iteration 768, loss = 0.86141183\n",
      "Iteration 769, loss = 0.86103302\n",
      "Iteration 770, loss = 0.86065427\n",
      "Iteration 771, loss = 0.86027558\n",
      "Iteration 772, loss = 0.85989696\n",
      "Iteration 773, loss = 0.85951841\n",
      "Iteration 774, loss = 0.85913992\n",
      "Iteration 775, loss = 0.85876151\n",
      "Iteration 776, loss = 0.85838316\n",
      "Iteration 777, loss = 0.85800489\n",
      "Iteration 778, loss = 0.85762669\n",
      "Iteration 779, loss = 0.85724857\n",
      "Iteration 780, loss = 0.85687053\n",
      "Iteration 781, loss = 0.85649256\n",
      "Iteration 782, loss = 0.85611467\n",
      "Iteration 783, loss = 0.85573687\n",
      "Iteration 784, loss = 0.85535914\n",
      "Iteration 785, loss = 0.85498151\n",
      "Iteration 786, loss = 0.85460395\n",
      "Iteration 787, loss = 0.85422649\n",
      "Iteration 788, loss = 0.85384911\n",
      "Iteration 789, loss = 0.85347182\n",
      "Iteration 790, loss = 0.85309462\n",
      "Iteration 791, loss = 0.85271751\n",
      "Iteration 792, loss = 0.85234050\n",
      "Iteration 793, loss = 0.85196358\n",
      "Iteration 794, loss = 0.85158676\n",
      "Iteration 795, loss = 0.85121004\n",
      "Iteration 796, loss = 0.85083341\n",
      "Iteration 797, loss = 0.85045689\n",
      "Iteration 798, loss = 0.85008047\n",
      "Iteration 799, loss = 0.84970415\n",
      "Iteration 800, loss = 0.84932793\n",
      "Iteration 801, loss = 0.84895182\n",
      "Iteration 802, loss = 0.84857582\n",
      "Iteration 803, loss = 0.84819993\n",
      "Iteration 804, loss = 0.84782415\n",
      "Iteration 805, loss = 0.84744847\n",
      "Iteration 806, loss = 0.84707292\n",
      "Iteration 807, loss = 0.84669747\n",
      "Iteration 808, loss = 0.84632214\n",
      "Iteration 809, loss = 0.84594692\n",
      "Iteration 810, loss = 0.84557183\n",
      "Iteration 811, loss = 0.84519685\n",
      "Iteration 812, loss = 0.84482199\n",
      "Iteration 813, loss = 0.84444726\n",
      "Iteration 814, loss = 0.84407265\n",
      "Iteration 815, loss = 0.84369816\n",
      "Iteration 816, loss = 0.84332379\n",
      "Iteration 817, loss = 0.84294956\n",
      "Iteration 818, loss = 0.84257545\n",
      "Iteration 819, loss = 0.84220147\n",
      "Iteration 820, loss = 0.84182762\n",
      "Iteration 821, loss = 0.84145391\n",
      "Iteration 822, loss = 0.84108033\n",
      "Iteration 823, loss = 0.84070688\n",
      "Iteration 824, loss = 0.84033357\n",
      "Iteration 825, loss = 0.83996039\n",
      "Iteration 826, loss = 0.83958735\n",
      "Iteration 827, loss = 0.83921446\n",
      "Iteration 828, loss = 0.83884170\n",
      "Iteration 829, loss = 0.83846908\n",
      "Iteration 830, loss = 0.83809661\n",
      "Iteration 831, loss = 0.83772428\n",
      "Iteration 832, loss = 0.83735210\n",
      "Iteration 833, loss = 0.83698007\n",
      "Iteration 834, loss = 0.83660818\n",
      "Iteration 835, loss = 0.83623644\n",
      "Iteration 836, loss = 0.83586486\n",
      "Iteration 837, loss = 0.83549342\n",
      "Iteration 838, loss = 0.83512214\n",
      "Iteration 839, loss = 0.83475101\n",
      "Iteration 840, loss = 0.83438004\n",
      "Iteration 841, loss = 0.83400922\n",
      "Iteration 842, loss = 0.83363856\n",
      "Iteration 843, loss = 0.83326806\n",
      "Iteration 844, loss = 0.83289772\n",
      "Iteration 845, loss = 0.83252755\n",
      "Iteration 846, loss = 0.83215753\n",
      "Iteration 847, loss = 0.83178768\n",
      "Iteration 848, loss = 0.83141799\n",
      "Iteration 849, loss = 0.83104847\n",
      "Iteration 850, loss = 0.83067912\n",
      "Iteration 851, loss = 0.83030993\n",
      "Iteration 852, loss = 0.82994091\n",
      "Iteration 853, loss = 0.82957207\n",
      "Iteration 854, loss = 0.82920339\n",
      "Iteration 855, loss = 0.82883489\n",
      "Iteration 856, loss = 0.82846656\n",
      "Iteration 857, loss = 0.82809841\n",
      "Iteration 858, loss = 0.82773044\n",
      "Iteration 859, loss = 0.82736264\n",
      "Iteration 860, loss = 0.82699502\n",
      "Iteration 861, loss = 0.82662758\n",
      "Iteration 862, loss = 0.82626032\n",
      "Iteration 863, loss = 0.82589324\n",
      "Iteration 864, loss = 0.82552634\n",
      "Iteration 865, loss = 0.82515963\n",
      "Iteration 866, loss = 0.82479311\n",
      "Iteration 867, loss = 0.82442677\n",
      "Iteration 868, loss = 0.82406061\n",
      "Iteration 869, loss = 0.82369465\n",
      "Iteration 870, loss = 0.82332888\n",
      "Iteration 871, loss = 0.82296329\n",
      "Iteration 872, loss = 0.82259790\n",
      "Iteration 873, loss = 0.82223270\n",
      "Iteration 874, loss = 0.82186769\n",
      "Iteration 875, loss = 0.82150288\n",
      "Iteration 876, loss = 0.82113827\n",
      "Iteration 877, loss = 0.82077385\n",
      "Iteration 878, loss = 0.82040963\n",
      "Iteration 879, loss = 0.82004560\n",
      "Iteration 880, loss = 0.81968178\n",
      "Iteration 881, loss = 0.81931816\n",
      "Iteration 882, loss = 0.81895474\n",
      "Iteration 883, loss = 0.81859152\n",
      "Iteration 884, loss = 0.81822851\n",
      "Iteration 885, loss = 0.81786571\n",
      "Iteration 886, loss = 0.81750310\n",
      "Iteration 887, loss = 0.81714071\n",
      "Iteration 888, loss = 0.81677853\n",
      "Iteration 889, loss = 0.81641655\n",
      "Iteration 890, loss = 0.81605478\n",
      "Iteration 891, loss = 0.81569323\n",
      "Iteration 892, loss = 0.81533188\n",
      "Iteration 893, loss = 0.81497075\n",
      "Iteration 894, loss = 0.81460984\n",
      "Iteration 895, loss = 0.81424913\n",
      "Iteration 896, loss = 0.81388865\n",
      "Iteration 897, loss = 0.81352838\n",
      "Iteration 898, loss = 0.81316833\n",
      "Iteration 899, loss = 0.81280850\n",
      "Iteration 900, loss = 0.81244888\n",
      "Iteration 901, loss = 0.81208949\n",
      "Iteration 902, loss = 0.81173032\n",
      "Iteration 903, loss = 0.81137137\n",
      "Iteration 904, loss = 0.81101265\n",
      "Iteration 905, loss = 0.81065415\n",
      "Iteration 906, loss = 0.81029587\n",
      "Iteration 907, loss = 0.80993783\n",
      "Iteration 908, loss = 0.80958001\n",
      "Iteration 909, loss = 0.80922241\n",
      "Iteration 910, loss = 0.80886505\n",
      "Iteration 911, loss = 0.80850791\n",
      "Iteration 912, loss = 0.80815101\n",
      "Iteration 913, loss = 0.80779434\n",
      "Iteration 914, loss = 0.80743790\n",
      "Iteration 915, loss = 0.80708169\n",
      "Iteration 916, loss = 0.80672572\n",
      "Iteration 917, loss = 0.80636998\n",
      "Iteration 918, loss = 0.80601448\n",
      "Iteration 919, loss = 0.80565922\n",
      "Iteration 920, loss = 0.80530419\n",
      "Iteration 921, loss = 0.80494941\n",
      "Iteration 922, loss = 0.80459486\n",
      "Iteration 923, loss = 0.80424055\n",
      "Iteration 924, loss = 0.80388648\n",
      "Iteration 925, loss = 0.80353266\n",
      "Iteration 926, loss = 0.80317908\n",
      "Iteration 927, loss = 0.80282574\n",
      "Iteration 928, loss = 0.80247265\n",
      "Iteration 929, loss = 0.80211980\n",
      "Iteration 930, loss = 0.80176720\n",
      "Iteration 931, loss = 0.80141484\n",
      "Iteration 932, loss = 0.80106273\n",
      "Iteration 933, loss = 0.80071087\n",
      "Iteration 934, loss = 0.80035926\n",
      "Iteration 935, loss = 0.80000790\n",
      "Iteration 936, loss = 0.79965680\n",
      "Iteration 937, loss = 0.79930594\n",
      "Iteration 938, loss = 0.79895533\n",
      "Iteration 939, loss = 0.79860498\n",
      "Iteration 940, loss = 0.79825488\n",
      "Iteration 941, loss = 0.79790504\n",
      "Iteration 942, loss = 0.79755545\n",
      "Iteration 943, loss = 0.79720612\n",
      "Iteration 944, loss = 0.79685705\n",
      "Iteration 945, loss = 0.79650823\n",
      "Iteration 946, loss = 0.79615967\n",
      "Iteration 947, loss = 0.79581138\n",
      "Iteration 948, loss = 0.79546334\n",
      "Iteration 949, loss = 0.79511556\n",
      "Iteration 950, loss = 0.79476804\n",
      "Iteration 951, loss = 0.79442079\n",
      "Iteration 952, loss = 0.79407380\n",
      "Iteration 953, loss = 0.79372707\n",
      "Iteration 954, loss = 0.79338061\n",
      "Iteration 955, loss = 0.79303441\n",
      "Iteration 956, loss = 0.79268848\n",
      "Iteration 957, loss = 0.79234281\n",
      "Iteration 958, loss = 0.79199741\n",
      "Iteration 959, loss = 0.79165228\n",
      "Iteration 960, loss = 0.79130742\n",
      "Iteration 961, loss = 0.79096282\n",
      "Iteration 962, loss = 0.79061850\n",
      "Iteration 963, loss = 0.79027445\n",
      "Iteration 964, loss = 0.78993066\n",
      "Iteration 965, loss = 0.78958715\n",
      "Iteration 966, loss = 0.78924392\n",
      "Iteration 967, loss = 0.78890095\n",
      "Iteration 968, loss = 0.78855826\n",
      "Iteration 969, loss = 0.78821584\n",
      "Iteration 970, loss = 0.78787370\n",
      "Iteration 971, loss = 0.78753184\n",
      "Iteration 972, loss = 0.78719025\n",
      "Iteration 973, loss = 0.78684893\n",
      "Iteration 974, loss = 0.78650790\n",
      "Iteration 975, loss = 0.78616714\n",
      "Iteration 976, loss = 0.78582666\n",
      "Iteration 977, loss = 0.78548647\n",
      "Iteration 978, loss = 0.78514655\n",
      "Iteration 979, loss = 0.78480691\n",
      "Iteration 980, loss = 0.78446755\n",
      "Iteration 981, loss = 0.78412848\n",
      "Iteration 982, loss = 0.78378968\n",
      "Iteration 983, loss = 0.78345117\n",
      "Iteration 984, loss = 0.78311295\n",
      "Iteration 985, loss = 0.78277501\n",
      "Iteration 986, loss = 0.78243735\n",
      "Iteration 987, loss = 0.78209998\n",
      "Iteration 988, loss = 0.78176289\n",
      "Iteration 989, loss = 0.78142609\n",
      "Iteration 990, loss = 0.78108958\n",
      "Iteration 991, loss = 0.78075335\n",
      "Iteration 992, loss = 0.78041742\n",
      "Iteration 993, loss = 0.78008177\n",
      "Iteration 994, loss = 0.77974641\n",
      "Iteration 995, loss = 0.77941134\n",
      "Iteration 996, loss = 0.77907656\n",
      "Iteration 997, loss = 0.77874207\n",
      "Iteration 998, loss = 0.77840787\n",
      "Iteration 999, loss = 0.77807396\n",
      "Iteration 1000, loss = 0.77774035\n",
      "Iteration 1, loss = 1.13942152\n",
      "Iteration 2, loss = 1.13924771\n",
      "Iteration 3, loss = 1.13900045\n",
      "Iteration 4, loss = 1.13868774\n",
      "Iteration 5, loss = 1.13831691\n",
      "Iteration 6, loss = 1.13789467\n",
      "Iteration 7, loss = 1.13742710\n",
      "Iteration 8, loss = 1.13691976\n",
      "Iteration 9, loss = 1.13637769\n",
      "Iteration 10, loss = 1.13580545\n",
      "Iteration 11, loss = 1.13520717\n",
      "Iteration 12, loss = 1.13458659\n",
      "Iteration 13, loss = 1.13394709\n",
      "Iteration 14, loss = 1.13329168\n",
      "Iteration 15, loss = 1.13262311\n",
      "Iteration 16, loss = 1.13194382\n",
      "Iteration 17, loss = 1.13125602\n",
      "Iteration 18, loss = 1.13056167\n",
      "Iteration 19, loss = 1.12986254\n",
      "Iteration 20, loss = 1.12916020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhruv/indexCode/cmpsc445/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 1.12845605\n",
      "Iteration 22, loss = 1.12775135\n",
      "Iteration 23, loss = 1.12704720\n",
      "Iteration 24, loss = 1.12634458\n",
      "Iteration 25, loss = 1.12564436\n",
      "Iteration 26, loss = 1.12494730\n",
      "Iteration 27, loss = 1.12425408\n",
      "Iteration 28, loss = 1.12356529\n",
      "Iteration 29, loss = 1.12288144\n",
      "Iteration 30, loss = 1.12220297\n",
      "Iteration 31, loss = 1.12153028\n",
      "Iteration 32, loss = 1.12086369\n",
      "Iteration 33, loss = 1.12020350\n",
      "Iteration 34, loss = 1.11954992\n",
      "Iteration 35, loss = 1.11890318\n",
      "Iteration 36, loss = 1.11826342\n",
      "Iteration 37, loss = 1.11763079\n",
      "Iteration 38, loss = 1.11700539\n",
      "Iteration 39, loss = 1.11638729\n",
      "Iteration 40, loss = 1.11577657\n",
      "Iteration 41, loss = 1.11517325\n",
      "Iteration 42, loss = 1.11457735\n",
      "Iteration 43, loss = 1.11398888\n",
      "Iteration 44, loss = 1.11340783\n",
      "Iteration 45, loss = 1.11283418\n",
      "Iteration 46, loss = 1.11226789\n",
      "Iteration 47, loss = 1.11170892\n",
      "Iteration 48, loss = 1.11115723\n",
      "Iteration 49, loss = 1.11061274\n",
      "Iteration 50, loss = 1.11007541\n",
      "Iteration 51, loss = 1.10954515\n",
      "Iteration 52, loss = 1.10902190\n",
      "Iteration 53, loss = 1.10850556\n",
      "Iteration 54, loss = 1.10799607\n",
      "Iteration 55, loss = 1.10749333\n",
      "Iteration 56, loss = 1.10699726\n",
      "Iteration 57, loss = 1.10650775\n",
      "Iteration 58, loss = 1.10602473\n",
      "Iteration 59, loss = 1.10554810\n",
      "Iteration 60, loss = 1.10507776\n",
      "Iteration 61, loss = 1.10461362\n",
      "Iteration 62, loss = 1.10415557\n",
      "Iteration 63, loss = 1.10370353\n",
      "Iteration 64, loss = 1.10325739\n",
      "Iteration 65, loss = 1.10281706\n",
      "Iteration 66, loss = 1.10238244\n",
      "Iteration 67, loss = 1.10195344\n",
      "Iteration 68, loss = 1.10152995\n",
      "Iteration 69, loss = 1.10111189\n",
      "Iteration 70, loss = 1.10069916\n",
      "Iteration 71, loss = 1.10029165\n",
      "Iteration 72, loss = 1.09988929\n",
      "Iteration 73, loss = 1.09949197\n",
      "Iteration 74, loss = 1.09909961\n",
      "Iteration 75, loss = 1.09871211\n",
      "Iteration 76, loss = 1.09832939\n",
      "Iteration 77, loss = 1.09795134\n",
      "Iteration 78, loss = 1.09757790\n",
      "Iteration 79, loss = 1.09720896\n",
      "Iteration 80, loss = 1.09684444\n",
      "Iteration 81, loss = 1.09648426\n",
      "Iteration 82, loss = 1.09612833\n",
      "Iteration 83, loss = 1.09577658\n",
      "Iteration 84, loss = 1.09542891\n",
      "Iteration 85, loss = 1.09508525\n",
      "Iteration 86, loss = 1.09474552\n",
      "Iteration 87, loss = 1.09440964\n",
      "Iteration 88, loss = 1.09407753\n",
      "Iteration 89, loss = 1.09374912\n",
      "Iteration 90, loss = 1.09342433\n",
      "Iteration 91, loss = 1.09310309\n",
      "Iteration 92, loss = 1.09278532\n",
      "Iteration 93, loss = 1.09247095\n",
      "Iteration 94, loss = 1.09215991\n",
      "Iteration 95, loss = 1.09185214\n",
      "Iteration 96, loss = 1.09154756\n",
      "Iteration 97, loss = 1.09124611\n",
      "Iteration 98, loss = 1.09094771\n",
      "Iteration 99, loss = 1.09065231\n",
      "Iteration 100, loss = 1.09035984\n",
      "Iteration 101, loss = 1.09007023\n",
      "Iteration 102, loss = 1.08978342\n",
      "Iteration 103, loss = 1.08949936\n",
      "Iteration 104, loss = 1.08921797\n",
      "Iteration 105, loss = 1.08893921\n",
      "Iteration 106, loss = 1.08866301\n",
      "Iteration 107, loss = 1.08838931\n",
      "Iteration 108, loss = 1.08811806\n",
      "Iteration 109, loss = 1.08784920\n",
      "Iteration 110, loss = 1.08758268\n",
      "Iteration 111, loss = 1.08731844\n",
      "Iteration 112, loss = 1.08705643\n",
      "Iteration 113, loss = 1.08679660\n",
      "Iteration 114, loss = 1.08653889\n",
      "Iteration 115, loss = 1.08628326\n",
      "Iteration 116, loss = 1.08602965\n",
      "Iteration 117, loss = 1.08577801\n",
      "Iteration 118, loss = 1.08552831\n",
      "Iteration 119, loss = 1.08528048\n",
      "Iteration 120, loss = 1.08503449\n",
      "Iteration 121, loss = 1.08479029\n",
      "Iteration 122, loss = 1.08454782\n",
      "Iteration 123, loss = 1.08430706\n",
      "Iteration 124, loss = 1.08406795\n",
      "Iteration 125, loss = 1.08383045\n",
      "Iteration 126, loss = 1.08359452\n",
      "Iteration 127, loss = 1.08336012\n",
      "Iteration 128, loss = 1.08312721\n",
      "Iteration 129, loss = 1.08289575\n",
      "Iteration 130, loss = 1.08266569\n",
      "Iteration 131, loss = 1.08243701\n",
      "Iteration 132, loss = 1.08220965\n",
      "Iteration 133, loss = 1.08198359\n",
      "Iteration 134, loss = 1.08175879\n",
      "Iteration 135, loss = 1.08153521\n",
      "Iteration 136, loss = 1.08131282\n",
      "Iteration 137, loss = 1.08109158\n",
      "Iteration 138, loss = 1.08087145\n",
      "Iteration 139, loss = 1.08065242\n",
      "Iteration 140, loss = 1.08043443\n",
      "Iteration 141, loss = 1.08021746\n",
      "Iteration 142, loss = 1.08000148\n",
      "Iteration 143, loss = 1.07978645\n",
      "Iteration 144, loss = 1.07957235\n",
      "Iteration 145, loss = 1.07935915\n",
      "Iteration 146, loss = 1.07914681\n",
      "Iteration 147, loss = 1.07893531\n",
      "Iteration 148, loss = 1.07872462\n",
      "Iteration 149, loss = 1.07851471\n",
      "Iteration 150, loss = 1.07830555\n",
      "Iteration 151, loss = 1.07809712\n",
      "Iteration 152, loss = 1.07788939\n",
      "Iteration 153, loss = 1.07768233\n",
      "Iteration 154, loss = 1.07747592\n",
      "Iteration 155, loss = 1.07727013\n",
      "Iteration 156, loss = 1.07706495\n",
      "Iteration 157, loss = 1.07686034\n",
      "Iteration 158, loss = 1.07665628\n",
      "Iteration 159, loss = 1.07645275\n",
      "Iteration 160, loss = 1.07624972\n",
      "Iteration 161, loss = 1.07604718\n",
      "Iteration 162, loss = 1.07584511\n",
      "Iteration 163, loss = 1.07564347\n",
      "Iteration 164, loss = 1.07544225\n",
      "Iteration 165, loss = 1.07524143\n",
      "Iteration 166, loss = 1.07504099\n",
      "Iteration 167, loss = 1.07484091\n",
      "Iteration 168, loss = 1.07464117\n",
      "Iteration 169, loss = 1.07444175\n",
      "Iteration 170, loss = 1.07424263\n",
      "Iteration 171, loss = 1.07404380\n",
      "Iteration 172, loss = 1.07384523\n",
      "Iteration 173, loss = 1.07364691\n",
      "Iteration 174, loss = 1.07344883\n",
      "Iteration 175, loss = 1.07325095\n",
      "Iteration 176, loss = 1.07305328\n",
      "Iteration 177, loss = 1.07285579\n",
      "Iteration 178, loss = 1.07265846\n",
      "Iteration 179, loss = 1.07246128\n",
      "Iteration 180, loss = 1.07226424\n",
      "Iteration 181, loss = 1.07206732\n",
      "Iteration 182, loss = 1.07187051\n",
      "Iteration 183, loss = 1.07167379\n",
      "Iteration 184, loss = 1.07147714\n",
      "Iteration 185, loss = 1.07128056\n",
      "Iteration 186, loss = 1.07108403\n",
      "Iteration 187, loss = 1.07088754\n",
      "Iteration 188, loss = 1.07069107\n",
      "Iteration 189, loss = 1.07049462\n",
      "Iteration 190, loss = 1.07029816\n",
      "Iteration 191, loss = 1.07010169\n",
      "Iteration 192, loss = 1.06990520\n",
      "Iteration 193, loss = 1.06970867\n",
      "Iteration 194, loss = 1.06951209\n",
      "Iteration 195, loss = 1.06931545\n",
      "Iteration 196, loss = 1.06911875\n",
      "Iteration 197, loss = 1.06892196\n",
      "Iteration 198, loss = 1.06872508\n",
      "Iteration 199, loss = 1.06852811\n",
      "Iteration 200, loss = 1.06833102\n",
      "Iteration 201, loss = 1.06813381\n",
      "Iteration 202, loss = 1.06793647\n",
      "Iteration 203, loss = 1.06773899\n",
      "Iteration 204, loss = 1.06754136\n",
      "Iteration 205, loss = 1.06734358\n",
      "Iteration 206, loss = 1.06714562\n",
      "Iteration 207, loss = 1.06694749\n",
      "Iteration 208, loss = 1.06674918\n",
      "Iteration 209, loss = 1.06655067\n",
      "Iteration 210, loss = 1.06635196\n",
      "Iteration 211, loss = 1.06615304\n",
      "Iteration 212, loss = 1.06595391\n",
      "Iteration 213, loss = 1.06575455\n",
      "Iteration 214, loss = 1.06555496\n",
      "Iteration 215, loss = 1.06535512\n",
      "Iteration 216, loss = 1.06515505\n",
      "Iteration 217, loss = 1.06495471\n",
      "Iteration 218, loss = 1.06475412\n",
      "Iteration 219, loss = 1.06455326\n",
      "Iteration 220, loss = 1.06435212\n",
      "Iteration 221, loss = 1.06415070\n",
      "Iteration 222, loss = 1.06394899\n",
      "Iteration 223, loss = 1.06374699\n",
      "Iteration 224, loss = 1.06354468\n",
      "Iteration 225, loss = 1.06334207\n",
      "Iteration 226, loss = 1.06313914\n",
      "Iteration 227, loss = 1.06293590\n",
      "Iteration 228, loss = 1.06273233\n",
      "Iteration 229, loss = 1.06252843\n",
      "Iteration 230, loss = 1.06232419\n",
      "Iteration 231, loss = 1.06211961\n",
      "Iteration 232, loss = 1.06191468\n",
      "Iteration 233, loss = 1.06170939\n",
      "Iteration 234, loss = 1.06150375\n",
      "Iteration 235, loss = 1.06129774\n",
      "Iteration 236, loss = 1.06109136\n",
      "Iteration 237, loss = 1.06088461\n",
      "Iteration 238, loss = 1.06067748\n",
      "Iteration 239, loss = 1.06046996\n",
      "Iteration 240, loss = 1.06026205\n",
      "Iteration 241, loss = 1.06005375\n",
      "Iteration 242, loss = 1.05984504\n",
      "Iteration 243, loss = 1.05963594\n",
      "Iteration 244, loss = 1.05942642\n",
      "Iteration 245, loss = 1.05921649\n",
      "Iteration 246, loss = 1.05900613\n",
      "Iteration 247, loss = 1.05879536\n",
      "Iteration 248, loss = 1.05858416\n",
      "Iteration 249, loss = 1.05837252\n",
      "Iteration 250, loss = 1.05816045\n",
      "Iteration 251, loss = 1.05794793\n",
      "Iteration 252, loss = 1.05773497\n",
      "Iteration 253, loss = 1.05752156\n",
      "Iteration 254, loss = 1.05730769\n",
      "Iteration 255, loss = 1.05709337\n",
      "Iteration 256, loss = 1.05687858\n",
      "Iteration 257, loss = 1.05666332\n",
      "Iteration 258, loss = 1.05644759\n",
      "Iteration 259, loss = 1.05623139\n",
      "Iteration 260, loss = 1.05601471\n",
      "Iteration 261, loss = 1.05579754\n",
      "Iteration 262, loss = 1.05557988\n",
      "Iteration 263, loss = 1.05536173\n",
      "Iteration 264, loss = 1.05514309\n",
      "Iteration 265, loss = 1.05492395\n",
      "Iteration 266, loss = 1.05470430\n",
      "Iteration 267, loss = 1.05448414\n",
      "Iteration 268, loss = 1.05426347\n",
      "Iteration 269, loss = 1.05404228\n",
      "Iteration 270, loss = 1.05382058\n",
      "Iteration 271, loss = 1.05359835\n",
      "Iteration 272, loss = 1.05337559\n",
      "Iteration 273, loss = 1.05315230\n",
      "Iteration 274, loss = 1.05292847\n",
      "Iteration 275, loss = 1.05270411\n",
      "Iteration 276, loss = 1.05247920\n",
      "Iteration 277, loss = 1.05225375\n",
      "Iteration 278, loss = 1.05202774\n",
      "Iteration 279, loss = 1.05180118\n",
      "Iteration 280, loss = 1.05157406\n",
      "Iteration 281, loss = 1.05134638\n",
      "Iteration 282, loss = 1.05111814\n",
      "Iteration 283, loss = 1.05088932\n",
      "Iteration 284, loss = 1.05065993\n",
      "Iteration 285, loss = 1.05042997\n",
      "Iteration 286, loss = 1.05019943\n",
      "Iteration 287, loss = 1.04996830\n",
      "Iteration 288, loss = 1.04973658\n",
      "Iteration 289, loss = 1.04950427\n",
      "Iteration 290, loss = 1.04927137\n",
      "Iteration 291, loss = 1.04903787\n",
      "Iteration 292, loss = 1.04880377\n",
      "Iteration 293, loss = 1.04856906\n",
      "Iteration 294, loss = 1.04833375\n",
      "Iteration 295, loss = 1.04809782\n",
      "Iteration 296, loss = 1.04786128\n",
      "Iteration 297, loss = 1.04762411\n",
      "Iteration 298, loss = 1.04738633\n",
      "Iteration 299, loss = 1.04714792\n",
      "Iteration 300, loss = 1.04690888\n",
      "Iteration 301, loss = 1.04666921\n",
      "Iteration 302, loss = 1.04642890\n",
      "Iteration 303, loss = 1.04618795\n",
      "Iteration 304, loss = 1.04594636\n",
      "Iteration 305, loss = 1.04570413\n",
      "Iteration 306, loss = 1.04546124\n",
      "Iteration 307, loss = 1.04521770\n",
      "Iteration 308, loss = 1.04497351\n",
      "Iteration 309, loss = 1.04472866\n",
      "Iteration 310, loss = 1.04448315\n",
      "Iteration 311, loss = 1.04423697\n",
      "Iteration 312, loss = 1.04399013\n",
      "Iteration 313, loss = 1.04374262\n",
      "Iteration 314, loss = 1.04349443\n",
      "Iteration 315, loss = 1.04324557\n",
      "Iteration 316, loss = 1.04299602\n",
      "Iteration 317, loss = 1.04274580\n",
      "Iteration 318, loss = 1.04249489\n",
      "Iteration 319, loss = 1.04224329\n",
      "Iteration 320, loss = 1.04199100\n",
      "Iteration 321, loss = 1.04173802\n",
      "Iteration 322, loss = 1.04148434\n",
      "Iteration 323, loss = 1.04122997\n",
      "Iteration 324, loss = 1.04097489\n",
      "Iteration 325, loss = 1.04071911\n",
      "Iteration 326, loss = 1.04046263\n",
      "Iteration 327, loss = 1.04020543\n",
      "Iteration 328, loss = 1.03994753\n",
      "Iteration 329, loss = 1.03968891\n",
      "Iteration 330, loss = 1.03942958\n",
      "Iteration 331, loss = 1.03916953\n",
      "Iteration 332, loss = 1.03890876\n",
      "Iteration 333, loss = 1.03864727\n",
      "Iteration 334, loss = 1.03838505\n",
      "Iteration 335, loss = 1.03812211\n",
      "Iteration 336, loss = 1.03785845\n",
      "Iteration 337, loss = 1.03759405\n",
      "Iteration 338, loss = 1.03732892\n",
      "Iteration 339, loss = 1.03706306\n",
      "Iteration 340, loss = 1.03679646\n",
      "Iteration 341, loss = 1.03652913\n",
      "Iteration 342, loss = 1.03626106\n",
      "Iteration 343, loss = 1.03599225\n",
      "Iteration 344, loss = 1.03572270\n",
      "Iteration 345, loss = 1.03545241\n",
      "Iteration 346, loss = 1.03518138\n",
      "Iteration 347, loss = 1.03490960\n",
      "Iteration 348, loss = 1.03463707\n",
      "Iteration 349, loss = 1.03436380\n",
      "Iteration 350, loss = 1.03408978\n",
      "Iteration 351, loss = 1.03381501\n",
      "Iteration 352, loss = 1.03353949\n",
      "Iteration 353, loss = 1.03326322\n",
      "Iteration 354, loss = 1.03298621\n",
      "Iteration 355, loss = 1.03270844\n",
      "Iteration 356, loss = 1.03242991\n",
      "Iteration 357, loss = 1.03215064\n",
      "Iteration 358, loss = 1.03187061\n",
      "Iteration 359, loss = 1.03158983\n",
      "Iteration 360, loss = 1.03130829\n",
      "Iteration 361, loss = 1.03102600\n",
      "Iteration 362, loss = 1.03074296\n",
      "Iteration 363, loss = 1.03045917\n",
      "Iteration 364, loss = 1.03017462\n",
      "Iteration 365, loss = 1.02988931\n",
      "Iteration 366, loss = 1.02960326\n",
      "Iteration 367, loss = 1.02931645\n",
      "Iteration 368, loss = 1.02902889\n",
      "Iteration 369, loss = 1.02874057\n",
      "Iteration 370, loss = 1.02845151\n",
      "Iteration 371, loss = 1.02816169\n",
      "Iteration 372, loss = 1.02787113\n",
      "Iteration 373, loss = 1.02757981\n",
      "Iteration 374, loss = 1.02728775\n",
      "Iteration 375, loss = 1.02699494\n",
      "Iteration 376, loss = 1.02670139\n",
      "Iteration 377, loss = 1.02640708\n",
      "Iteration 378, loss = 1.02611204\n",
      "Iteration 379, loss = 1.02581625\n",
      "Iteration 380, loss = 1.02551972\n",
      "Iteration 381, loss = 1.02522246\n",
      "Iteration 382, loss = 1.02492445\n",
      "Iteration 383, loss = 1.02462571\n",
      "Iteration 384, loss = 1.02432623\n",
      "Iteration 385, loss = 1.02402602\n",
      "Iteration 386, loss = 1.02372508\n",
      "Iteration 387, loss = 1.02342341\n",
      "Iteration 388, loss = 1.02312101\n",
      "Iteration 389, loss = 1.02281789\n",
      "Iteration 390, loss = 1.02251404\n",
      "Iteration 391, loss = 1.02220948\n",
      "Iteration 392, loss = 1.02190419\n",
      "Iteration 393, loss = 1.02159819\n",
      "Iteration 394, loss = 1.02129148\n",
      "Iteration 395, loss = 1.02098406\n",
      "Iteration 396, loss = 1.02067592\n",
      "Iteration 397, loss = 1.02036709\n",
      "Iteration 398, loss = 1.02005754\n",
      "Iteration 399, loss = 1.01974730\n",
      "Iteration 400, loss = 1.01943636\n",
      "Iteration 401, loss = 1.01912473\n",
      "Iteration 402, loss = 1.01881240\n",
      "Iteration 403, loss = 1.01849939\n",
      "Iteration 404, loss = 1.01818569\n",
      "Iteration 405, loss = 1.01787131\n",
      "Iteration 406, loss = 1.01755625\n",
      "Iteration 407, loss = 1.01724051\n",
      "Iteration 408, loss = 1.01692410\n",
      "Iteration 409, loss = 1.01660702\n",
      "Iteration 410, loss = 1.01628927\n",
      "Iteration 411, loss = 1.01597086\n",
      "Iteration 412, loss = 1.01565179\n",
      "Iteration 413, loss = 1.01533206\n",
      "Iteration 414, loss = 1.01501169\n",
      "Iteration 415, loss = 1.01469066\n",
      "Iteration 416, loss = 1.01436899\n",
      "Iteration 417, loss = 1.01404667\n",
      "Iteration 418, loss = 1.01372372\n",
      "Iteration 419, loss = 1.01340013\n",
      "Iteration 420, loss = 1.01307591\n",
      "Iteration 421, loss = 1.01275107\n",
      "Iteration 422, loss = 1.01242560\n",
      "Iteration 423, loss = 1.01209950\n",
      "Iteration 424, loss = 1.01177280\n",
      "Iteration 425, loss = 1.01144548\n",
      "Iteration 426, loss = 1.01111755\n",
      "Iteration 427, loss = 1.01078901\n",
      "Iteration 428, loss = 1.01045988\n",
      "Iteration 429, loss = 1.01013014\n",
      "Iteration 430, loss = 1.00979982\n",
      "Iteration 431, loss = 1.00946890\n",
      "Iteration 432, loss = 1.00913739\n",
      "Iteration 433, loss = 1.00880530\n",
      "Iteration 434, loss = 1.00847263\n",
      "Iteration 435, loss = 1.00813939\n",
      "Iteration 436, loss = 1.00780557\n",
      "Iteration 437, loss = 1.00747119\n",
      "Iteration 438, loss = 1.00713624\n",
      "Iteration 439, loss = 1.00680073\n",
      "Iteration 440, loss = 1.00646466\n",
      "Iteration 441, loss = 1.00612803\n",
      "Iteration 442, loss = 1.00579086\n",
      "Iteration 443, loss = 1.00545314\n",
      "Iteration 444, loss = 1.00511487\n",
      "Iteration 445, loss = 1.00477607\n",
      "Iteration 446, loss = 1.00443672\n",
      "Iteration 447, loss = 1.00409685\n",
      "Iteration 448, loss = 1.00375644\n",
      "Iteration 449, loss = 1.00341551\n",
      "Iteration 450, loss = 1.00307405\n",
      "Iteration 451, loss = 1.00273207\n",
      "Iteration 452, loss = 1.00238957\n",
      "Iteration 453, loss = 1.00204656\n",
      "Iteration 454, loss = 1.00170303\n",
      "Iteration 455, loss = 1.00135900\n",
      "Iteration 456, loss = 1.00101446\n",
      "Iteration 457, loss = 1.00066942\n",
      "Iteration 458, loss = 1.00032388\n",
      "Iteration 459, loss = 0.99997784\n",
      "Iteration 460, loss = 0.99963131\n",
      "Iteration 461, loss = 0.99928428\n",
      "Iteration 462, loss = 0.99893677\n",
      "Iteration 463, loss = 0.99858876\n",
      "Iteration 464, loss = 0.99824028\n",
      "Iteration 465, loss = 0.99789131\n",
      "Iteration 466, loss = 0.99754186\n",
      "Iteration 467, loss = 0.99719193\n",
      "Iteration 468, loss = 0.99684153\n",
      "Iteration 469, loss = 0.99649065\n",
      "Iteration 470, loss = 0.99613930\n",
      "Iteration 471, loss = 0.99578748\n",
      "Iteration 472, loss = 0.99543520\n",
      "Iteration 473, loss = 0.99508245\n",
      "Iteration 474, loss = 0.99472924\n",
      "Iteration 475, loss = 0.99437556\n",
      "Iteration 476, loss = 0.99402143\n",
      "Iteration 477, loss = 0.99366683\n",
      "Iteration 478, loss = 0.99331178\n",
      "Iteration 479, loss = 0.99295628\n",
      "Iteration 480, loss = 0.99260032\n",
      "Iteration 481, loss = 0.99224390\n",
      "Iteration 482, loss = 0.99188704\n",
      "Iteration 483, loss = 0.99152972\n",
      "Iteration 484, loss = 0.99117196\n",
      "Iteration 485, loss = 0.99081375\n",
      "Iteration 486, loss = 0.99045509\n",
      "Iteration 487, loss = 0.99009598\n",
      "Iteration 488, loss = 0.98973643\n",
      "Iteration 489, loss = 0.98937644\n",
      "Iteration 490, loss = 0.98901600\n",
      "Iteration 491, loss = 0.98865512\n",
      "Iteration 492, loss = 0.98829380\n",
      "Iteration 493, loss = 0.98793204\n",
      "Iteration 494, loss = 0.98756984\n",
      "Iteration 495, loss = 0.98720719\n",
      "Iteration 496, loss = 0.98684411\n",
      "Iteration 497, loss = 0.98648059\n",
      "Iteration 498, loss = 0.98611663\n",
      "Iteration 499, loss = 0.98575223\n",
      "Iteration 500, loss = 0.98538740\n",
      "Iteration 501, loss = 0.98502213\n",
      "Iteration 502, loss = 0.98465642\n",
      "Iteration 503, loss = 0.98429027\n",
      "Iteration 504, loss = 0.98392369\n",
      "Iteration 505, loss = 0.98355667\n",
      "Iteration 506, loss = 0.98318921\n",
      "Iteration 507, loss = 0.98282131\n",
      "Iteration 508, loss = 0.98245298\n",
      "Iteration 509, loss = 0.98208422\n",
      "Iteration 510, loss = 0.98171501\n",
      "Iteration 511, loss = 0.98134537\n",
      "Iteration 512, loss = 0.98097529\n",
      "Iteration 513, loss = 0.98060478\n",
      "Iteration 514, loss = 0.98023382\n",
      "Iteration 515, loss = 0.97986243\n",
      "Iteration 516, loss = 0.97949060\n",
      "Iteration 517, loss = 0.97911834\n",
      "Iteration 518, loss = 0.97874564\n",
      "Iteration 519, loss = 0.97837249\n",
      "Iteration 520, loss = 0.97799891\n",
      "Iteration 521, loss = 0.97762489\n",
      "Iteration 522, loss = 0.97725043\n",
      "Iteration 523, loss = 0.97687553\n",
      "Iteration 524, loss = 0.97650019\n",
      "Iteration 525, loss = 0.97612441\n",
      "Iteration 526, loss = 0.97574819\n",
      "Iteration 527, loss = 0.97537153\n",
      "Iteration 528, loss = 0.97499443\n",
      "Iteration 529, loss = 0.97461688\n",
      "Iteration 530, loss = 0.97423890\n",
      "Iteration 531, loss = 0.97386047\n",
      "Iteration 532, loss = 0.97348160\n",
      "Iteration 533, loss = 0.97310228\n",
      "Iteration 534, loss = 0.97272252\n",
      "Iteration 535, loss = 0.97234232\n",
      "Iteration 536, loss = 0.97196167\n",
      "Iteration 537, loss = 0.97158058\n",
      "Iteration 538, loss = 0.97119904\n",
      "Iteration 539, loss = 0.97081706\n",
      "Iteration 540, loss = 0.97043463\n",
      "Iteration 541, loss = 0.97005175\n",
      "Iteration 542, loss = 0.96966843\n",
      "Iteration 543, loss = 0.96928466\n",
      "Iteration 544, loss = 0.96890045\n",
      "Iteration 545, loss = 0.96851579\n",
      "Iteration 546, loss = 0.96813068\n",
      "Iteration 547, loss = 0.96774512\n",
      "Iteration 548, loss = 0.96735912\n",
      "Iteration 549, loss = 0.96697266\n",
      "Iteration 550, loss = 0.96658576\n",
      "Iteration 551, loss = 0.96619841\n",
      "Iteration 552, loss = 0.96581061\n",
      "Iteration 553, loss = 0.96542236\n",
      "Iteration 554, loss = 0.96503367\n",
      "Iteration 555, loss = 0.96464452\n",
      "Iteration 556, loss = 0.96425493\n",
      "Iteration 557, loss = 0.96386488\n",
      "Iteration 558, loss = 0.96347439\n",
      "Iteration 559, loss = 0.96308345\n",
      "Iteration 560, loss = 0.96269206\n",
      "Iteration 561, loss = 0.96230022\n",
      "Iteration 562, loss = 0.96190793\n",
      "Iteration 563, loss = 0.96151519\n",
      "Iteration 564, loss = 0.96112200\n",
      "Iteration 565, loss = 0.96072837\n",
      "Iteration 566, loss = 0.96033428\n",
      "Iteration 567, loss = 0.95993975\n",
      "Iteration 568, loss = 0.95954477\n",
      "Iteration 569, loss = 0.95914935\n",
      "Iteration 570, loss = 0.95875347\n",
      "Iteration 571, loss = 0.95835715\n",
      "Iteration 572, loss = 0.95796039\n",
      "Iteration 573, loss = 0.95756317\n",
      "Iteration 574, loss = 0.95716552\n",
      "Iteration 575, loss = 0.95676741\n",
      "Iteration 576, loss = 0.95636886\n",
      "Iteration 577, loss = 0.95596987\n",
      "Iteration 578, loss = 0.95557043\n",
      "Iteration 579, loss = 0.95517055\n",
      "Iteration 580, loss = 0.95477023\n",
      "Iteration 581, loss = 0.95436946\n",
      "Iteration 582, loss = 0.95396826\n",
      "Iteration 583, loss = 0.95356661\n",
      "Iteration 584, loss = 0.95316452\n",
      "Iteration 585, loss = 0.95276200\n",
      "Iteration 586, loss = 0.95235903\n",
      "Iteration 587, loss = 0.95195563\n",
      "Iteration 588, loss = 0.95155179\n",
      "Iteration 589, loss = 0.95114752\n",
      "Iteration 590, loss = 0.95074281\n",
      "Iteration 591, loss = 0.95033766\n",
      "Iteration 592, loss = 0.94993209\n",
      "Iteration 593, loss = 0.94952608\n",
      "Iteration 594, loss = 0.94911964\n",
      "Iteration 595, loss = 0.94871277\n",
      "Iteration 596, loss = 0.94830547\n",
      "Iteration 597, loss = 0.94789774\n",
      "Iteration 598, loss = 0.94748958\n",
      "Iteration 599, loss = 0.94708100\n",
      "Iteration 600, loss = 0.94667200\n",
      "Iteration 601, loss = 0.94626257\n",
      "Iteration 602, loss = 0.94585272\n",
      "Iteration 603, loss = 0.94544244\n",
      "Iteration 604, loss = 0.94503175\n",
      "Iteration 605, loss = 0.94462064\n",
      "Iteration 606, loss = 0.94420912\n",
      "Iteration 607, loss = 0.94379717\n",
      "Iteration 608, loss = 0.94338482\n",
      "Iteration 609, loss = 0.94297205\n",
      "Iteration 610, loss = 0.94255887\n",
      "Iteration 611, loss = 0.94214528\n",
      "Iteration 612, loss = 0.94173128\n",
      "Iteration 613, loss = 0.94131687\n",
      "Iteration 614, loss = 0.94090206\n",
      "Iteration 615, loss = 0.94048684\n",
      "Iteration 616, loss = 0.94007122\n",
      "Iteration 617, loss = 0.93965520\n",
      "Iteration 618, loss = 0.93923879\n",
      "Iteration 619, loss = 0.93882197\n",
      "Iteration 620, loss = 0.93840476\n",
      "Iteration 621, loss = 0.93798715\n",
      "Iteration 622, loss = 0.93756916\n",
      "Iteration 623, loss = 0.93715077\n",
      "Iteration 624, loss = 0.93673199\n",
      "Iteration 625, loss = 0.93631283\n",
      "Iteration 626, loss = 0.93589328\n",
      "Iteration 627, loss = 0.93547334\n",
      "Iteration 628, loss = 0.93505303\n",
      "Iteration 629, loss = 0.93463234\n",
      "Iteration 630, loss = 0.93421126\n",
      "Iteration 631, loss = 0.93378982\n",
      "Iteration 632, loss = 0.93336799\n",
      "Iteration 633, loss = 0.93294580\n",
      "Iteration 634, loss = 0.93252323\n",
      "Iteration 635, loss = 0.93210030\n",
      "Iteration 636, loss = 0.93167700\n",
      "Iteration 637, loss = 0.93125334\n",
      "Iteration 638, loss = 0.93082932\n",
      "Iteration 639, loss = 0.93040493\n",
      "Iteration 640, loss = 0.92998019\n",
      "Iteration 641, loss = 0.92955509\n",
      "Iteration 642, loss = 0.92912964\n",
      "Iteration 643, loss = 0.92870383\n",
      "Iteration 644, loss = 0.92827768\n",
      "Iteration 645, loss = 0.92785118\n",
      "Iteration 646, loss = 0.92742433\n",
      "Iteration 647, loss = 0.92699714\n",
      "Iteration 648, loss = 0.92656961\n",
      "Iteration 649, loss = 0.92614174\n",
      "Iteration 650, loss = 0.92571354\n",
      "Iteration 651, loss = 0.92528500\n",
      "Iteration 652, loss = 0.92485613\n",
      "Iteration 653, loss = 0.92442692\n",
      "Iteration 654, loss = 0.92399740\n",
      "Iteration 655, loss = 0.92356754\n",
      "Iteration 656, loss = 0.92313737\n",
      "Iteration 657, loss = 0.92270687\n",
      "Iteration 658, loss = 0.92227606\n",
      "Iteration 659, loss = 0.92184493\n",
      "Iteration 660, loss = 0.92141348\n",
      "Iteration 661, loss = 0.92098173\n",
      "Iteration 662, loss = 0.92054967\n",
      "Iteration 663, loss = 0.92011730\n",
      "Iteration 664, loss = 0.91968462\n",
      "Iteration 665, loss = 0.91925165\n",
      "Iteration 666, loss = 0.91881838\n",
      "Iteration 667, loss = 0.91838481\n",
      "Iteration 668, loss = 0.91795095\n",
      "Iteration 669, loss = 0.91751679\n",
      "Iteration 670, loss = 0.91708235\n",
      "Iteration 671, loss = 0.91664762\n",
      "Iteration 672, loss = 0.91621260\n",
      "Iteration 673, loss = 0.91577731\n",
      "Iteration 674, loss = 0.91534173\n",
      "Iteration 675, loss = 0.91490588\n",
      "Iteration 676, loss = 0.91446976\n",
      "Iteration 677, loss = 0.91403336\n",
      "Iteration 678, loss = 0.91359670\n",
      "Iteration 679, loss = 0.91315977\n",
      "Iteration 680, loss = 0.91272257\n",
      "Iteration 681, loss = 0.91228512\n",
      "Iteration 682, loss = 0.91184740\n",
      "Iteration 683, loss = 0.91140944\n",
      "Iteration 684, loss = 0.91097121\n",
      "Iteration 685, loss = 0.91053274\n",
      "Iteration 686, loss = 0.91009402\n",
      "Iteration 687, loss = 0.90965505\n",
      "Iteration 688, loss = 0.90921585\n",
      "Iteration 689, loss = 0.90877640\n",
      "Iteration 690, loss = 0.90833671\n",
      "Iteration 691, loss = 0.90789679\n",
      "Iteration 692, loss = 0.90745664\n",
      "Iteration 693, loss = 0.90701626\n",
      "Iteration 694, loss = 0.90657565\n",
      "Iteration 695, loss = 0.90613482\n",
      "Iteration 696, loss = 0.90569377\n",
      "Iteration 697, loss = 0.90525250\n",
      "Iteration 698, loss = 0.90481101\n",
      "Iteration 699, loss = 0.90436931\n",
      "Iteration 700, loss = 0.90392740\n",
      "Iteration 701, loss = 0.90348529\n",
      "Iteration 702, loss = 0.90304296\n",
      "Iteration 703, loss = 0.90260044\n",
      "Iteration 704, loss = 0.90215772\n",
      "Iteration 705, loss = 0.90171480\n",
      "Iteration 706, loss = 0.90127169\n",
      "Iteration 707, loss = 0.90082839\n",
      "Iteration 708, loss = 0.90038490\n",
      "Iteration 709, loss = 0.89994122\n",
      "Iteration 710, loss = 0.89949736\n",
      "Iteration 711, loss = 0.89905332\n",
      "Iteration 712, loss = 0.89860911\n",
      "Iteration 713, loss = 0.89816472\n",
      "Iteration 714, loss = 0.89772016\n",
      "Iteration 715, loss = 0.89727543\n",
      "Iteration 716, loss = 0.89683054\n",
      "Iteration 717, loss = 0.89638548\n",
      "Iteration 718, loss = 0.89594027\n",
      "Iteration 719, loss = 0.89549489\n",
      "Iteration 720, loss = 0.89504937\n",
      "Iteration 721, loss = 0.89460369\n",
      "Iteration 722, loss = 0.89415786\n",
      "Iteration 723, loss = 0.89371189\n",
      "Iteration 724, loss = 0.89326577\n",
      "Iteration 725, loss = 0.89281952\n",
      "Iteration 726, loss = 0.89237313\n",
      "Iteration 727, loss = 0.89192660\n",
      "Iteration 728, loss = 0.89147994\n",
      "Iteration 729, loss = 0.89103316\n",
      "Iteration 730, loss = 0.89058625\n",
      "Iteration 731, loss = 0.89013921\n",
      "Iteration 732, loss = 0.88969206\n",
      "Iteration 733, loss = 0.88924479\n",
      "Iteration 734, loss = 0.88879740\n",
      "Iteration 735, loss = 0.88834991\n",
      "Iteration 736, loss = 0.88790230\n",
      "Iteration 737, loss = 0.88745460\n",
      "Iteration 738, loss = 0.88700678\n",
      "Iteration 739, loss = 0.88655887\n",
      "Iteration 740, loss = 0.88611087\n",
      "Iteration 741, loss = 0.88566276\n",
      "Iteration 742, loss = 0.88521457\n",
      "Iteration 743, loss = 0.88476629\n",
      "Iteration 744, loss = 0.88431793\n",
      "Iteration 745, loss = 0.88386948\n",
      "Iteration 746, loss = 0.88342095\n",
      "Iteration 747, loss = 0.88297235\n",
      "Iteration 748, loss = 0.88252367\n",
      "Iteration 749, loss = 0.88207493\n",
      "Iteration 750, loss = 0.88162611\n",
      "Iteration 751, loss = 0.88117724\n",
      "Iteration 752, loss = 0.88072830\n",
      "Iteration 753, loss = 0.88027930\n",
      "Iteration 754, loss = 0.87983024\n",
      "Iteration 755, loss = 0.87938113\n",
      "Iteration 756, loss = 0.87893198\n",
      "Iteration 757, loss = 0.87848277\n",
      "Iteration 758, loss = 0.87803352\n",
      "Iteration 759, loss = 0.87758423\n",
      "Iteration 760, loss = 0.87713490\n",
      "Iteration 761, loss = 0.87668554\n",
      "Iteration 762, loss = 0.87623614\n",
      "Iteration 763, loss = 0.87578671\n",
      "Iteration 764, loss = 0.87533726\n",
      "Iteration 765, loss = 0.87488778\n",
      "Iteration 766, loss = 0.87443828\n",
      "Iteration 767, loss = 0.87398877\n",
      "Iteration 768, loss = 0.87353923\n",
      "Iteration 769, loss = 0.87308969\n",
      "Iteration 770, loss = 0.87264014\n",
      "Iteration 771, loss = 0.87219058\n",
      "Iteration 772, loss = 0.87174101\n",
      "Iteration 773, loss = 0.87129145\n",
      "Iteration 774, loss = 0.87084189\n",
      "Iteration 775, loss = 0.87039233\n",
      "Iteration 776, loss = 0.86994278\n",
      "Iteration 777, loss = 0.86949324\n",
      "Iteration 778, loss = 0.86904372\n",
      "Iteration 779, loss = 0.86859421\n",
      "Iteration 780, loss = 0.86814472\n",
      "Iteration 781, loss = 0.86769526\n",
      "Iteration 782, loss = 0.86724582\n",
      "Iteration 783, loss = 0.86679640\n",
      "Iteration 784, loss = 0.86634702\n",
      "Iteration 785, loss = 0.86589767\n",
      "Iteration 786, loss = 0.86544836\n",
      "Iteration 787, loss = 0.86499909\n",
      "Iteration 788, loss = 0.86454986\n",
      "Iteration 789, loss = 0.86410067\n",
      "Iteration 790, loss = 0.86365154\n",
      "Iteration 791, loss = 0.86320245\n",
      "Iteration 792, loss = 0.86275342\n",
      "Iteration 793, loss = 0.86230444\n",
      "Iteration 794, loss = 0.86185553\n",
      "Iteration 795, loss = 0.86140667\n",
      "Iteration 796, loss = 0.86095788\n",
      "Iteration 797, loss = 0.86050916\n",
      "Iteration 798, loss = 0.86006051\n",
      "Iteration 799, loss = 0.85961193\n",
      "Iteration 800, loss = 0.85916343\n",
      "Iteration 801, loss = 0.85871500\n",
      "Iteration 802, loss = 0.85826666\n",
      "Iteration 803, loss = 0.85781840\n",
      "Iteration 804, loss = 0.85737023\n",
      "Iteration 805, loss = 0.85692215\n",
      "Iteration 806, loss = 0.85647416\n",
      "Iteration 807, loss = 0.85602627\n",
      "Iteration 808, loss = 0.85557847\n",
      "Iteration 809, loss = 0.85513078\n",
      "Iteration 810, loss = 0.85468319\n",
      "Iteration 811, loss = 0.85423570\n",
      "Iteration 812, loss = 0.85378833\n",
      "Iteration 813, loss = 0.85334106\n",
      "Iteration 814, loss = 0.85289391\n",
      "Iteration 815, loss = 0.85244688\n",
      "Iteration 816, loss = 0.85199996\n",
      "Iteration 817, loss = 0.85155317\n",
      "Iteration 818, loss = 0.85110651\n",
      "Iteration 819, loss = 0.85065997\n",
      "Iteration 820, loss = 0.85021356\n",
      "Iteration 821, loss = 0.84976728\n",
      "Iteration 822, loss = 0.84932114\n",
      "Iteration 823, loss = 0.84887514\n",
      "Iteration 824, loss = 0.84842928\n",
      "Iteration 825, loss = 0.84798356\n",
      "Iteration 826, loss = 0.84753798\n",
      "Iteration 827, loss = 0.84709256\n",
      "Iteration 828, loss = 0.84664728\n",
      "Iteration 829, loss = 0.84620216\n",
      "Iteration 830, loss = 0.84575720\n",
      "Iteration 831, loss = 0.84531239\n",
      "Iteration 832, loss = 0.84486775\n",
      "Iteration 833, loss = 0.84442327\n",
      "Iteration 834, loss = 0.84397895\n",
      "Iteration 835, loss = 0.84353480\n",
      "Iteration 836, loss = 0.84309083\n",
      "Iteration 837, loss = 0.84264703\n",
      "Iteration 838, loss = 0.84220340\n",
      "Iteration 839, loss = 0.84175995\n",
      "Iteration 840, loss = 0.84131669\n",
      "Iteration 841, loss = 0.84087360\n",
      "Iteration 842, loss = 0.84043071\n",
      "Iteration 843, loss = 0.83998800\n",
      "Iteration 844, loss = 0.83954548\n",
      "Iteration 845, loss = 0.83910315\n",
      "Iteration 846, loss = 0.83866102\n",
      "Iteration 847, loss = 0.83821909\n",
      "Iteration 848, loss = 0.83777736\n",
      "Iteration 849, loss = 0.83733583\n",
      "Iteration 850, loss = 0.83689450\n",
      "Iteration 851, loss = 0.83645338\n",
      "Iteration 852, loss = 0.83601247\n",
      "Iteration 853, loss = 0.83557178\n",
      "Iteration 854, loss = 0.83513129\n",
      "Iteration 855, loss = 0.83469102\n",
      "Iteration 856, loss = 0.83425098\n",
      "Iteration 857, loss = 0.83381115\n",
      "Iteration 858, loss = 0.83337154\n",
      "Iteration 859, loss = 0.83293216\n",
      "Iteration 860, loss = 0.83249301\n",
      "Iteration 861, loss = 0.83205409\n",
      "Iteration 862, loss = 0.83161540\n",
      "Iteration 863, loss = 0.83117694\n",
      "Iteration 864, loss = 0.83073872\n",
      "Iteration 865, loss = 0.83030073\n",
      "Iteration 866, loss = 0.82986299\n",
      "Iteration 867, loss = 0.82942549\n",
      "Iteration 868, loss = 0.82898823\n",
      "Iteration 869, loss = 0.82855123\n",
      "Iteration 870, loss = 0.82811447\n",
      "Iteration 871, loss = 0.82767796\n",
      "Iteration 872, loss = 0.82724170\n",
      "Iteration 873, loss = 0.82680570\n",
      "Iteration 874, loss = 0.82636995\n",
      "Iteration 875, loss = 0.82593447\n",
      "Iteration 876, loss = 0.82549924\n",
      "Iteration 877, loss = 0.82506428\n",
      "Iteration 878, loss = 0.82462959\n",
      "Iteration 879, loss = 0.82419516\n",
      "Iteration 880, loss = 0.82376100\n",
      "Iteration 881, loss = 0.82332711\n",
      "Iteration 882, loss = 0.82289349\n",
      "Iteration 883, loss = 0.82246015\n",
      "Iteration 884, loss = 0.82202708\n",
      "Iteration 885, loss = 0.82159429\n",
      "Iteration 886, loss = 0.82116179\n",
      "Iteration 887, loss = 0.82072956\n",
      "Iteration 888, loss = 0.82029762\n",
      "Iteration 889, loss = 0.81986596\n",
      "Iteration 890, loss = 0.81943460\n",
      "Iteration 891, loss = 0.81900352\n",
      "Iteration 892, loss = 0.81857273\n",
      "Iteration 893, loss = 0.81814224\n",
      "Iteration 894, loss = 0.81771204\n",
      "Iteration 895, loss = 0.81728213\n",
      "Iteration 896, loss = 0.81685253\n",
      "Iteration 897, loss = 0.81642323\n",
      "Iteration 898, loss = 0.81599422\n",
      "Iteration 899, loss = 0.81556552\n",
      "Iteration 900, loss = 0.81513713\n",
      "Iteration 901, loss = 0.81470904\n",
      "Iteration 902, loss = 0.81428126\n",
      "Iteration 903, loss = 0.81385379\n",
      "Iteration 904, loss = 0.81342663\n",
      "Iteration 905, loss = 0.81299979\n",
      "Iteration 906, loss = 0.81257326\n",
      "Iteration 907, loss = 0.81214705\n",
      "Iteration 908, loss = 0.81172115\n",
      "Iteration 909, loss = 0.81129557\n",
      "Iteration 910, loss = 0.81087032\n",
      "Iteration 911, loss = 0.81044539\n",
      "Iteration 912, loss = 0.81002078\n",
      "Iteration 913, loss = 0.80959650\n",
      "Iteration 914, loss = 0.80917254\n",
      "Iteration 915, loss = 0.80874891\n",
      "Iteration 916, loss = 0.80832561\n",
      "Iteration 917, loss = 0.80790265\n",
      "Iteration 918, loss = 0.80748002\n",
      "Iteration 919, loss = 0.80705772\n",
      "Iteration 920, loss = 0.80663575\n",
      "Iteration 921, loss = 0.80621413\n",
      "Iteration 922, loss = 0.80579284\n",
      "Iteration 923, loss = 0.80537189\n",
      "Iteration 924, loss = 0.80495129\n",
      "Iteration 925, loss = 0.80453102\n",
      "Iteration 926, loss = 0.80411110\n",
      "Iteration 927, loss = 0.80369153\n",
      "Iteration 928, loss = 0.80327230\n",
      "Iteration 929, loss = 0.80285342\n",
      "Iteration 930, loss = 0.80243489\n",
      "Iteration 931, loss = 0.80201671\n",
      "Iteration 932, loss = 0.80159888\n",
      "Iteration 933, loss = 0.80118140\n",
      "Iteration 934, loss = 0.80076428\n",
      "Iteration 935, loss = 0.80034751\n",
      "Iteration 936, loss = 0.79993110\n",
      "Iteration 937, loss = 0.79951504\n",
      "Iteration 938, loss = 0.79909935\n",
      "Iteration 939, loss = 0.79868401\n",
      "Iteration 940, loss = 0.79826904\n",
      "Iteration 941, loss = 0.79785442\n",
      "Iteration 942, loss = 0.79744018\n",
      "Iteration 943, loss = 0.79702629\n",
      "Iteration 944, loss = 0.79661277\n",
      "Iteration 945, loss = 0.79619962\n",
      "Iteration 946, loss = 0.79578683\n",
      "Iteration 947, loss = 0.79537442\n",
      "Iteration 948, loss = 0.79496237\n",
      "Iteration 949, loss = 0.79455070\n",
      "Iteration 950, loss = 0.79413939\n",
      "Iteration 951, loss = 0.79372846\n",
      "Iteration 952, loss = 0.79331790\n",
      "Iteration 953, loss = 0.79290772\n",
      "Iteration 954, loss = 0.79249791\n",
      "Iteration 955, loss = 0.79208848\n",
      "Iteration 956, loss = 0.79167943\n",
      "Iteration 957, loss = 0.79127076\n",
      "Iteration 958, loss = 0.79086246\n",
      "Iteration 959, loss = 0.79045455\n",
      "Iteration 960, loss = 0.79004702\n",
      "Iteration 961, loss = 0.78963987\n",
      "Iteration 962, loss = 0.78923310\n",
      "Iteration 963, loss = 0.78882672\n",
      "Iteration 964, loss = 0.78842072\n",
      "Iteration 965, loss = 0.78801511\n",
      "Iteration 966, loss = 0.78760988\n",
      "Iteration 967, loss = 0.78720504\n",
      "Iteration 968, loss = 0.78680059\n",
      "Iteration 969, loss = 0.78639653\n",
      "Iteration 970, loss = 0.78599286\n",
      "Iteration 971, loss = 0.78558958\n",
      "Iteration 972, loss = 0.78518669\n",
      "Iteration 973, loss = 0.78478419\n",
      "Iteration 974, loss = 0.78438209\n",
      "Iteration 975, loss = 0.78398038\n",
      "Iteration 976, loss = 0.78357906\n",
      "Iteration 977, loss = 0.78317814\n",
      "Iteration 978, loss = 0.78277762\n",
      "Iteration 979, loss = 0.78237749\n",
      "Iteration 980, loss = 0.78197775\n",
      "Iteration 981, loss = 0.78157842\n",
      "Iteration 982, loss = 0.78117949\n",
      "Iteration 983, loss = 0.78078095\n",
      "Iteration 984, loss = 0.78038281\n",
      "Iteration 985, loss = 0.77998508\n",
      "Iteration 986, loss = 0.77958774\n",
      "Iteration 987, loss = 0.77919081\n",
      "Iteration 988, loss = 0.77879428\n",
      "Iteration 989, loss = 0.77839815\n",
      "Iteration 990, loss = 0.77800243\n",
      "Iteration 991, loss = 0.77760711\n",
      "Iteration 992, loss = 0.77721219\n",
      "Iteration 993, loss = 0.77681768\n",
      "Iteration 994, loss = 0.77642358\n",
      "Iteration 995, loss = 0.77602988\n",
      "Iteration 996, loss = 0.77563659\n",
      "Iteration 997, loss = 0.77524371\n",
      "Iteration 998, loss = 0.77485123\n",
      "Iteration 999, loss = 0.77445917\n",
      "Iteration 1000, loss = 0.77406751\n",
      "Iteration 1, loss = 1.12503461\n",
      "Iteration 2, loss = 1.12493217\n",
      "Iteration 3, loss = 1.12478636\n",
      "Iteration 4, loss = 1.12460185\n",
      "Iteration 5, loss = 1.12438289\n",
      "Iteration 6, loss = 1.12413334\n",
      "Iteration 7, loss = 1.12385673\n",
      "Iteration 8, loss = 1.12355626\n",
      "Iteration 9, loss = 1.12323483\n",
      "Iteration 10, loss = 1.12289507\n",
      "Iteration 11, loss = 1.12253934\n",
      "Iteration 12, loss = 1.12216980\n",
      "Iteration 13, loss = 1.12178837\n",
      "Iteration 14, loss = 1.12139680\n",
      "Iteration 15, loss = 1.12099665\n",
      "Iteration 16, loss = 1.12058934\n",
      "Iteration 17, loss = 1.12017612\n",
      "Iteration 18, loss = 1.11975814\n",
      "Iteration 19, loss = 1.11933641\n",
      "Iteration 20, loss = 1.11891184\n",
      "Iteration 21, loss = 1.11848523\n",
      "Iteration 22, loss = 1.11805731\n",
      "Iteration 23, loss = 1.11762873\n",
      "Iteration 24, loss = 1.11720007\n",
      "Iteration 25, loss = 1.11677182\n",
      "Iteration 26, loss = 1.11634444\n",
      "Iteration 27, loss = 1.11591833\n",
      "Iteration 28, loss = 1.11549383\n",
      "Iteration 29, loss = 1.11507127\n",
      "Iteration 30, loss = 1.11465090\n",
      "Iteration 31, loss = 1.11423297\n",
      "Iteration 32, loss = 1.11381768\n",
      "Iteration 33, loss = 1.11340521\n",
      "Iteration 34, loss = 1.11299570\n",
      "Iteration 35, loss = 1.11258930\n",
      "Iteration 36, loss = 1.11218611\n",
      "Iteration 37, loss = 1.11178623\n",
      "Iteration 38, loss = 1.11138973\n",
      "Iteration 39, loss = 1.11099668\n",
      "Iteration 40, loss = 1.11060712\n",
      "Iteration 41, loss = 1.11022110\n",
      "Iteration 42, loss = 1.10983864\n",
      "Iteration 43, loss = 1.10945976\n",
      "Iteration 44, loss = 1.10908448\n",
      "Iteration 45, loss = 1.10871279\n",
      "Iteration 46, loss = 1.10834470\n",
      "Iteration 47, loss = 1.10798020\n",
      "Iteration 48, loss = 1.10761927\n",
      "Iteration 49, loss = 1.10726190\n",
      "Iteration 50, loss = 1.10690807\n",
      "Iteration 51, loss = 1.10655775\n",
      "Iteration 52, loss = 1.10621092\n",
      "Iteration 53, loss = 1.10586754\n",
      "Iteration 54, loss = 1.10552759\n",
      "Iteration 55, loss = 1.10519102\n",
      "Iteration 56, loss = 1.10485781\n",
      "Iteration 57, loss = 1.10452792\n",
      "Iteration 58, loss = 1.10420130\n",
      "Iteration 59, loss = 1.10387792\n",
      "Iteration 60, loss = 1.10355775\n",
      "Iteration 61, loss = 1.10324073\n",
      "Iteration 62, loss = 1.10292682\n",
      "Iteration 63, loss = 1.10261599\n",
      "Iteration 64, loss = 1.10230820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhruv/indexCode/cmpsc445/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 65, loss = 1.10200339\n",
      "Iteration 66, loss = 1.10170153\n",
      "Iteration 67, loss = 1.10140257\n",
      "Iteration 68, loss = 1.10110648\n",
      "Iteration 69, loss = 1.10081321\n",
      "Iteration 70, loss = 1.10052272\n",
      "Iteration 71, loss = 1.10023496\n",
      "Iteration 72, loss = 1.09994990\n",
      "Iteration 73, loss = 1.09966749\n",
      "Iteration 74, loss = 1.09938769\n",
      "Iteration 75, loss = 1.09911046\n",
      "Iteration 76, loss = 1.09883576\n",
      "Iteration 77, loss = 1.09856355\n",
      "Iteration 78, loss = 1.09829379\n",
      "Iteration 79, loss = 1.09802644\n",
      "Iteration 80, loss = 1.09776147\n",
      "Iteration 81, loss = 1.09749882\n",
      "Iteration 82, loss = 1.09723847\n",
      "Iteration 83, loss = 1.09698038\n",
      "Iteration 84, loss = 1.09672451\n",
      "Iteration 85, loss = 1.09647082\n",
      "Iteration 86, loss = 1.09621928\n",
      "Iteration 87, loss = 1.09596985\n",
      "Iteration 88, loss = 1.09572249\n",
      "Iteration 89, loss = 1.09547718\n",
      "Iteration 90, loss = 1.09523387\n",
      "Iteration 91, loss = 1.09499253\n",
      "Iteration 92, loss = 1.09475313\n",
      "Iteration 93, loss = 1.09451564\n",
      "Iteration 94, loss = 1.09428002\n",
      "Iteration 95, loss = 1.09404624\n",
      "Iteration 96, loss = 1.09381427\n",
      "Iteration 97, loss = 1.09358408\n",
      "Iteration 98, loss = 1.09335564\n",
      "Iteration 99, loss = 1.09312891\n",
      "Iteration 100, loss = 1.09290386\n",
      "Iteration 101, loss = 1.09268047\n",
      "Iteration 102, loss = 1.09245872\n",
      "Iteration 103, loss = 1.09223855\n",
      "Iteration 104, loss = 1.09201996\n",
      "Iteration 105, loss = 1.09180291\n",
      "Iteration 106, loss = 1.09158738\n",
      "Iteration 107, loss = 1.09137333\n",
      "Iteration 108, loss = 1.09116074\n",
      "Iteration 109, loss = 1.09094959\n",
      "Iteration 110, loss = 1.09073985\n",
      "Iteration 111, loss = 1.09053148\n",
      "Iteration 112, loss = 1.09032448\n",
      "Iteration 113, loss = 1.09011881\n",
      "Iteration 114, loss = 1.08991444\n",
      "Iteration 115, loss = 1.08971136\n",
      "Iteration 116, loss = 1.08950953\n",
      "Iteration 117, loss = 1.08930895\n",
      "Iteration 118, loss = 1.08910957\n",
      "Iteration 119, loss = 1.08891138\n",
      "Iteration 120, loss = 1.08871437\n",
      "Iteration 121, loss = 1.08851849\n",
      "Iteration 122, loss = 1.08832374\n",
      "Iteration 123, loss = 1.08813009\n",
      "Iteration 124, loss = 1.08793752\n",
      "Iteration 125, loss = 1.08774601\n",
      "Iteration 126, loss = 1.08755554\n",
      "Iteration 127, loss = 1.08736608\n",
      "Iteration 128, loss = 1.08717762\n",
      "Iteration 129, loss = 1.08699014\n",
      "Iteration 130, loss = 1.08680362\n",
      "Iteration 131, loss = 1.08661803\n",
      "Iteration 132, loss = 1.08643337\n",
      "Iteration 133, loss = 1.08624961\n",
      "Iteration 134, loss = 1.08606672\n",
      "Iteration 135, loss = 1.08588471\n",
      "Iteration 136, loss = 1.08570354\n",
      "Iteration 137, loss = 1.08552319\n",
      "Iteration 138, loss = 1.08534366\n",
      "Iteration 139, loss = 1.08516492\n",
      "Iteration 140, loss = 1.08498696\n",
      "Iteration 141, loss = 1.08480976\n",
      "Iteration 142, loss = 1.08463330\n",
      "Iteration 143, loss = 1.08445757\n",
      "Iteration 144, loss = 1.08428254\n",
      "Iteration 145, loss = 1.08410822\n",
      "Iteration 146, loss = 1.08393457\n",
      "Iteration 147, loss = 1.08376158\n",
      "Iteration 148, loss = 1.08358925\n",
      "Iteration 149, loss = 1.08341754\n",
      "Iteration 150, loss = 1.08324646\n",
      "Iteration 151, loss = 1.08307598\n",
      "Iteration 152, loss = 1.08290608\n",
      "Iteration 153, loss = 1.08273677\n",
      "Iteration 154, loss = 1.08256801\n",
      "Iteration 155, loss = 1.08239980\n",
      "Iteration 156, loss = 1.08223212\n",
      "Iteration 157, loss = 1.08206497\n",
      "Iteration 158, loss = 1.08189832\n",
      "Iteration 159, loss = 1.08173216\n",
      "Iteration 160, loss = 1.08156648\n",
      "Iteration 161, loss = 1.08140128\n",
      "Iteration 162, loss = 1.08123652\n",
      "Iteration 163, loss = 1.08107221\n",
      "Iteration 164, loss = 1.08090833\n",
      "Iteration 165, loss = 1.08074486\n",
      "Iteration 166, loss = 1.08058180\n",
      "Iteration 167, loss = 1.08041914\n",
      "Iteration 168, loss = 1.08025686\n",
      "Iteration 169, loss = 1.08009494\n",
      "Iteration 170, loss = 1.07993339\n",
      "Iteration 171, loss = 1.07977219\n",
      "Iteration 172, loss = 1.07961132\n",
      "Iteration 173, loss = 1.07945078\n",
      "Iteration 174, loss = 1.07929055\n",
      "Iteration 175, loss = 1.07913063\n",
      "Iteration 176, loss = 1.07897100\n",
      "Iteration 177, loss = 1.07881165\n",
      "Iteration 178, loss = 1.07865258\n",
      "Iteration 179, loss = 1.07849377\n",
      "Iteration 180, loss = 1.07833521\n",
      "Iteration 181, loss = 1.07817689\n",
      "Iteration 182, loss = 1.07801881\n",
      "Iteration 183, loss = 1.07786095\n",
      "Iteration 184, loss = 1.07770331\n",
      "Iteration 185, loss = 1.07754587\n",
      "Iteration 186, loss = 1.07738862\n",
      "Iteration 187, loss = 1.07723156\n",
      "Iteration 188, loss = 1.07707467\n",
      "Iteration 189, loss = 1.07691796\n",
      "Iteration 190, loss = 1.07676140\n",
      "Iteration 191, loss = 1.07660499\n",
      "Iteration 192, loss = 1.07644872\n",
      "Iteration 193, loss = 1.07629259\n",
      "Iteration 194, loss = 1.07613658\n",
      "Iteration 195, loss = 1.07598068\n",
      "Iteration 196, loss = 1.07582489\n",
      "Iteration 197, loss = 1.07566920\n",
      "Iteration 198, loss = 1.07551360\n",
      "Iteration 199, loss = 1.07535809\n",
      "Iteration 200, loss = 1.07520264\n",
      "Iteration 201, loss = 1.07504727\n",
      "Iteration 202, loss = 1.07489196\n",
      "Iteration 203, loss = 1.07473669\n",
      "Iteration 204, loss = 1.07458147\n",
      "Iteration 205, loss = 1.07442629\n",
      "Iteration 206, loss = 1.07427114\n",
      "Iteration 207, loss = 1.07411600\n",
      "Iteration 208, loss = 1.07396089\n",
      "Iteration 209, loss = 1.07380578\n",
      "Iteration 210, loss = 1.07365067\n",
      "Iteration 211, loss = 1.07349555\n",
      "Iteration 212, loss = 1.07334042\n",
      "Iteration 213, loss = 1.07318527\n",
      "Iteration 214, loss = 1.07303009\n",
      "Iteration 215, loss = 1.07287488\n",
      "Iteration 216, loss = 1.07271963\n",
      "Iteration 217, loss = 1.07256433\n",
      "Iteration 218, loss = 1.07240897\n",
      "Iteration 219, loss = 1.07225356\n",
      "Iteration 220, loss = 1.07209807\n",
      "Iteration 221, loss = 1.07194252\n",
      "Iteration 222, loss = 1.07178689\n",
      "Iteration 223, loss = 1.07163117\n",
      "Iteration 224, loss = 1.07147535\n",
      "Iteration 225, loss = 1.07131945\n",
      "Iteration 226, loss = 1.07116343\n",
      "Iteration 227, loss = 1.07100731\n",
      "Iteration 228, loss = 1.07085107\n",
      "Iteration 229, loss = 1.07069471\n",
      "Iteration 230, loss = 1.07053823\n",
      "Iteration 231, loss = 1.07038161\n",
      "Iteration 232, loss = 1.07022485\n",
      "Iteration 233, loss = 1.07006795\n",
      "Iteration 234, loss = 1.06991089\n",
      "Iteration 235, loss = 1.06975369\n",
      "Iteration 236, loss = 1.06959632\n",
      "Iteration 237, loss = 1.06943878\n",
      "Iteration 238, loss = 1.06928108\n",
      "Iteration 239, loss = 1.06912320\n",
      "Iteration 240, loss = 1.06896513\n",
      "Iteration 241, loss = 1.06880688\n",
      "Iteration 242, loss = 1.06864843\n",
      "Iteration 243, loss = 1.06848979\n",
      "Iteration 244, loss = 1.06833095\n",
      "Iteration 245, loss = 1.06817189\n",
      "Iteration 246, loss = 1.06801263\n",
      "Iteration 247, loss = 1.06785315\n",
      "Iteration 248, loss = 1.06769344\n",
      "Iteration 249, loss = 1.06753351\n",
      "Iteration 250, loss = 1.06737335\n",
      "Iteration 251, loss = 1.06721294\n",
      "Iteration 252, loss = 1.06705230\n",
      "Iteration 253, loss = 1.06689141\n",
      "Iteration 254, loss = 1.06673027\n",
      "Iteration 255, loss = 1.06656888\n",
      "Iteration 256, loss = 1.06640722\n",
      "Iteration 257, loss = 1.06624530\n",
      "Iteration 258, loss = 1.06608311\n",
      "Iteration 259, loss = 1.06592065\n",
      "Iteration 260, loss = 1.06575790\n",
      "Iteration 261, loss = 1.06559488\n",
      "Iteration 262, loss = 1.06543157\n",
      "Iteration 263, loss = 1.06526797\n",
      "Iteration 264, loss = 1.06510407\n",
      "Iteration 265, loss = 1.06493987\n",
      "Iteration 266, loss = 1.06477537\n",
      "Iteration 267, loss = 1.06461056\n",
      "Iteration 268, loss = 1.06444544\n",
      "Iteration 269, loss = 1.06428001\n",
      "Iteration 270, loss = 1.06411425\n",
      "Iteration 271, loss = 1.06394817\n",
      "Iteration 272, loss = 1.06378176\n",
      "Iteration 273, loss = 1.06361501\n",
      "Iteration 274, loss = 1.06344793\n",
      "Iteration 275, loss = 1.06328051\n",
      "Iteration 276, loss = 1.06311275\n",
      "Iteration 277, loss = 1.06294464\n",
      "Iteration 278, loss = 1.06277617\n",
      "Iteration 279, loss = 1.06260735\n",
      "Iteration 280, loss = 1.06243817\n",
      "Iteration 281, loss = 1.06226863\n",
      "Iteration 282, loss = 1.06209872\n",
      "Iteration 283, loss = 1.06192844\n",
      "Iteration 284, loss = 1.06175779\n",
      "Iteration 285, loss = 1.06158675\n",
      "Iteration 286, loss = 1.06141534\n",
      "Iteration 287, loss = 1.06124354\n",
      "Iteration 288, loss = 1.06107135\n",
      "Iteration 289, loss = 1.06089877\n",
      "Iteration 290, loss = 1.06072579\n",
      "Iteration 291, loss = 1.06055241\n",
      "Iteration 292, loss = 1.06037864\n",
      "Iteration 293, loss = 1.06020445\n",
      "Iteration 294, loss = 1.06002985\n",
      "Iteration 295, loss = 1.05985484\n",
      "Iteration 296, loss = 1.05967942\n",
      "Iteration 297, loss = 1.05950357\n",
      "Iteration 298, loss = 1.05932730\n",
      "Iteration 299, loss = 1.05915061\n",
      "Iteration 300, loss = 1.05897348\n",
      "Iteration 301, loss = 1.05879592\n",
      "Iteration 302, loss = 1.05861793\n",
      "Iteration 303, loss = 1.05843949\n",
      "Iteration 304, loss = 1.05826062\n",
      "Iteration 305, loss = 1.05808129\n",
      "Iteration 306, loss = 1.05790152\n",
      "Iteration 307, loss = 1.05772130\n",
      "Iteration 308, loss = 1.05754061\n",
      "Iteration 309, loss = 1.05735948\n",
      "Iteration 310, loss = 1.05717788\n",
      "Iteration 311, loss = 1.05699581\n",
      "Iteration 312, loss = 1.05681328\n",
      "Iteration 313, loss = 1.05663027\n",
      "Iteration 314, loss = 1.05644680\n",
      "Iteration 315, loss = 1.05626284\n",
      "Iteration 316, loss = 1.05607841\n",
      "Iteration 317, loss = 1.05589349\n",
      "Iteration 318, loss = 1.05570809\n",
      "Iteration 319, loss = 1.05552220\n",
      "Iteration 320, loss = 1.05533582\n",
      "Iteration 321, loss = 1.05514895\n",
      "Iteration 322, loss = 1.05496158\n",
      "Iteration 323, loss = 1.05477371\n",
      "Iteration 324, loss = 1.05458533\n",
      "Iteration 325, loss = 1.05439645\n",
      "Iteration 326, loss = 1.05420706\n",
      "Iteration 327, loss = 1.05401717\n",
      "Iteration 328, loss = 1.05382676\n",
      "Iteration 329, loss = 1.05363583\n",
      "Iteration 330, loss = 1.05344438\n",
      "Iteration 331, loss = 1.05325241\n",
      "Iteration 332, loss = 1.05305992\n",
      "Iteration 333, loss = 1.05286690\n",
      "Iteration 334, loss = 1.05267335\n",
      "Iteration 335, loss = 1.05247927\n",
      "Iteration 336, loss = 1.05228466\n",
      "Iteration 337, loss = 1.05208950\n",
      "Iteration 338, loss = 1.05189381\n",
      "Iteration 339, loss = 1.05169758\n",
      "Iteration 340, loss = 1.05150080\n",
      "Iteration 341, loss = 1.05130347\n",
      "Iteration 342, loss = 1.05110559\n",
      "Iteration 343, loss = 1.05090717\n",
      "Iteration 344, loss = 1.05070818\n",
      "Iteration 345, loss = 1.05050864\n",
      "Iteration 346, loss = 1.05030854\n",
      "Iteration 347, loss = 1.05010788\n",
      "Iteration 348, loss = 1.04990665\n",
      "Iteration 349, loss = 1.04970486\n",
      "Iteration 350, loss = 1.04950250\n",
      "Iteration 351, loss = 1.04929957\n",
      "Iteration 352, loss = 1.04909606\n",
      "Iteration 353, loss = 1.04889198\n",
      "Iteration 354, loss = 1.04868732\n",
      "Iteration 355, loss = 1.04848208\n",
      "Iteration 356, loss = 1.04827626\n",
      "Iteration 357, loss = 1.04806985\n",
      "Iteration 358, loss = 1.04786286\n",
      "Iteration 359, loss = 1.04765527\n",
      "Iteration 360, loss = 1.04744710\n",
      "Iteration 361, loss = 1.04723833\n",
      "Iteration 362, loss = 1.04702896\n",
      "Iteration 363, loss = 1.04681900\n",
      "Iteration 364, loss = 1.04660844\n",
      "Iteration 365, loss = 1.04639728\n",
      "Iteration 366, loss = 1.04618551\n",
      "Iteration 367, loss = 1.04597314\n",
      "Iteration 368, loss = 1.04576015\n",
      "Iteration 369, loss = 1.04554656\n",
      "Iteration 370, loss = 1.04533236\n",
      "Iteration 371, loss = 1.04511754\n",
      "Iteration 372, loss = 1.04490211\n",
      "Iteration 373, loss = 1.04468606\n",
      "Iteration 374, loss = 1.04446938\n",
      "Iteration 375, loss = 1.04425209\n",
      "Iteration 376, loss = 1.04403417\n",
      "Iteration 377, loss = 1.04381563\n",
      "Iteration 378, loss = 1.04359646\n",
      "Iteration 379, loss = 1.04337666\n",
      "Iteration 380, loss = 1.04315623\n",
      "Iteration 381, loss = 1.04293517\n",
      "Iteration 382, loss = 1.04271347\n",
      "Iteration 383, loss = 1.04249113\n",
      "Iteration 384, loss = 1.04226816\n",
      "Iteration 385, loss = 1.04204455\n",
      "Iteration 386, loss = 1.04182029\n",
      "Iteration 387, loss = 1.04159539\n",
      "Iteration 388, loss = 1.04136985\n",
      "Iteration 389, loss = 1.04114366\n",
      "Iteration 390, loss = 1.04091682\n",
      "Iteration 391, loss = 1.04068933\n",
      "Iteration 392, loss = 1.04046119\n",
      "Iteration 393, loss = 1.04023240\n",
      "Iteration 394, loss = 1.04000295\n",
      "Iteration 395, loss = 1.03977284\n",
      "Iteration 396, loss = 1.03954208\n",
      "Iteration 397, loss = 1.03931065\n",
      "Iteration 398, loss = 1.03907857\n",
      "Iteration 399, loss = 1.03884582\n",
      "Iteration 400, loss = 1.03861241\n",
      "Iteration 401, loss = 1.03837833\n",
      "Iteration 402, loss = 1.03814359\n",
      "Iteration 403, loss = 1.03790817\n",
      "Iteration 404, loss = 1.03767209\n",
      "Iteration 405, loss = 1.03743534\n",
      "Iteration 406, loss = 1.03719791\n",
      "Iteration 407, loss = 1.03695981\n",
      "Iteration 408, loss = 1.03672103\n",
      "Iteration 409, loss = 1.03648158\n",
      "Iteration 410, loss = 1.03624145\n",
      "Iteration 411, loss = 1.03600063\n",
      "Iteration 412, loss = 1.03575914\n",
      "Iteration 413, loss = 1.03551697\n",
      "Iteration 414, loss = 1.03527411\n",
      "Iteration 415, loss = 1.03503057\n",
      "Iteration 416, loss = 1.03478634\n",
      "Iteration 417, loss = 1.03454143\n",
      "Iteration 418, loss = 1.03429582\n",
      "Iteration 419, loss = 1.03404953\n",
      "Iteration 420, loss = 1.03380255\n",
      "Iteration 421, loss = 1.03355488\n",
      "Iteration 422, loss = 1.03330651\n",
      "Iteration 423, loss = 1.03305745\n",
      "Iteration 424, loss = 1.03280770\n",
      "Iteration 425, loss = 1.03255724\n",
      "Iteration 426, loss = 1.03230610\n",
      "Iteration 427, loss = 1.03205425\n",
      "Iteration 428, loss = 1.03180171\n",
      "Iteration 429, loss = 1.03154846\n",
      "Iteration 430, loss = 1.03129452\n",
      "Iteration 431, loss = 1.03103987\n",
      "Iteration 432, loss = 1.03078452\n",
      "Iteration 433, loss = 1.03052847\n",
      "Iteration 434, loss = 1.03027171\n",
      "Iteration 435, loss = 1.03001425\n",
      "Iteration 436, loss = 1.02975608\n",
      "Iteration 437, loss = 1.02949721\n",
      "Iteration 438, loss = 1.02923762\n",
      "Iteration 439, loss = 1.02897733\n",
      "Iteration 440, loss = 1.02871633\n",
      "Iteration 441, loss = 1.02845461\n",
      "Iteration 442, loss = 1.02819219\n",
      "Iteration 443, loss = 1.02792906\n",
      "Iteration 444, loss = 1.02766521\n",
      "Iteration 445, loss = 1.02740065\n",
      "Iteration 446, loss = 1.02713537\n",
      "Iteration 447, loss = 1.02686938\n",
      "Iteration 448, loss = 1.02660268\n",
      "Iteration 449, loss = 1.02633526\n",
      "Iteration 450, loss = 1.02606713\n",
      "Iteration 451, loss = 1.02579827\n",
      "Iteration 452, loss = 1.02552870\n",
      "Iteration 453, loss = 1.02525841\n",
      "Iteration 454, loss = 1.02498741\n",
      "Iteration 455, loss = 1.02471568\n",
      "Iteration 456, loss = 1.02444324\n",
      "Iteration 457, loss = 1.02417007\n",
      "Iteration 458, loss = 1.02389619\n",
      "Iteration 459, loss = 1.02362158\n",
      "Iteration 460, loss = 1.02334625\n",
      "Iteration 461, loss = 1.02307020\n",
      "Iteration 462, loss = 1.02279343\n",
      "Iteration 463, loss = 1.02251594\n",
      "Iteration 464, loss = 1.02223772\n",
      "Iteration 465, loss = 1.02195878\n",
      "Iteration 466, loss = 1.02167912\n",
      "Iteration 467, loss = 1.02139873\n",
      "Iteration 468, loss = 1.02111762\n",
      "Iteration 469, loss = 1.02083579\n",
      "Iteration 470, loss = 1.02055323\n",
      "Iteration 471, loss = 1.02026995\n",
      "Iteration 472, loss = 1.01998594\n",
      "Iteration 473, loss = 1.01970120\n",
      "Iteration 474, loss = 1.01941575\n",
      "Iteration 475, loss = 1.01912956\n",
      "Iteration 476, loss = 1.01884265\n",
      "Iteration 477, loss = 1.01855502\n",
      "Iteration 478, loss = 1.01826666\n",
      "Iteration 479, loss = 1.01797757\n",
      "Iteration 480, loss = 1.01768776\n",
      "Iteration 481, loss = 1.01739723\n",
      "Iteration 482, loss = 1.01710596\n",
      "Iteration 483, loss = 1.01681397\n",
      "Iteration 484, loss = 1.01652126\n",
      "Iteration 485, loss = 1.01622782\n",
      "Iteration 486, loss = 1.01593366\n",
      "Iteration 487, loss = 1.01563877\n",
      "Iteration 488, loss = 1.01534315\n",
      "Iteration 489, loss = 1.01504681\n",
      "Iteration 490, loss = 1.01474974\n",
      "Iteration 491, loss = 1.01445195\n",
      "Iteration 492, loss = 1.01415344\n",
      "Iteration 493, loss = 1.01385420\n",
      "Iteration 494, loss = 1.01355423\n",
      "Iteration 495, loss = 1.01325355\n",
      "Iteration 496, loss = 1.01295213\n",
      "Iteration 497, loss = 1.01265000\n",
      "Iteration 498, loss = 1.01234714\n",
      "Iteration 499, loss = 1.01204356\n",
      "Iteration 500, loss = 1.01173926\n",
      "Iteration 501, loss = 1.01143423\n",
      "Iteration 502, loss = 1.01112848\n",
      "Iteration 503, loss = 1.01082201\n",
      "Iteration 504, loss = 1.01051482\n",
      "Iteration 505, loss = 1.01020691\n",
      "Iteration 506, loss = 1.00989828\n",
      "Iteration 507, loss = 1.00958893\n",
      "Iteration 508, loss = 1.00927886\n",
      "Iteration 509, loss = 1.00896807\n",
      "Iteration 510, loss = 1.00865657\n",
      "Iteration 511, loss = 1.00834434\n",
      "Iteration 512, loss = 1.00803140\n",
      "Iteration 513, loss = 1.00771774\n",
      "Iteration 514, loss = 1.00740337\n",
      "Iteration 515, loss = 1.00708828\n",
      "Iteration 516, loss = 1.00677247\n",
      "Iteration 517, loss = 1.00645595\n",
      "Iteration 518, loss = 1.00613872\n",
      "Iteration 519, loss = 1.00582078\n",
      "Iteration 520, loss = 1.00550212\n",
      "Iteration 521, loss = 1.00518275\n",
      "Iteration 522, loss = 1.00486268\n",
      "Iteration 523, loss = 1.00454189\n",
      "Iteration 524, loss = 1.00422039\n",
      "Iteration 525, loss = 1.00389818\n",
      "Iteration 526, loss = 1.00357527\n",
      "Iteration 527, loss = 1.00325165\n",
      "Iteration 528, loss = 1.00292732\n",
      "Iteration 529, loss = 1.00260229\n",
      "Iteration 530, loss = 1.00227656\n",
      "Iteration 531, loss = 1.00195012\n",
      "Iteration 532, loss = 1.00162298\n",
      "Iteration 533, loss = 1.00129514\n",
      "Iteration 534, loss = 1.00096660\n",
      "Iteration 535, loss = 1.00063735\n",
      "Iteration 536, loss = 1.00030741\n",
      "Iteration 537, loss = 0.99997678\n",
      "Iteration 538, loss = 0.99964544\n",
      "Iteration 539, loss = 0.99931341\n",
      "Iteration 540, loss = 0.99898069\n",
      "Iteration 541, loss = 0.99864727\n",
      "Iteration 542, loss = 0.99831316\n",
      "Iteration 543, loss = 0.99797836\n",
      "Iteration 544, loss = 0.99764287\n",
      "Iteration 545, loss = 0.99730669\n",
      "Iteration 546, loss = 0.99696982\n",
      "Iteration 547, loss = 0.99663227\n",
      "Iteration 548, loss = 0.99629403\n",
      "Iteration 549, loss = 0.99595511\n",
      "Iteration 550, loss = 0.99561550\n",
      "Iteration 551, loss = 0.99527521\n",
      "Iteration 552, loss = 0.99493424\n",
      "Iteration 553, loss = 0.99459259\n",
      "Iteration 554, loss = 0.99425027\n",
      "Iteration 555, loss = 0.99390726\n",
      "Iteration 556, loss = 0.99356359\n",
      "Iteration 557, loss = 0.99321923\n",
      "Iteration 558, loss = 0.99287421\n",
      "Iteration 559, loss = 0.99252851\n",
      "Iteration 560, loss = 0.99218215\n",
      "Iteration 561, loss = 0.99183511\n",
      "Iteration 562, loss = 0.99148741\n",
      "Iteration 563, loss = 0.99113904\n",
      "Iteration 564, loss = 0.99079001\n",
      "Iteration 565, loss = 0.99044032\n",
      "Iteration 566, loss = 0.99008996\n",
      "Iteration 567, loss = 0.98973894\n",
      "Iteration 568, loss = 0.98938727\n",
      "Iteration 569, loss = 0.98903494\n",
      "Iteration 570, loss = 0.98868195\n",
      "Iteration 571, loss = 0.98832831\n",
      "Iteration 572, loss = 0.98797402\n",
      "Iteration 573, loss = 0.98761908\n",
      "Iteration 574, loss = 0.98726349\n",
      "Iteration 575, loss = 0.98690725\n",
      "Iteration 576, loss = 0.98655036\n",
      "Iteration 577, loss = 0.98619284\n",
      "Iteration 578, loss = 0.98583466\n",
      "Iteration 579, loss = 0.98547585\n",
      "Iteration 580, loss = 0.98511640\n",
      "Iteration 581, loss = 0.98475631\n",
      "Iteration 582, loss = 0.98439559\n",
      "Iteration 583, loss = 0.98403423\n",
      "Iteration 584, loss = 0.98367224\n",
      "Iteration 585, loss = 0.98330962\n",
      "Iteration 586, loss = 0.98294637\n",
      "Iteration 587, loss = 0.98258250\n",
      "Iteration 588, loss = 0.98221800\n",
      "Iteration 589, loss = 0.98185287\n",
      "Iteration 590, loss = 0.98148713\n",
      "Iteration 591, loss = 0.98112076\n",
      "Iteration 592, loss = 0.98075378\n",
      "Iteration 593, loss = 0.98038618\n",
      "Iteration 594, loss = 0.98001797\n",
      "Iteration 595, loss = 0.97964914\n",
      "Iteration 596, loss = 0.97927971\n",
      "Iteration 597, loss = 0.97890967\n",
      "Iteration 598, loss = 0.97853902\n",
      "Iteration 599, loss = 0.97816776\n",
      "Iteration 600, loss = 0.97779591\n",
      "Iteration 601, loss = 0.97742345\n",
      "Iteration 602, loss = 0.97705040\n",
      "Iteration 603, loss = 0.97667674\n",
      "Iteration 604, loss = 0.97630250\n",
      "Iteration 605, loss = 0.97592766\n",
      "Iteration 606, loss = 0.97555223\n",
      "Iteration 607, loss = 0.97517622\n",
      "Iteration 608, loss = 0.97479961\n",
      "Iteration 609, loss = 0.97442243\n",
      "Iteration 610, loss = 0.97404466\n",
      "Iteration 611, loss = 0.97366631\n",
      "Iteration 612, loss = 0.97328738\n",
      "Iteration 613, loss = 0.97290788\n",
      "Iteration 614, loss = 0.97252781\n",
      "Iteration 615, loss = 0.97214716\n",
      "Iteration 616, loss = 0.97176594\n",
      "Iteration 617, loss = 0.97138416\n",
      "Iteration 618, loss = 0.97100181\n",
      "Iteration 619, loss = 0.97061890\n",
      "Iteration 620, loss = 0.97023543\n",
      "Iteration 621, loss = 0.96985140\n",
      "Iteration 622, loss = 0.96946682\n",
      "Iteration 623, loss = 0.96908168\n",
      "Iteration 624, loss = 0.96869599\n",
      "Iteration 625, loss = 0.96830975\n",
      "Iteration 626, loss = 0.96792296\n",
      "Iteration 627, loss = 0.96753563\n",
      "Iteration 628, loss = 0.96714776\n",
      "Iteration 629, loss = 0.96675935\n",
      "Iteration 630, loss = 0.96637040\n",
      "Iteration 631, loss = 0.96598091\n",
      "Iteration 632, loss = 0.96559089\n",
      "Iteration 633, loss = 0.96520034\n",
      "Iteration 634, loss = 0.96480926\n",
      "Iteration 635, loss = 0.96441766\n",
      "Iteration 636, loss = 0.96402553\n",
      "Iteration 637, loss = 0.96363288\n",
      "Iteration 638, loss = 0.96323972\n",
      "Iteration 639, loss = 0.96284603\n",
      "Iteration 640, loss = 0.96245184\n",
      "Iteration 641, loss = 0.96205713\n",
      "Iteration 642, loss = 0.96166191\n",
      "Iteration 643, loss = 0.96126618\n",
      "Iteration 644, loss = 0.96086996\n",
      "Iteration 645, loss = 0.96047323\n",
      "Iteration 646, loss = 0.96007600\n",
      "Iteration 647, loss = 0.95967827\n",
      "Iteration 648, loss = 0.95928005\n",
      "Iteration 649, loss = 0.95888134\n",
      "Iteration 650, loss = 0.95848214\n",
      "Iteration 651, loss = 0.95808245\n",
      "Iteration 652, loss = 0.95768228\n",
      "Iteration 653, loss = 0.95728163\n",
      "Iteration 654, loss = 0.95688049\n",
      "Iteration 655, loss = 0.95647888\n",
      "Iteration 656, loss = 0.95607680\n",
      "Iteration 657, loss = 0.95567425\n",
      "Iteration 658, loss = 0.95527123\n",
      "Iteration 659, loss = 0.95486774\n",
      "Iteration 660, loss = 0.95446379\n",
      "Iteration 661, loss = 0.95405937\n",
      "Iteration 662, loss = 0.95365450\n",
      "Iteration 663, loss = 0.95324918\n",
      "Iteration 664, loss = 0.95284340\n",
      "Iteration 665, loss = 0.95243717\n",
      "Iteration 666, loss = 0.95203049\n",
      "Iteration 667, loss = 0.95162337\n",
      "Iteration 668, loss = 0.95121580\n",
      "Iteration 669, loss = 0.95080779\n",
      "Iteration 670, loss = 0.95039935\n",
      "Iteration 671, loss = 0.94999048\n",
      "Iteration 672, loss = 0.94958117\n",
      "Iteration 673, loss = 0.94917143\n",
      "Iteration 674, loss = 0.94876127\n",
      "Iteration 675, loss = 0.94835068\n",
      "Iteration 676, loss = 0.94793967\n",
      "Iteration 677, loss = 0.94752825\n",
      "Iteration 678, loss = 0.94711640\n",
      "Iteration 679, loss = 0.94670415\n",
      "Iteration 680, loss = 0.94629149\n",
      "Iteration 681, loss = 0.94587841\n",
      "Iteration 682, loss = 0.94546494\n",
      "Iteration 683, loss = 0.94505106\n",
      "Iteration 684, loss = 0.94463678\n",
      "Iteration 685, loss = 0.94422211\n",
      "Iteration 686, loss = 0.94380705\n",
      "Iteration 687, loss = 0.94339159\n",
      "Iteration 688, loss = 0.94297574\n",
      "Iteration 689, loss = 0.94255951\n",
      "Iteration 690, loss = 0.94214290\n",
      "Iteration 691, loss = 0.94172591\n",
      "Iteration 692, loss = 0.94130854\n",
      "Iteration 693, loss = 0.94089080\n",
      "Iteration 694, loss = 0.94047269\n",
      "Iteration 695, loss = 0.94005421\n",
      "Iteration 696, loss = 0.93963536\n",
      "Iteration 697, loss = 0.93921615\n",
      "Iteration 698, loss = 0.93879658\n",
      "Iteration 699, loss = 0.93837666\n",
      "Iteration 700, loss = 0.93795638\n",
      "Iteration 701, loss = 0.93753575\n",
      "Iteration 702, loss = 0.93711477\n",
      "Iteration 703, loss = 0.93669344\n",
      "Iteration 704, loss = 0.93627177\n",
      "Iteration 705, loss = 0.93584977\n",
      "Iteration 706, loss = 0.93542742\n",
      "Iteration 707, loss = 0.93500474\n",
      "Iteration 708, loss = 0.93458173\n",
      "Iteration 709, loss = 0.93415840\n",
      "Iteration 710, loss = 0.93373473\n",
      "Iteration 711, loss = 0.93331075\n",
      "Iteration 712, loss = 0.93288644\n",
      "Iteration 713, loss = 0.93246182\n",
      "Iteration 714, loss = 0.93203688\n",
      "Iteration 715, loss = 0.93161163\n",
      "Iteration 716, loss = 0.93118608\n",
      "Iteration 717, loss = 0.93076022\n",
      "Iteration 718, loss = 0.93033405\n",
      "Iteration 719, loss = 0.92990759\n",
      "Iteration 720, loss = 0.92948083\n",
      "Iteration 721, loss = 0.92905378\n",
      "Iteration 722, loss = 0.92862643\n",
      "Iteration 723, loss = 0.92819880\n",
      "Iteration 724, loss = 0.92777089\n",
      "Iteration 725, loss = 0.92734269\n",
      "Iteration 726, loss = 0.92691421\n",
      "Iteration 727, loss = 0.92648546\n",
      "Iteration 728, loss = 0.92605643\n",
      "Iteration 729, loss = 0.92562713\n",
      "Iteration 730, loss = 0.92519757\n",
      "Iteration 731, loss = 0.92476774\n",
      "Iteration 732, loss = 0.92433765\n",
      "Iteration 733, loss = 0.92390730\n",
      "Iteration 734, loss = 0.92347669\n",
      "Iteration 735, loss = 0.92304583\n",
      "Iteration 736, loss = 0.92261472\n",
      "Iteration 737, loss = 0.92218337\n",
      "Iteration 738, loss = 0.92175177\n",
      "Iteration 739, loss = 0.92131993\n",
      "Iteration 740, loss = 0.92088785\n",
      "Iteration 741, loss = 0.92045554\n",
      "Iteration 742, loss = 0.92002299\n",
      "Iteration 743, loss = 0.91959021\n",
      "Iteration 744, loss = 0.91915721\n",
      "Iteration 745, loss = 0.91872398\n",
      "Iteration 746, loss = 0.91829054\n",
      "Iteration 747, loss = 0.91785687\n",
      "Iteration 748, loss = 0.91742299\n",
      "Iteration 749, loss = 0.91698890\n",
      "Iteration 750, loss = 0.91655460\n",
      "Iteration 751, loss = 0.91612010\n",
      "Iteration 752, loss = 0.91568539\n",
      "Iteration 753, loss = 0.91525048\n",
      "Iteration 754, loss = 0.91481537\n",
      "Iteration 755, loss = 0.91438007\n",
      "Iteration 756, loss = 0.91394458\n",
      "Iteration 757, loss = 0.91350890\n",
      "Iteration 758, loss = 0.91307303\n",
      "Iteration 759, loss = 0.91263698\n",
      "Iteration 760, loss = 0.91220075\n",
      "Iteration 761, loss = 0.91176434\n",
      "Iteration 762, loss = 0.91132776\n",
      "Iteration 763, loss = 0.91089101\n",
      "Iteration 764, loss = 0.91045409\n",
      "Iteration 765, loss = 0.91001701\n",
      "Iteration 766, loss = 0.90957976\n",
      "Iteration 767, loss = 0.90914235\n",
      "Iteration 768, loss = 0.90870479\n",
      "Iteration 769, loss = 0.90826707\n",
      "Iteration 770, loss = 0.90782921\n",
      "Iteration 771, loss = 0.90739119\n",
      "Iteration 772, loss = 0.90695303\n",
      "Iteration 773, loss = 0.90651473\n",
      "Iteration 774, loss = 0.90607629\n",
      "Iteration 775, loss = 0.90563771\n",
      "Iteration 776, loss = 0.90519900\n",
      "Iteration 777, loss = 0.90476016\n",
      "Iteration 778, loss = 0.90432119\n",
      "Iteration 779, loss = 0.90388209\n",
      "Iteration 780, loss = 0.90344288\n",
      "Iteration 781, loss = 0.90300354\n",
      "Iteration 782, loss = 0.90256409\n",
      "Iteration 783, loss = 0.90212452\n",
      "Iteration 784, loss = 0.90168485\n",
      "Iteration 785, loss = 0.90124506\n",
      "Iteration 786, loss = 0.90080517\n",
      "Iteration 787, loss = 0.90036518\n",
      "Iteration 788, loss = 0.89992509\n",
      "Iteration 789, loss = 0.89948491\n",
      "Iteration 790, loss = 0.89904463\n",
      "Iteration 791, loss = 0.89860426\n",
      "Iteration 792, loss = 0.89816380\n",
      "Iteration 793, loss = 0.89772326\n",
      "Iteration 794, loss = 0.89728263\n",
      "Iteration 795, loss = 0.89684193\n",
      "Iteration 796, loss = 0.89640115\n",
      "Iteration 797, loss = 0.89596029\n",
      "Iteration 798, loss = 0.89551936\n",
      "Iteration 799, loss = 0.89507837\n",
      "Iteration 800, loss = 0.89463731\n",
      "Iteration 801, loss = 0.89419619\n",
      "Iteration 802, loss = 0.89375500\n",
      "Iteration 803, loss = 0.89331376\n",
      "Iteration 804, loss = 0.89287247\n",
      "Iteration 805, loss = 0.89243113\n",
      "Iteration 806, loss = 0.89198973\n",
      "Iteration 807, loss = 0.89154829\n",
      "Iteration 808, loss = 0.89110681\n",
      "Iteration 809, loss = 0.89066528\n",
      "Iteration 810, loss = 0.89022372\n",
      "Iteration 811, loss = 0.88978212\n",
      "Iteration 812, loss = 0.88934049\n",
      "Iteration 813, loss = 0.88889883\n",
      "Iteration 814, loss = 0.88845715\n",
      "Iteration 815, loss = 0.88801544\n",
      "Iteration 816, loss = 0.88757370\n",
      "Iteration 817, loss = 0.88713195\n",
      "Iteration 818, loss = 0.88669019\n",
      "Iteration 819, loss = 0.88624841\n",
      "Iteration 820, loss = 0.88580661\n",
      "Iteration 821, loss = 0.88536482\n",
      "Iteration 822, loss = 0.88492301\n",
      "Iteration 823, loss = 0.88448120\n",
      "Iteration 824, loss = 0.88403940\n",
      "Iteration 825, loss = 0.88359759\n",
      "Iteration 826, loss = 0.88315579\n",
      "Iteration 827, loss = 0.88271400\n",
      "Iteration 828, loss = 0.88227222\n",
      "Iteration 829, loss = 0.88183045\n",
      "Iteration 830, loss = 0.88138869\n",
      "Iteration 831, loss = 0.88094696\n",
      "Iteration 832, loss = 0.88050525\n",
      "Iteration 833, loss = 0.88006356\n",
      "Iteration 834, loss = 0.87962189\n",
      "Iteration 835, loss = 0.87918026\n",
      "Iteration 836, loss = 0.87873865\n",
      "Iteration 837, loss = 0.87829708\n",
      "Iteration 838, loss = 0.87785555\n",
      "Iteration 839, loss = 0.87741405\n",
      "Iteration 840, loss = 0.87697260\n",
      "Iteration 841, loss = 0.87653119\n",
      "Iteration 842, loss = 0.87608983\n",
      "Iteration 843, loss = 0.87564851\n",
      "Iteration 844, loss = 0.87520725\n",
      "Iteration 845, loss = 0.87476604\n",
      "Iteration 846, loss = 0.87432489\n",
      "Iteration 847, loss = 0.87388380\n",
      "Iteration 848, loss = 0.87344277\n",
      "Iteration 849, loss = 0.87300180\n",
      "Iteration 850, loss = 0.87256090\n",
      "Iteration 851, loss = 0.87212007\n",
      "Iteration 852, loss = 0.87167931\n",
      "Iteration 853, loss = 0.87123863\n",
      "Iteration 854, loss = 0.87079802\n",
      "Iteration 855, loss = 0.87035749\n",
      "Iteration 856, loss = 0.86991704\n",
      "Iteration 857, loss = 0.86947667\n",
      "Iteration 858, loss = 0.86903639\n",
      "Iteration 859, loss = 0.86859620\n",
      "Iteration 860, loss = 0.86815611\n",
      "Iteration 861, loss = 0.86771610\n",
      "Iteration 862, loss = 0.86727619\n",
      "Iteration 863, loss = 0.86683638\n",
      "Iteration 864, loss = 0.86639667\n",
      "Iteration 865, loss = 0.86595706\n",
      "Iteration 866, loss = 0.86551755\n",
      "Iteration 867, loss = 0.86507816\n",
      "Iteration 868, loss = 0.86463887\n",
      "Iteration 869, loss = 0.86419970\n",
      "Iteration 870, loss = 0.86376064\n",
      "Iteration 871, loss = 0.86332170\n",
      "Iteration 872, loss = 0.86288287\n",
      "Iteration 873, loss = 0.86244417\n",
      "Iteration 874, loss = 0.86200559\n",
      "Iteration 875, loss = 0.86156714\n",
      "Iteration 876, loss = 0.86112882\n",
      "Iteration 877, loss = 0.86069062\n",
      "Iteration 878, loss = 0.86025256\n",
      "Iteration 879, loss = 0.85981464\n",
      "Iteration 880, loss = 0.85937685\n",
      "Iteration 881, loss = 0.85893920\n",
      "Iteration 882, loss = 0.85850169\n",
      "Iteration 883, loss = 0.85806433\n",
      "Iteration 884, loss = 0.85762711\n",
      "Iteration 885, loss = 0.85719004\n",
      "Iteration 886, loss = 0.85675312\n",
      "Iteration 887, loss = 0.85631636\n",
      "Iteration 888, loss = 0.85587975\n",
      "Iteration 889, loss = 0.85544329\n",
      "Iteration 890, loss = 0.85500700\n",
      "Iteration 891, loss = 0.85457086\n",
      "Iteration 892, loss = 0.85413489\n",
      "Iteration 893, loss = 0.85369909\n",
      "Iteration 894, loss = 0.85326345\n",
      "Iteration 895, loss = 0.85282798\n",
      "Iteration 896, loss = 0.85239269\n",
      "Iteration 897, loss = 0.85195757\n",
      "Iteration 898, loss = 0.85152262\n",
      "Iteration 899, loss = 0.85108785\n",
      "Iteration 900, loss = 0.85065327\n",
      "Iteration 901, loss = 0.85021886\n",
      "Iteration 902, loss = 0.84978464\n",
      "Iteration 903, loss = 0.84935060\n",
      "Iteration 904, loss = 0.84891676\n",
      "Iteration 905, loss = 0.84848310\n",
      "Iteration 906, loss = 0.84804964\n",
      "Iteration 907, loss = 0.84761637\n",
      "Iteration 908, loss = 0.84718329\n",
      "Iteration 909, loss = 0.84675042\n",
      "Iteration 910, loss = 0.84631774\n",
      "Iteration 911, loss = 0.84588527\n",
      "Iteration 912, loss = 0.84545300\n",
      "Iteration 913, loss = 0.84502094\n",
      "Iteration 914, loss = 0.84458908\n",
      "Iteration 915, loss = 0.84415744\n",
      "Iteration 916, loss = 0.84372600\n",
      "Iteration 917, loss = 0.84329478\n",
      "Iteration 918, loss = 0.84286378\n",
      "Iteration 919, loss = 0.84243299\n",
      "Iteration 920, loss = 0.84200242\n",
      "Iteration 921, loss = 0.84157208\n",
      "Iteration 922, loss = 0.84114195\n",
      "Iteration 923, loss = 0.84071205\n",
      "Iteration 924, loss = 0.84028238\n",
      "Iteration 925, loss = 0.83985294\n",
      "Iteration 926, loss = 0.83942373\n",
      "Iteration 927, loss = 0.83899475\n",
      "Iteration 928, loss = 0.83856600\n",
      "Iteration 929, loss = 0.83813749\n",
      "Iteration 930, loss = 0.83770922\n",
      "Iteration 931, loss = 0.83728118\n",
      "Iteration 932, loss = 0.83685339\n",
      "Iteration 933, loss = 0.83642584\n",
      "Iteration 934, loss = 0.83599853\n",
      "Iteration 935, loss = 0.83557147\n",
      "Iteration 936, loss = 0.83514466\n",
      "Iteration 937, loss = 0.83471810\n",
      "Iteration 938, loss = 0.83429179\n",
      "Iteration 939, loss = 0.83386574\n",
      "Iteration 940, loss = 0.83343993\n",
      "Iteration 941, loss = 0.83301439\n",
      "Iteration 942, loss = 0.83258910\n",
      "Iteration 943, loss = 0.83216407\n",
      "Iteration 944, loss = 0.83173931\n",
      "Iteration 945, loss = 0.83131480\n",
      "Iteration 946, loss = 0.83089057\n",
      "Iteration 947, loss = 0.83046659\n",
      "Iteration 948, loss = 0.83004289\n",
      "Iteration 949, loss = 0.82961946\n",
      "Iteration 950, loss = 0.82919629\n",
      "Iteration 951, loss = 0.82877340\n",
      "Iteration 952, loss = 0.82835079\n",
      "Iteration 953, loss = 0.82792845\n",
      "Iteration 954, loss = 0.82750638\n",
      "Iteration 955, loss = 0.82708460\n",
      "Iteration 956, loss = 0.82666310\n",
      "Iteration 957, loss = 0.82624188\n",
      "Iteration 958, loss = 0.82582094\n",
      "Iteration 959, loss = 0.82540029\n",
      "Iteration 960, loss = 0.82497992\n",
      "Iteration 961, loss = 0.82455984\n",
      "Iteration 962, loss = 0.82414006\n",
      "Iteration 963, loss = 0.82372056\n",
      "Iteration 964, loss = 0.82330135\n",
      "Iteration 965, loss = 0.82288244\n",
      "Iteration 966, loss = 0.82246383\n",
      "Iteration 967, loss = 0.82204551\n",
      "Iteration 968, loss = 0.82162749\n",
      "Iteration 969, loss = 0.82120977\n",
      "Iteration 970, loss = 0.82079235\n",
      "Iteration 971, loss = 0.82037523\n",
      "Iteration 972, loss = 0.81995842\n",
      "Iteration 973, loss = 0.81954191\n",
      "Iteration 974, loss = 0.81912571\n",
      "Iteration 975, loss = 0.81870982\n",
      "Iteration 976, loss = 0.81829423\n",
      "Iteration 977, loss = 0.81787896\n",
      "Iteration 978, loss = 0.81746400\n",
      "Iteration 979, loss = 0.81704935\n",
      "Iteration 980, loss = 0.81663501\n",
      "Iteration 981, loss = 0.81622100\n",
      "Iteration 982, loss = 0.81580729\n",
      "Iteration 983, loss = 0.81539391\n",
      "Iteration 984, loss = 0.81498085\n",
      "Iteration 985, loss = 0.81456811\n",
      "Iteration 986, loss = 0.81415569\n",
      "Iteration 987, loss = 0.81374359\n",
      "Iteration 988, loss = 0.81333182\n",
      "Iteration 989, loss = 0.81292038\n",
      "Iteration 990, loss = 0.81250926\n",
      "Iteration 991, loss = 0.81209847\n",
      "Iteration 992, loss = 0.81168801\n",
      "Iteration 993, loss = 0.81127788\n",
      "Iteration 994, loss = 0.81086808\n",
      "Iteration 995, loss = 0.81045862\n",
      "Iteration 996, loss = 0.81004949\n",
      "Iteration 997, loss = 0.80964070\n",
      "Iteration 998, loss = 0.80923224\n",
      "Iteration 999, loss = 0.80882412\n",
      "Iteration 1000, loss = 0.80841634\n",
      "Iteration 1, loss = 1.16644730\n",
      "Iteration 2, loss = 1.16626136\n",
      "Iteration 3, loss = 1.16599614\n",
      "Iteration 4, loss = 1.16565957\n",
      "Iteration 5, loss = 1.16525878\n",
      "Iteration 6, loss = 1.16480017\n",
      "Iteration 7, loss = 1.16428953\n",
      "Iteration 8, loss = 1.16373205\n",
      "Iteration 9, loss = 1.16313243\n",
      "Iteration 10, loss = 1.16249489\n",
      "Iteration 11, loss = 1.16182321\n",
      "Iteration 12, loss = 1.16112085\n",
      "Iteration 13, loss = 1.16039087\n",
      "Iteration 14, loss = 1.15963608\n",
      "Iteration 15, loss = 1.15885898\n",
      "Iteration 16, loss = 1.15806186\n",
      "Iteration 17, loss = 1.15724676\n",
      "Iteration 18, loss = 1.15641553\n",
      "Iteration 19, loss = 1.15556985\n",
      "Iteration 20, loss = 1.15471125\n",
      "Iteration 21, loss = 1.15384109\n",
      "Iteration 22, loss = 1.15296063\n",
      "Iteration 23, loss = 1.15207101\n",
      "Iteration 24, loss = 1.15117324\n",
      "Iteration 25, loss = 1.15026828\n",
      "Iteration 26, loss = 1.14935697\n",
      "Iteration 27, loss = 1.14844010\n",
      "Iteration 28, loss = 1.14751840\n",
      "Iteration 29, loss = 1.14659251\n",
      "Iteration 30, loss = 1.14566303\n",
      "Iteration 31, loss = 1.14473054\n",
      "Iteration 32, loss = 1.14379554\n",
      "Iteration 33, loss = 1.14285851\n",
      "Iteration 34, loss = 1.14191990\n",
      "Iteration 35, loss = 1.14098011\n",
      "Iteration 36, loss = 1.14003953\n",
      "Iteration 37, loss = 1.13909853\n",
      "Iteration 38, loss = 1.13815745\n",
      "Iteration 39, loss = 1.13721659\n",
      "Iteration 40, loss = 1.13627628\n",
      "Iteration 41, loss = 1.13533678\n",
      "Iteration 42, loss = 1.13439838\n",
      "Iteration 43, loss = 1.13346133\n",
      "Iteration 44, loss = 1.13252589\n",
      "Iteration 45, loss = 1.13159227\n",
      "Iteration 46, loss = 1.13066072\n",
      "Iteration 47, loss = 1.12973144\n",
      "Iteration 48, loss = 1.12880464\n",
      "Iteration 49, loss = 1.12788052\n",
      "Iteration 50, loss = 1.12695928\n",
      "Iteration 51, loss = 1.12604108\n",
      "Iteration 52, loss = 1.12512612\n",
      "Iteration 53, loss = 1.12421455\n",
      "Iteration 54, loss = 1.12330655\n",
      "Iteration 55, loss = 1.12240225\n",
      "Iteration 56, loss = 1.12150183\n",
      "Iteration 57, loss = 1.12060540\n",
      "Iteration 58, loss = 1.11971312\n",
      "Iteration 59, loss = 1.11882510\n",
      "Iteration 60, loss = 1.11794148\n",
      "Iteration 61, loss = 1.11706237\n",
      "Iteration 62, loss = 1.11618788\n",
      "Iteration 63, loss = 1.11531811\n",
      "Iteration 64, loss = 1.11445317\n",
      "Iteration 65, loss = 1.11359313\n",
      "Iteration 66, loss = 1.11273810\n",
      "Iteration 67, loss = 1.11188814\n",
      "Iteration 68, loss = 1.11104334\n",
      "Iteration 69, loss = 1.11020375\n",
      "Iteration 70, loss = 1.10936944\n",
      "Iteration 71, loss = 1.10854046\n",
      "Iteration 72, loss = 1.10771686\n",
      "Iteration 73, loss = 1.10689868\n",
      "Iteration 74, loss = 1.10608597\n",
      "Iteration 75, loss = 1.10527875\n",
      "Iteration 76, loss = 1.10447704\n",
      "Iteration 77, loss = 1.10368087\n",
      "Iteration 78, loss = 1.10289026\n",
      "Iteration 79, loss = 1.10210521\n",
      "Iteration 80, loss = 1.10132573\n",
      "Iteration 81, loss = 1.10055181\n",
      "Iteration 82, loss = 1.09978346\n",
      "Iteration 83, loss = 1.09902066\n",
      "Iteration 84, loss = 1.09826341\n",
      "Iteration 85, loss = 1.09751167\n",
      "Iteration 86, loss = 1.09676544\n",
      "Iteration 87, loss = 1.09602469\n",
      "Iteration 88, loss = 1.09528938\n",
      "Iteration 89, loss = 1.09455949\n",
      "Iteration 90, loss = 1.09383499\n",
      "Iteration 91, loss = 1.09311582\n",
      "Iteration 92, loss = 1.09240196\n",
      "Iteration 93, loss = 1.09169336\n",
      "Iteration 94, loss = 1.09098997\n",
      "Iteration 95, loss = 1.09029175\n",
      "Iteration 96, loss = 1.08959864\n",
      "Iteration 97, loss = 1.08891059\n",
      "Iteration 98, loss = 1.08822756\n",
      "Iteration 99, loss = 1.08754948\n",
      "Iteration 100, loss = 1.08687630\n",
      "Iteration 101, loss = 1.08620797\n",
      "Iteration 102, loss = 1.08554442\n",
      "Iteration 103, loss = 1.08488559\n",
      "Iteration 104, loss = 1.08423143\n",
      "Iteration 105, loss = 1.08358187\n",
      "Iteration 106, loss = 1.08293685\n",
      "Iteration 107, loss = 1.08229632\n",
      "Iteration 108, loss = 1.08166021\n",
      "Iteration 109, loss = 1.08102845\n",
      "Iteration 110, loss = 1.08040100\n",
      "Iteration 111, loss = 1.07977778\n",
      "Iteration 112, loss = 1.07915873\n",
      "Iteration 113, loss = 1.07854380\n",
      "Iteration 114, loss = 1.07793292\n",
      "Iteration 115, loss = 1.07732602\n",
      "Iteration 116, loss = 1.07672306\n",
      "Iteration 117, loss = 1.07612398\n",
      "Iteration 118, loss = 1.07552870\n",
      "Iteration 119, loss = 1.07493718\n",
      "Iteration 120, loss = 1.07434935\n",
      "Iteration 121, loss = 1.07376517\n",
      "Iteration 122, loss = 1.07318456\n",
      "Iteration 123, loss = 1.07260748\n",
      "Iteration 124, loss = 1.07203388\n",
      "Iteration 125, loss = 1.07146369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhruv/indexCode/cmpsc445/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 126, loss = 1.07089687\n",
      "Iteration 127, loss = 1.07033335\n",
      "Iteration 128, loss = 1.06977310\n",
      "Iteration 129, loss = 1.06921606\n",
      "Iteration 130, loss = 1.06866218\n",
      "Iteration 131, loss = 1.06811141\n",
      "Iteration 132, loss = 1.06756370\n",
      "Iteration 133, loss = 1.06701901\n",
      "Iteration 134, loss = 1.06647729\n",
      "Iteration 135, loss = 1.06593849\n",
      "Iteration 136, loss = 1.06540257\n",
      "Iteration 137, loss = 1.06486948\n",
      "Iteration 138, loss = 1.06433919\n",
      "Iteration 139, loss = 1.06381165\n",
      "Iteration 140, loss = 1.06328682\n",
      "Iteration 141, loss = 1.06276466\n",
      "Iteration 142, loss = 1.06224513\n",
      "Iteration 143, loss = 1.06172819\n",
      "Iteration 144, loss = 1.06121380\n",
      "Iteration 145, loss = 1.06070193\n",
      "Iteration 146, loss = 1.06019254\n",
      "Iteration 147, loss = 1.05968559\n",
      "Iteration 148, loss = 1.05918105\n",
      "Iteration 149, loss = 1.05867889\n",
      "Iteration 150, loss = 1.05817907\n",
      "Iteration 151, loss = 1.05768156\n",
      "Iteration 152, loss = 1.05718633\n",
      "Iteration 153, loss = 1.05669335\n",
      "Iteration 154, loss = 1.05620258\n",
      "Iteration 155, loss = 1.05571399\n",
      "Iteration 156, loss = 1.05522757\n",
      "Iteration 157, loss = 1.05474327\n",
      "Iteration 158, loss = 1.05426107\n",
      "Iteration 159, loss = 1.05378095\n",
      "Iteration 160, loss = 1.05330287\n",
      "Iteration 161, loss = 1.05282680\n",
      "Iteration 162, loss = 1.05235274\n",
      "Iteration 163, loss = 1.05188064\n",
      "Iteration 164, loss = 1.05141048\n",
      "Iteration 165, loss = 1.05094224\n",
      "Iteration 166, loss = 1.05047590\n",
      "Iteration 167, loss = 1.05001143\n",
      "Iteration 168, loss = 1.04954881\n",
      "Iteration 169, loss = 1.04908801\n",
      "Iteration 170, loss = 1.04862902\n",
      "Iteration 171, loss = 1.04817181\n",
      "Iteration 172, loss = 1.04771636\n",
      "Iteration 173, loss = 1.04726265\n",
      "Iteration 174, loss = 1.04681067\n",
      "Iteration 175, loss = 1.04636038\n",
      "Iteration 176, loss = 1.04591178\n",
      "Iteration 177, loss = 1.04546483\n",
      "Iteration 178, loss = 1.04501953\n",
      "Iteration 179, loss = 1.04457586\n",
      "Iteration 180, loss = 1.04413378\n",
      "Iteration 181, loss = 1.04369330\n",
      "Iteration 182, loss = 1.04325439\n",
      "Iteration 183, loss = 1.04281703\n",
      "Iteration 184, loss = 1.04238120\n",
      "Iteration 185, loss = 1.04194689\n",
      "Iteration 186, loss = 1.04151409\n",
      "Iteration 187, loss = 1.04108277\n",
      "Iteration 188, loss = 1.04065292\n",
      "Iteration 189, loss = 1.04022453\n",
      "Iteration 190, loss = 1.03979757\n",
      "Iteration 191, loss = 1.03937204\n",
      "Iteration 192, loss = 1.03894792\n",
      "Iteration 193, loss = 1.03852519\n",
      "Iteration 194, loss = 1.03810384\n",
      "Iteration 195, loss = 1.03768386\n",
      "Iteration 196, loss = 1.03726522\n",
      "Iteration 197, loss = 1.03684793\n",
      "Iteration 198, loss = 1.03643195\n",
      "Iteration 199, loss = 1.03601729\n",
      "Iteration 200, loss = 1.03560392\n",
      "Iteration 201, loss = 1.03519183\n",
      "Iteration 202, loss = 1.03478101\n",
      "Iteration 203, loss = 1.03437145\n",
      "Iteration 204, loss = 1.03396314\n",
      "Iteration 205, loss = 1.03355605\n",
      "Iteration 206, loss = 1.03315019\n",
      "Iteration 207, loss = 1.03274553\n",
      "Iteration 208, loss = 1.03234206\n",
      "Iteration 209, loss = 1.03193978\n",
      "Iteration 210, loss = 1.03153867\n",
      "Iteration 211, loss = 1.03113872\n",
      "Iteration 212, loss = 1.03073992\n",
      "Iteration 213, loss = 1.03034226\n",
      "Iteration 214, loss = 1.02994572\n",
      "Iteration 215, loss = 1.02955029\n",
      "Iteration 216, loss = 1.02915597\n",
      "Iteration 217, loss = 1.02876274\n",
      "Iteration 218, loss = 1.02837059\n",
      "Iteration 219, loss = 1.02797951\n",
      "Iteration 220, loss = 1.02758949\n",
      "Iteration 221, loss = 1.02720052\n",
      "Iteration 222, loss = 1.02681260\n",
      "Iteration 223, loss = 1.02642570\n",
      "Iteration 224, loss = 1.02603982\n",
      "Iteration 225, loss = 1.02565495\n",
      "Iteration 226, loss = 1.02527108\n",
      "Iteration 227, loss = 1.02488820\n",
      "Iteration 228, loss = 1.02450629\n",
      "Iteration 229, loss = 1.02412536\n",
      "Iteration 230, loss = 1.02374539\n",
      "Iteration 231, loss = 1.02336637\n",
      "Iteration 232, loss = 1.02298829\n",
      "Iteration 233, loss = 1.02261115\n",
      "Iteration 234, loss = 1.02223493\n",
      "Iteration 235, loss = 1.02185962\n",
      "Iteration 236, loss = 1.02148522\n",
      "Iteration 237, loss = 1.02111171\n",
      "Iteration 238, loss = 1.02073910\n",
      "Iteration 239, loss = 1.02036736\n",
      "Iteration 240, loss = 1.01999649\n",
      "Iteration 241, loss = 1.01962648\n",
      "Iteration 242, loss = 1.01925733\n",
      "Iteration 243, loss = 1.01888902\n",
      "Iteration 244, loss = 1.01852155\n",
      "Iteration 245, loss = 1.01815491\n",
      "Iteration 246, loss = 1.01778908\n",
      "Iteration 247, loss = 1.01742407\n",
      "Iteration 248, loss = 1.01705986\n",
      "Iteration 249, loss = 1.01669645\n",
      "Iteration 250, loss = 1.01633382\n",
      "Iteration 251, loss = 1.01597198\n",
      "Iteration 252, loss = 1.01561091\n",
      "Iteration 253, loss = 1.01525060\n",
      "Iteration 254, loss = 1.01489104\n",
      "Iteration 255, loss = 1.01453224\n",
      "Iteration 256, loss = 1.01417418\n",
      "Iteration 257, loss = 1.01381685\n",
      "Iteration 258, loss = 1.01346025\n",
      "Iteration 259, loss = 1.01310437\n",
      "Iteration 260, loss = 1.01274920\n",
      "Iteration 261, loss = 1.01239474\n",
      "Iteration 262, loss = 1.01204097\n",
      "Iteration 263, loss = 1.01168790\n",
      "Iteration 264, loss = 1.01133551\n",
      "Iteration 265, loss = 1.01098379\n",
      "Iteration 266, loss = 1.01063275\n",
      "Iteration 267, loss = 1.01028237\n",
      "Iteration 268, loss = 1.00993264\n",
      "Iteration 269, loss = 1.00958356\n",
      "Iteration 270, loss = 1.00923513\n",
      "Iteration 271, loss = 1.00888733\n",
      "Iteration 272, loss = 1.00854017\n",
      "Iteration 273, loss = 1.00819362\n",
      "Iteration 274, loss = 1.00784770\n",
      "Iteration 275, loss = 1.00750238\n",
      "Iteration 276, loss = 1.00715767\n",
      "Iteration 277, loss = 1.00681355\n",
      "Iteration 278, loss = 1.00647003\n",
      "Iteration 279, loss = 1.00612709\n",
      "Iteration 280, loss = 1.00578473\n",
      "Iteration 281, loss = 1.00544295\n",
      "Iteration 282, loss = 1.00510173\n",
      "Iteration 283, loss = 1.00476108\n",
      "Iteration 284, loss = 1.00442098\n",
      "Iteration 285, loss = 1.00408143\n",
      "Iteration 286, loss = 1.00374242\n",
      "Iteration 287, loss = 1.00340395\n",
      "Iteration 288, loss = 1.00306602\n",
      "Iteration 289, loss = 1.00272861\n",
      "Iteration 290, loss = 1.00239172\n",
      "Iteration 291, loss = 1.00205535\n",
      "Iteration 292, loss = 1.00171949\n",
      "Iteration 293, loss = 1.00138413\n",
      "Iteration 294, loss = 1.00104927\n",
      "Iteration 295, loss = 1.00071491\n",
      "Iteration 296, loss = 1.00038104\n",
      "Iteration 297, loss = 1.00004765\n",
      "Iteration 298, loss = 0.99971473\n",
      "Iteration 299, loss = 0.99938230\n",
      "Iteration 300, loss = 0.99905033\n",
      "Iteration 301, loss = 0.99871882\n",
      "Iteration 302, loss = 0.99838777\n",
      "Iteration 303, loss = 0.99805718\n",
      "Iteration 304, loss = 0.99772703\n",
      "Iteration 305, loss = 0.99739733\n",
      "Iteration 306, loss = 0.99706806\n",
      "Iteration 307, loss = 0.99673923\n",
      "Iteration 308, loss = 0.99641083\n",
      "Iteration 309, loss = 0.99608286\n",
      "Iteration 310, loss = 0.99575530\n",
      "Iteration 311, loss = 0.99542816\n",
      "Iteration 312, loss = 0.99510144\n",
      "Iteration 313, loss = 0.99477511\n",
      "Iteration 314, loss = 0.99444919\n",
      "Iteration 315, loss = 0.99412367\n",
      "Iteration 316, loss = 0.99379854\n",
      "Iteration 317, loss = 0.99347380\n",
      "Iteration 318, loss = 0.99314945\n",
      "Iteration 319, loss = 0.99282547\n",
      "Iteration 320, loss = 0.99250188\n",
      "Iteration 321, loss = 0.99217865\n",
      "Iteration 322, loss = 0.99185579\n",
      "Iteration 323, loss = 0.99153330\n",
      "Iteration 324, loss = 0.99121117\n",
      "Iteration 325, loss = 0.99088939\n",
      "Iteration 326, loss = 0.99056797\n",
      "Iteration 327, loss = 0.99024689\n",
      "Iteration 328, loss = 0.98992616\n",
      "Iteration 329, loss = 0.98960577\n",
      "Iteration 330, loss = 0.98928571\n",
      "Iteration 331, loss = 0.98896599\n",
      "Iteration 332, loss = 0.98864660\n",
      "Iteration 333, loss = 0.98832754\n",
      "Iteration 334, loss = 0.98800880\n",
      "Iteration 335, loss = 0.98769037\n",
      "Iteration 336, loss = 0.98737226\n",
      "Iteration 337, loss = 0.98705447\n",
      "Iteration 338, loss = 0.98673698\n",
      "Iteration 339, loss = 0.98641980\n",
      "Iteration 340, loss = 0.98610291\n",
      "Iteration 341, loss = 0.98578633\n",
      "Iteration 342, loss = 0.98547004\n",
      "Iteration 343, loss = 0.98515404\n",
      "Iteration 344, loss = 0.98483833\n",
      "Iteration 345, loss = 0.98452291\n",
      "Iteration 346, loss = 0.98420777\n",
      "Iteration 347, loss = 0.98389291\n",
      "Iteration 348, loss = 0.98357832\n",
      "Iteration 349, loss = 0.98326400\n",
      "Iteration 350, loss = 0.98294996\n",
      "Iteration 351, loss = 0.98263618\n",
      "Iteration 352, loss = 0.98232267\n",
      "Iteration 353, loss = 0.98200941\n",
      "Iteration 354, loss = 0.98169642\n",
      "Iteration 355, loss = 0.98138368\n",
      "Iteration 356, loss = 0.98107119\n",
      "Iteration 357, loss = 0.98075895\n",
      "Iteration 358, loss = 0.98044696\n",
      "Iteration 359, loss = 0.98013522\n",
      "Iteration 360, loss = 0.97982371\n",
      "Iteration 361, loss = 0.97951245\n",
      "Iteration 362, loss = 0.97920141\n",
      "Iteration 363, loss = 0.97889062\n",
      "Iteration 364, loss = 0.97858005\n",
      "Iteration 365, loss = 0.97826972\n",
      "Iteration 366, loss = 0.97795960\n",
      "Iteration 367, loss = 0.97764972\n",
      "Iteration 368, loss = 0.97734005\n",
      "Iteration 369, loss = 0.97703060\n",
      "Iteration 370, loss = 0.97672137\n",
      "Iteration 371, loss = 0.97641235\n",
      "Iteration 372, loss = 0.97610355\n",
      "Iteration 373, loss = 0.97579495\n",
      "Iteration 374, loss = 0.97548656\n",
      "Iteration 375, loss = 0.97517838\n",
      "Iteration 376, loss = 0.97487040\n",
      "Iteration 377, loss = 0.97456262\n",
      "Iteration 378, loss = 0.97425504\n",
      "Iteration 379, loss = 0.97394765\n",
      "Iteration 380, loss = 0.97364046\n",
      "Iteration 381, loss = 0.97333346\n",
      "Iteration 382, loss = 0.97302665\n",
      "Iteration 383, loss = 0.97272003\n",
      "Iteration 384, loss = 0.97241359\n",
      "Iteration 385, loss = 0.97210734\n",
      "Iteration 386, loss = 0.97180127\n",
      "Iteration 387, loss = 0.97149539\n",
      "Iteration 388, loss = 0.97118968\n",
      "Iteration 389, loss = 0.97088415\n",
      "Iteration 390, loss = 0.97057879\n",
      "Iteration 391, loss = 0.97027360\n",
      "Iteration 392, loss = 0.96996859\n",
      "Iteration 393, loss = 0.96966375\n",
      "Iteration 394, loss = 0.96935907\n",
      "Iteration 395, loss = 0.96905456\n",
      "Iteration 396, loss = 0.96875022\n",
      "Iteration 397, loss = 0.96844603\n",
      "Iteration 398, loss = 0.96814201\n",
      "Iteration 399, loss = 0.96783815\n",
      "Iteration 400, loss = 0.96753445\n",
      "Iteration 401, loss = 0.96723090\n",
      "Iteration 402, loss = 0.96692751\n",
      "Iteration 403, loss = 0.96662427\n",
      "Iteration 404, loss = 0.96632118\n",
      "Iteration 405, loss = 0.96601825\n",
      "Iteration 406, loss = 0.96571546\n",
      "Iteration 407, loss = 0.96541282\n",
      "Iteration 408, loss = 0.96511033\n",
      "Iteration 409, loss = 0.96480798\n",
      "Iteration 410, loss = 0.96450578\n",
      "Iteration 411, loss = 0.96420371\n",
      "Iteration 412, loss = 0.96390179\n",
      "Iteration 413, loss = 0.96360001\n",
      "Iteration 414, loss = 0.96329837\n",
      "Iteration 415, loss = 0.96299686\n",
      "Iteration 416, loss = 0.96269549\n",
      "Iteration 417, loss = 0.96239425\n",
      "Iteration 418, loss = 0.96209315\n",
      "Iteration 419, loss = 0.96179218\n",
      "Iteration 420, loss = 0.96149134\n",
      "Iteration 421, loss = 0.96119063\n",
      "Iteration 422, loss = 0.96089005\n",
      "Iteration 423, loss = 0.96058959\n",
      "Iteration 424, loss = 0.96028926\n",
      "Iteration 425, loss = 0.95998906\n",
      "Iteration 426, loss = 0.95968899\n",
      "Iteration 427, loss = 0.95938903\n",
      "Iteration 428, loss = 0.95908920\n",
      "Iteration 429, loss = 0.95878949\n",
      "Iteration 430, loss = 0.95848990\n",
      "Iteration 431, loss = 0.95819043\n",
      "Iteration 432, loss = 0.95789108\n",
      "Iteration 433, loss = 0.95759185\n",
      "Iteration 434, loss = 0.95729273\n",
      "Iteration 435, loss = 0.95699373\n",
      "Iteration 436, loss = 0.95669485\n",
      "Iteration 437, loss = 0.95639607\n",
      "Iteration 438, loss = 0.95609742\n",
      "Iteration 439, loss = 0.95579887\n",
      "Iteration 440, loss = 0.95550044\n",
      "Iteration 441, loss = 0.95520211\n",
      "Iteration 442, loss = 0.95490390\n",
      "Iteration 443, loss = 0.95460580\n",
      "Iteration 444, loss = 0.95430780\n",
      "Iteration 445, loss = 0.95400991\n",
      "Iteration 446, loss = 0.95371213\n",
      "Iteration 447, loss = 0.95341446\n",
      "Iteration 448, loss = 0.95311689\n",
      "Iteration 449, loss = 0.95281943\n",
      "Iteration 450, loss = 0.95252207\n",
      "Iteration 451, loss = 0.95222482\n",
      "Iteration 452, loss = 0.95192766\n",
      "Iteration 453, loss = 0.95163061\n",
      "Iteration 454, loss = 0.95133367\n",
      "Iteration 455, loss = 0.95103682\n",
      "Iteration 456, loss = 0.95074008\n",
      "Iteration 457, loss = 0.95044343\n",
      "Iteration 458, loss = 0.95014689\n",
      "Iteration 459, loss = 0.94985044\n",
      "Iteration 460, loss = 0.94955409\n",
      "Iteration 461, loss = 0.94925784\n",
      "Iteration 462, loss = 0.94896169\n",
      "Iteration 463, loss = 0.94866564\n",
      "Iteration 464, loss = 0.94836968\n",
      "Iteration 465, loss = 0.94807382\n",
      "Iteration 466, loss = 0.94777805\n",
      "Iteration 467, loss = 0.94748238\n",
      "Iteration 468, loss = 0.94718680\n",
      "Iteration 469, loss = 0.94689132\n",
      "Iteration 470, loss = 0.94659593\n",
      "Iteration 471, loss = 0.94630064\n",
      "Iteration 472, loss = 0.94600543\n",
      "Iteration 473, loss = 0.94571032\n",
      "Iteration 474, loss = 0.94541531\n",
      "Iteration 475, loss = 0.94512038\n",
      "Iteration 476, loss = 0.94482555\n",
      "Iteration 477, loss = 0.94453081\n",
      "Iteration 478, loss = 0.94423616\n",
      "Iteration 479, loss = 0.94394160\n",
      "Iteration 480, loss = 0.94364713\n",
      "Iteration 481, loss = 0.94335275\n",
      "Iteration 482, loss = 0.94305847\n",
      "Iteration 483, loss = 0.94276427\n",
      "Iteration 484, loss = 0.94247016\n",
      "Iteration 485, loss = 0.94217614\n",
      "Iteration 486, loss = 0.94188221\n",
      "Iteration 487, loss = 0.94158836\n",
      "Iteration 488, loss = 0.94129461\n",
      "Iteration 489, loss = 0.94100094\n",
      "Iteration 490, loss = 0.94070736\n",
      "Iteration 491, loss = 0.94041387\n",
      "Iteration 492, loss = 0.94012047\n",
      "Iteration 493, loss = 0.93982715\n",
      "Iteration 494, loss = 0.93953393\n",
      "Iteration 495, loss = 0.93924078\n",
      "Iteration 496, loss = 0.93894773\n",
      "Iteration 497, loss = 0.93865476\n",
      "Iteration 498, loss = 0.93836188\n",
      "Iteration 499, loss = 0.93806909\n",
      "Iteration 500, loss = 0.93777638\n",
      "Iteration 501, loss = 0.93748376\n",
      "Iteration 502, loss = 0.93719122\n",
      "Iteration 503, loss = 0.93689877\n",
      "Iteration 504, loss = 0.93660641\n",
      "Iteration 505, loss = 0.93631413\n",
      "Iteration 506, loss = 0.93602194\n",
      "Iteration 507, loss = 0.93572984\n",
      "Iteration 508, loss = 0.93543782\n",
      "Iteration 509, loss = 0.93514588\n",
      "Iteration 510, loss = 0.93485403\n",
      "Iteration 511, loss = 0.93456227\n",
      "Iteration 512, loss = 0.93427059\n",
      "Iteration 513, loss = 0.93397900\n",
      "Iteration 514, loss = 0.93368749\n",
      "Iteration 515, loss = 0.93339607\n",
      "Iteration 516, loss = 0.93310474\n",
      "Iteration 517, loss = 0.93281348\n",
      "Iteration 518, loss = 0.93252232\n",
      "Iteration 519, loss = 0.93223124\n",
      "Iteration 520, loss = 0.93194025\n",
      "Iteration 521, loss = 0.93164934\n",
      "Iteration 522, loss = 0.93135851\n",
      "Iteration 523, loss = 0.93106778\n",
      "Iteration 524, loss = 0.93077712\n",
      "Iteration 525, loss = 0.93048656\n",
      "Iteration 526, loss = 0.93019608\n",
      "Iteration 527, loss = 0.92990568\n",
      "Iteration 528, loss = 0.92961537\n",
      "Iteration 529, loss = 0.92932515\n",
      "Iteration 530, loss = 0.92903501\n",
      "Iteration 531, loss = 0.92874496\n",
      "Iteration 532, loss = 0.92845499\n",
      "Iteration 533, loss = 0.92816511\n",
      "Iteration 534, loss = 0.92787532\n",
      "Iteration 535, loss = 0.92758561\n",
      "Iteration 536, loss = 0.92729599\n",
      "Iteration 537, loss = 0.92700645\n",
      "Iteration 538, loss = 0.92671701\n",
      "Iteration 539, loss = 0.92642765\n",
      "Iteration 540, loss = 0.92613837\n",
      "Iteration 541, loss = 0.92584918\n",
      "Iteration 542, loss = 0.92556008\n",
      "Iteration 543, loss = 0.92527107\n",
      "Iteration 544, loss = 0.92498215\n",
      "Iteration 545, loss = 0.92469331\n",
      "Iteration 546, loss = 0.92440456\n",
      "Iteration 547, loss = 0.92411590\n",
      "Iteration 548, loss = 0.92382732\n",
      "Iteration 549, loss = 0.92353883\n",
      "Iteration 550, loss = 0.92325044\n",
      "Iteration 551, loss = 0.92296213\n",
      "Iteration 552, loss = 0.92267390\n",
      "Iteration 553, loss = 0.92238577\n",
      "Iteration 554, loss = 0.92209773\n",
      "Iteration 555, loss = 0.92180977\n",
      "Iteration 556, loss = 0.92152191\n",
      "Iteration 557, loss = 0.92123413\n",
      "Iteration 558, loss = 0.92094645\n",
      "Iteration 559, loss = 0.92065885\n",
      "Iteration 560, loss = 0.92037134\n",
      "Iteration 561, loss = 0.92008393\n",
      "Iteration 562, loss = 0.91979660\n",
      "Iteration 563, loss = 0.91950937\n",
      "Iteration 564, loss = 0.91922222\n",
      "Iteration 565, loss = 0.91893517\n",
      "Iteration 566, loss = 0.91864821\n",
      "Iteration 567, loss = 0.91836134\n",
      "Iteration 568, loss = 0.91807456\n",
      "Iteration 569, loss = 0.91778787\n",
      "Iteration 570, loss = 0.91750128\n",
      "Iteration 571, loss = 0.91721478\n",
      "Iteration 572, loss = 0.91692837\n",
      "Iteration 573, loss = 0.91664205\n",
      "Iteration 574, loss = 0.91635582\n",
      "Iteration 575, loss = 0.91606969\n",
      "Iteration 576, loss = 0.91578366\n",
      "Iteration 577, loss = 0.91549771\n",
      "Iteration 578, loss = 0.91521186\n",
      "Iteration 579, loss = 0.91492611\n",
      "Iteration 580, loss = 0.91464045\n",
      "Iteration 581, loss = 0.91435488\n",
      "Iteration 582, loss = 0.91406941\n",
      "Iteration 583, loss = 0.91378403\n",
      "Iteration 584, loss = 0.91349875\n",
      "Iteration 585, loss = 0.91321357\n",
      "Iteration 586, loss = 0.91292848\n",
      "Iteration 587, loss = 0.91264349\n",
      "Iteration 588, loss = 0.91235859\n",
      "Iteration 589, loss = 0.91207379\n",
      "Iteration 590, loss = 0.91178909\n",
      "Iteration 591, loss = 0.91150448\n",
      "Iteration 592, loss = 0.91121997\n",
      "Iteration 593, loss = 0.91093557\n",
      "Iteration 594, loss = 0.91065125\n",
      "Iteration 595, loss = 0.91036704\n",
      "Iteration 596, loss = 0.91008293\n",
      "Iteration 597, loss = 0.90979891\n",
      "Iteration 598, loss = 0.90951499\n",
      "Iteration 599, loss = 0.90923118\n",
      "Iteration 600, loss = 0.90894746\n",
      "Iteration 601, loss = 0.90866384\n",
      "Iteration 602, loss = 0.90838032\n",
      "Iteration 603, loss = 0.90809691\n",
      "Iteration 604, loss = 0.90781359\n",
      "Iteration 605, loss = 0.90753038\n",
      "Iteration 606, loss = 0.90724726\n",
      "Iteration 607, loss = 0.90696425\n",
      "Iteration 608, loss = 0.90668134\n",
      "Iteration 609, loss = 0.90639854\n",
      "Iteration 610, loss = 0.90611583\n",
      "Iteration 611, loss = 0.90583323\n",
      "Iteration 612, loss = 0.90555073\n",
      "Iteration 613, loss = 0.90526834\n",
      "Iteration 614, loss = 0.90498605\n",
      "Iteration 615, loss = 0.90470386\n",
      "Iteration 616, loss = 0.90442178\n",
      "Iteration 617, loss = 0.90413980\n",
      "Iteration 618, loss = 0.90385792\n",
      "Iteration 619, loss = 0.90357616\n",
      "Iteration 620, loss = 0.90329449\n",
      "Iteration 621, loss = 0.90301294\n",
      "Iteration 622, loss = 0.90273149\n",
      "Iteration 623, loss = 0.90245014\n",
      "Iteration 624, loss = 0.90216890\n",
      "Iteration 625, loss = 0.90188777\n",
      "Iteration 626, loss = 0.90160675\n",
      "Iteration 627, loss = 0.90132584\n",
      "Iteration 628, loss = 0.90104503\n",
      "Iteration 629, loss = 0.90076433\n",
      "Iteration 630, loss = 0.90048374\n",
      "Iteration 631, loss = 0.90020326\n",
      "Iteration 632, loss = 0.89992288\n",
      "Iteration 633, loss = 0.89964262\n",
      "Iteration 634, loss = 0.89936247\n",
      "Iteration 635, loss = 0.89908242\n",
      "Iteration 636, loss = 0.89880249\n",
      "Iteration 637, loss = 0.89852267\n",
      "Iteration 638, loss = 0.89824296\n",
      "Iteration 639, loss = 0.89796336\n",
      "Iteration 640, loss = 0.89768387\n",
      "Iteration 641, loss = 0.89740449\n",
      "Iteration 642, loss = 0.89712523\n",
      "Iteration 643, loss = 0.89684607\n",
      "Iteration 644, loss = 0.89656703\n",
      "Iteration 645, loss = 0.89628811\n",
      "Iteration 646, loss = 0.89600930\n",
      "Iteration 647, loss = 0.89573060\n",
      "Iteration 648, loss = 0.89545201\n",
      "Iteration 649, loss = 0.89517354\n",
      "Iteration 650, loss = 0.89489518\n",
      "Iteration 651, loss = 0.89461694\n",
      "Iteration 652, loss = 0.89433882\n",
      "Iteration 653, loss = 0.89406081\n",
      "Iteration 654, loss = 0.89378291\n",
      "Iteration 655, loss = 0.89350513\n",
      "Iteration 656, loss = 0.89322747\n",
      "Iteration 657, loss = 0.89294993\n",
      "Iteration 658, loss = 0.89267250\n",
      "Iteration 659, loss = 0.89239519\n",
      "Iteration 660, loss = 0.89211799\n",
      "Iteration 661, loss = 0.89184092\n",
      "Iteration 662, loss = 0.89156396\n",
      "Iteration 663, loss = 0.89128712\n",
      "Iteration 664, loss = 0.89101040\n",
      "Iteration 665, loss = 0.89073380\n",
      "Iteration 666, loss = 0.89045732\n",
      "Iteration 667, loss = 0.89018096\n",
      "Iteration 668, loss = 0.88990472\n",
      "Iteration 669, loss = 0.88962860\n",
      "Iteration 670, loss = 0.88935260\n",
      "Iteration 671, loss = 0.88907672\n",
      "Iteration 672, loss = 0.88880096\n",
      "Iteration 673, loss = 0.88852533\n",
      "Iteration 674, loss = 0.88824981\n",
      "Iteration 675, loss = 0.88797442\n",
      "Iteration 676, loss = 0.88769915\n",
      "Iteration 677, loss = 0.88742400\n",
      "Iteration 678, loss = 0.88714898\n",
      "Iteration 679, loss = 0.88687408\n",
      "Iteration 680, loss = 0.88659930\n",
      "Iteration 681, loss = 0.88632465\n",
      "Iteration 682, loss = 0.88605012\n",
      "Iteration 683, loss = 0.88577572\n",
      "Iteration 684, loss = 0.88550144\n",
      "Iteration 685, loss = 0.88522728\n",
      "Iteration 686, loss = 0.88495325\n",
      "Iteration 687, loss = 0.88467935\n",
      "Iteration 688, loss = 0.88440557\n",
      "Iteration 689, loss = 0.88413192\n",
      "Iteration 690, loss = 0.88385840\n",
      "Iteration 691, loss = 0.88358500\n",
      "Iteration 692, loss = 0.88331173\n",
      "Iteration 693, loss = 0.88303858\n",
      "Iteration 694, loss = 0.88276557\n",
      "Iteration 695, loss = 0.88249268\n",
      "Iteration 696, loss = 0.88221992\n",
      "Iteration 697, loss = 0.88194729\n",
      "Iteration 698, loss = 0.88167479\n",
      "Iteration 699, loss = 0.88140242\n",
      "Iteration 700, loss = 0.88113017\n",
      "Iteration 701, loss = 0.88085806\n",
      "Iteration 702, loss = 0.88058607\n",
      "Iteration 703, loss = 0.88031422\n",
      "Iteration 704, loss = 0.88004250\n",
      "Iteration 705, loss = 0.87977090\n",
      "Iteration 706, loss = 0.87949944\n",
      "Iteration 707, loss = 0.87922811\n",
      "Iteration 708, loss = 0.87895691\n",
      "Iteration 709, loss = 0.87868585\n",
      "Iteration 710, loss = 0.87841491\n",
      "Iteration 711, loss = 0.87814411\n",
      "Iteration 712, loss = 0.87787344\n",
      "Iteration 713, loss = 0.87760290\n",
      "Iteration 714, loss = 0.87733250\n",
      "Iteration 715, loss = 0.87706223\n",
      "Iteration 716, loss = 0.87679209\n",
      "Iteration 717, loss = 0.87652209\n",
      "Iteration 718, loss = 0.87625222\n",
      "Iteration 719, loss = 0.87598249\n",
      "Iteration 720, loss = 0.87571289\n",
      "Iteration 721, loss = 0.87544343\n",
      "Iteration 722, loss = 0.87517410\n",
      "Iteration 723, loss = 0.87490490\n",
      "Iteration 724, loss = 0.87463585\n",
      "Iteration 725, loss = 0.87436693\n",
      "Iteration 726, loss = 0.87409814\n",
      "Iteration 727, loss = 0.87382949\n",
      "Iteration 728, loss = 0.87356098\n",
      "Iteration 729, loss = 0.87329261\n",
      "Iteration 730, loss = 0.87302437\n",
      "Iteration 731, loss = 0.87275627\n",
      "Iteration 732, loss = 0.87248831\n",
      "Iteration 733, loss = 0.87222048\n",
      "Iteration 734, loss = 0.87195280\n",
      "Iteration 735, loss = 0.87168525\n",
      "Iteration 736, loss = 0.87141784\n",
      "Iteration 737, loss = 0.87115058\n",
      "Iteration 738, loss = 0.87088345\n",
      "Iteration 739, loss = 0.87061646\n",
      "Iteration 740, loss = 0.87034961\n",
      "Iteration 741, loss = 0.87008290\n",
      "Iteration 742, loss = 0.86981633\n",
      "Iteration 743, loss = 0.86954990\n",
      "Iteration 744, loss = 0.86928361\n",
      "Iteration 745, loss = 0.86901746\n",
      "Iteration 746, loss = 0.86875146\n",
      "Iteration 747, loss = 0.86848559\n",
      "Iteration 748, loss = 0.86821987\n",
      "Iteration 749, loss = 0.86795429\n",
      "Iteration 750, loss = 0.86768885\n",
      "Iteration 751, loss = 0.86742355\n",
      "Iteration 752, loss = 0.86715840\n",
      "Iteration 753, loss = 0.86689338\n",
      "Iteration 754, loss = 0.86662852\n",
      "Iteration 755, loss = 0.86636379\n",
      "Iteration 756, loss = 0.86609921\n",
      "Iteration 757, loss = 0.86583477\n",
      "Iteration 758, loss = 0.86557048\n",
      "Iteration 759, loss = 0.86530633\n",
      "Iteration 760, loss = 0.86504232\n",
      "Iteration 761, loss = 0.86477846\n",
      "Iteration 762, loss = 0.86451475\n",
      "Iteration 763, loss = 0.86425118\n",
      "Iteration 764, loss = 0.86398775\n",
      "Iteration 765, loss = 0.86372447\n",
      "Iteration 766, loss = 0.86346133\n",
      "Iteration 767, loss = 0.86319835\n",
      "Iteration 768, loss = 0.86293550\n",
      "Iteration 769, loss = 0.86267281\n",
      "Iteration 770, loss = 0.86241026\n",
      "Iteration 771, loss = 0.86214785\n",
      "Iteration 772, loss = 0.86188560\n",
      "Iteration 773, loss = 0.86162349\n",
      "Iteration 774, loss = 0.86136153\n",
      "Iteration 775, loss = 0.86109971\n",
      "Iteration 776, loss = 0.86083805\n",
      "Iteration 777, loss = 0.86057653\n",
      "Iteration 778, loss = 0.86031516\n",
      "Iteration 779, loss = 0.86005394\n",
      "Iteration 780, loss = 0.85979286\n",
      "Iteration 781, loss = 0.85953194\n",
      "Iteration 782, loss = 0.85927116\n",
      "Iteration 783, loss = 0.85901054\n",
      "Iteration 784, loss = 0.85875006\n",
      "Iteration 785, loss = 0.85848973\n",
      "Iteration 786, loss = 0.85822955\n",
      "Iteration 787, loss = 0.85796952\n",
      "Iteration 788, loss = 0.85770964\n",
      "Iteration 789, loss = 0.85744992\n",
      "Iteration 790, loss = 0.85719034\n",
      "Iteration 791, loss = 0.85693091\n",
      "Iteration 792, loss = 0.85667163\n",
      "Iteration 793, loss = 0.85641251\n",
      "Iteration 794, loss = 0.85615353\n",
      "Iteration 795, loss = 0.85589471\n",
      "Iteration 796, loss = 0.85563604\n",
      "Iteration 797, loss = 0.85537752\n",
      "Iteration 798, loss = 0.85511915\n",
      "Iteration 799, loss = 0.85486093\n",
      "Iteration 800, loss = 0.85460287\n",
      "Iteration 801, loss = 0.85434496\n",
      "Iteration 802, loss = 0.85408720\n",
      "Iteration 803, loss = 0.85382959\n",
      "Iteration 804, loss = 0.85357214\n",
      "Iteration 805, loss = 0.85331484\n",
      "Iteration 806, loss = 0.85305769\n",
      "Iteration 807, loss = 0.85280070\n",
      "Iteration 808, loss = 0.85254386\n",
      "Iteration 809, loss = 0.85228717\n",
      "Iteration 810, loss = 0.85203064\n",
      "Iteration 811, loss = 0.85177426\n",
      "Iteration 812, loss = 0.85151803\n",
      "Iteration 813, loss = 0.85126196\n",
      "Iteration 814, loss = 0.85100605\n",
      "Iteration 815, loss = 0.85075029\n",
      "Iteration 816, loss = 0.85049468\n",
      "Iteration 817, loss = 0.85023923\n",
      "Iteration 818, loss = 0.84998393\n",
      "Iteration 819, loss = 0.84972879\n",
      "Iteration 820, loss = 0.84947380\n",
      "Iteration 821, loss = 0.84921897\n",
      "Iteration 822, loss = 0.84896430\n",
      "Iteration 823, loss = 0.84870978\n",
      "Iteration 824, loss = 0.84845542\n",
      "Iteration 825, loss = 0.84820121\n",
      "Iteration 826, loss = 0.84794716\n",
      "Iteration 827, loss = 0.84769327\n",
      "Iteration 828, loss = 0.84743953\n",
      "Iteration 829, loss = 0.84718595\n",
      "Iteration 830, loss = 0.84693252\n",
      "Iteration 831, loss = 0.84667926\n",
      "Iteration 832, loss = 0.84642615\n",
      "Iteration 833, loss = 0.84617319\n",
      "Iteration 834, loss = 0.84592040\n",
      "Iteration 835, loss = 0.84566776\n",
      "Iteration 836, loss = 0.84541528\n",
      "Iteration 837, loss = 0.84516296\n",
      "Iteration 838, loss = 0.84491079\n",
      "Iteration 839, loss = 0.84465878\n",
      "Iteration 840, loss = 0.84440693\n",
      "Iteration 841, loss = 0.84415524\n",
      "Iteration 842, loss = 0.84390371\n",
      "Iteration 843, loss = 0.84365234\n",
      "Iteration 844, loss = 0.84340112\n",
      "Iteration 845, loss = 0.84315007\n",
      "Iteration 846, loss = 0.84289917\n",
      "Iteration 847, loss = 0.84264843\n",
      "Iteration 848, loss = 0.84239785\n",
      "Iteration 849, loss = 0.84214743\n",
      "Iteration 850, loss = 0.84189717\n",
      "Iteration 851, loss = 0.84164707\n",
      "Iteration 852, loss = 0.84139713\n",
      "Iteration 853, loss = 0.84114735\n",
      "Iteration 854, loss = 0.84089772\n",
      "Iteration 855, loss = 0.84064826\n",
      "Iteration 856, loss = 0.84039896\n",
      "Iteration 857, loss = 0.84014982\n",
      "Iteration 858, loss = 0.83990083\n",
      "Iteration 859, loss = 0.83965201\n",
      "Iteration 860, loss = 0.83940335\n",
      "Iteration 861, loss = 0.83915485\n",
      "Iteration 862, loss = 0.83890651\n",
      "Iteration 863, loss = 0.83865833\n",
      "Iteration 864, loss = 0.83841031\n",
      "Iteration 865, loss = 0.83816245\n",
      "Iteration 866, loss = 0.83791476\n",
      "Iteration 867, loss = 0.83766722\n",
      "Iteration 868, loss = 0.83741985\n",
      "Iteration 869, loss = 0.83717264\n",
      "Iteration 870, loss = 0.83692558\n",
      "Iteration 871, loss = 0.83667869\n",
      "Iteration 872, loss = 0.83643197\n",
      "Iteration 873, loss = 0.83618540\n",
      "Iteration 874, loss = 0.83593899\n",
      "Iteration 875, loss = 0.83569275\n",
      "Iteration 876, loss = 0.83544667\n",
      "Iteration 877, loss = 0.83520075\n",
      "Iteration 878, loss = 0.83495499\n",
      "Iteration 879, loss = 0.83470940\n",
      "Iteration 880, loss = 0.83446397\n",
      "Iteration 881, loss = 0.83421870\n",
      "Iteration 882, loss = 0.83397359\n",
      "Iteration 883, loss = 0.83372864\n",
      "Iteration 884, loss = 0.83348386\n",
      "Iteration 885, loss = 0.83323924\n",
      "Iteration 886, loss = 0.83299478\n",
      "Iteration 887, loss = 0.83275049\n",
      "Iteration 888, loss = 0.83250636\n",
      "Iteration 889, loss = 0.83226239\n",
      "Iteration 890, loss = 0.83201859\n",
      "Iteration 891, loss = 0.83177494\n",
      "Iteration 892, loss = 0.83153146\n",
      "Iteration 893, loss = 0.83128815\n",
      "Iteration 894, loss = 0.83104500\n",
      "Iteration 895, loss = 0.83080201\n",
      "Iteration 896, loss = 0.83055918\n",
      "Iteration 897, loss = 0.83031652\n",
      "Iteration 898, loss = 0.83007402\n",
      "Iteration 899, loss = 0.82983169\n",
      "Iteration 900, loss = 0.82958952\n",
      "Iteration 901, loss = 0.82934751\n",
      "Iteration 902, loss = 0.82910567\n",
      "Iteration 903, loss = 0.82886399\n",
      "Iteration 904, loss = 0.82862248\n",
      "Iteration 905, loss = 0.82838113\n",
      "Iteration 906, loss = 0.82813994\n",
      "Iteration 907, loss = 0.82789892\n",
      "Iteration 908, loss = 0.82765806\n",
      "Iteration 909, loss = 0.82741736\n",
      "Iteration 910, loss = 0.82717683\n",
      "Iteration 911, loss = 0.82693647\n",
      "Iteration 912, loss = 0.82669627\n",
      "Iteration 913, loss = 0.82645623\n",
      "Iteration 914, loss = 0.82621636\n",
      "Iteration 915, loss = 0.82597665\n",
      "Iteration 916, loss = 0.82573711\n",
      "Iteration 917, loss = 0.82549773\n",
      "Iteration 918, loss = 0.82525852\n",
      "Iteration 919, loss = 0.82501947\n",
      "Iteration 920, loss = 0.82478059\n",
      "Iteration 921, loss = 0.82454187\n",
      "Iteration 922, loss = 0.82430332\n",
      "Iteration 923, loss = 0.82406493\n",
      "Iteration 924, loss = 0.82382671\n",
      "Iteration 925, loss = 0.82358865\n",
      "Iteration 926, loss = 0.82335076\n",
      "Iteration 927, loss = 0.82311303\n",
      "Iteration 928, loss = 0.82287546\n",
      "Iteration 929, loss = 0.82263807\n",
      "Iteration 930, loss = 0.82240083\n",
      "Iteration 931, loss = 0.82216377\n",
      "Iteration 932, loss = 0.82192687\n",
      "Iteration 933, loss = 0.82169013\n",
      "Iteration 934, loss = 0.82145356\n",
      "Iteration 935, loss = 0.82121715\n",
      "Iteration 936, loss = 0.82098091\n",
      "Iteration 937, loss = 0.82074484\n",
      "Iteration 938, loss = 0.82050893\n",
      "Iteration 939, loss = 0.82027318\n",
      "Iteration 940, loss = 0.82003761\n",
      "Iteration 941, loss = 0.81980219\n",
      "Iteration 942, loss = 0.81956695\n",
      "Iteration 943, loss = 0.81933187\n",
      "Iteration 944, loss = 0.81909695\n",
      "Iteration 945, loss = 0.81886220\n",
      "Iteration 946, loss = 0.81862762\n",
      "Iteration 947, loss = 0.81839320\n",
      "Iteration 948, loss = 0.81815895\n",
      "Iteration 949, loss = 0.81792486\n",
      "Iteration 950, loss = 0.81769094\n",
      "Iteration 951, loss = 0.81745718\n",
      "Iteration 952, loss = 0.81722360\n",
      "Iteration 953, loss = 0.81699017\n",
      "Iteration 954, loss = 0.81675691\n",
      "Iteration 955, loss = 0.81652382\n",
      "Iteration 956, loss = 0.81629090\n",
      "Iteration 957, loss = 0.81605814\n",
      "Iteration 958, loss = 0.81582554\n",
      "Iteration 959, loss = 0.81559312\n",
      "Iteration 960, loss = 0.81536085\n",
      "Iteration 961, loss = 0.81512876\n",
      "Iteration 962, loss = 0.81489683\n",
      "Iteration 963, loss = 0.81466507\n",
      "Iteration 964, loss = 0.81443347\n",
      "Iteration 965, loss = 0.81420204\n",
      "Iteration 966, loss = 0.81397077\n",
      "Iteration 967, loss = 0.81373967\n",
      "Iteration 968, loss = 0.81350874\n",
      "Iteration 969, loss = 0.81327797\n",
      "Iteration 970, loss = 0.81304737\n",
      "Iteration 971, loss = 0.81281693\n",
      "Iteration 972, loss = 0.81258666\n",
      "Iteration 973, loss = 0.81235656\n",
      "Iteration 974, loss = 0.81212662\n",
      "Iteration 975, loss = 0.81189685\n",
      "Iteration 976, loss = 0.81166725\n",
      "Iteration 977, loss = 0.81143781\n",
      "Iteration 978, loss = 0.81120853\n",
      "Iteration 979, loss = 0.81097943\n",
      "Iteration 980, loss = 0.81075049\n",
      "Iteration 981, loss = 0.81052171\n",
      "Iteration 982, loss = 0.81029310\n",
      "Iteration 983, loss = 0.81006466\n",
      "Iteration 984, loss = 0.80983638\n",
      "Iteration 985, loss = 0.80960827\n",
      "Iteration 986, loss = 0.80938033\n",
      "Iteration 987, loss = 0.80915255\n",
      "Iteration 988, loss = 0.80892494\n",
      "Iteration 989, loss = 0.80869749\n",
      "Iteration 990, loss = 0.80847021\n",
      "Iteration 991, loss = 0.80824310\n",
      "Iteration 992, loss = 0.80801615\n",
      "Iteration 993, loss = 0.80778936\n",
      "Iteration 994, loss = 0.80756275\n",
      "Iteration 995, loss = 0.80733630\n",
      "Iteration 996, loss = 0.80711001\n",
      "Iteration 997, loss = 0.80688389\n",
      "Iteration 998, loss = 0.80665794\n",
      "Iteration 999, loss = 0.80643215\n",
      "Iteration 1000, loss = 0.80620653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhruv/indexCode/cmpsc445/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.19607120\n",
      "Iteration 2, loss = 1.19567926\n",
      "Iteration 3, loss = 1.19512237\n",
      "Iteration 4, loss = 1.19441920\n",
      "Iteration 5, loss = 1.19358698\n",
      "Iteration 6, loss = 1.19264149\n",
      "Iteration 7, loss = 1.19159722\n",
      "Iteration 8, loss = 1.19046737\n",
      "Iteration 9, loss = 1.18926397\n",
      "Iteration 10, loss = 1.18799793\n",
      "Iteration 11, loss = 1.18667913\n",
      "Iteration 12, loss = 1.18531650\n",
      "Iteration 13, loss = 1.18391809\n",
      "Iteration 14, loss = 1.18249115\n",
      "Iteration 15, loss = 1.18104217\n",
      "Iteration 16, loss = 1.17957697\n",
      "Iteration 17, loss = 1.17810074\n",
      "Iteration 18, loss = 1.17661811\n",
      "Iteration 19, loss = 1.17513319\n",
      "Iteration 20, loss = 1.17364963\n",
      "Iteration 21, loss = 1.17217065\n",
      "Iteration 22, loss = 1.17069909\n",
      "Iteration 23, loss = 1.16923743\n",
      "Iteration 24, loss = 1.16778785\n",
      "Iteration 25, loss = 1.16635223\n",
      "Iteration 26, loss = 1.16493222\n",
      "Iteration 27, loss = 1.16352921\n",
      "Iteration 28, loss = 1.16214442\n",
      "Iteration 29, loss = 1.16077884\n",
      "Iteration 30, loss = 1.15943334\n",
      "Iteration 31, loss = 1.15810861\n",
      "Iteration 32, loss = 1.15680521\n",
      "Iteration 33, loss = 1.15552361\n",
      "Iteration 34, loss = 1.15426414\n",
      "Iteration 35, loss = 1.15302705\n",
      "Iteration 36, loss = 1.15181251\n",
      "Iteration 37, loss = 1.15062061\n",
      "Iteration 38, loss = 1.14945139\n",
      "Iteration 39, loss = 1.14830481\n",
      "Iteration 40, loss = 1.14718079\n",
      "Iteration 41, loss = 1.14607922\n",
      "Iteration 42, loss = 1.14499992\n",
      "Iteration 43, loss = 1.14394270\n",
      "Iteration 44, loss = 1.14290735\n",
      "Iteration 45, loss = 1.14189360\n",
      "Iteration 46, loss = 1.14090119\n",
      "Iteration 47, loss = 1.13992983\n",
      "Iteration 48, loss = 1.13897921\n",
      "Iteration 49, loss = 1.13804902\n",
      "Iteration 50, loss = 1.13713892\n",
      "Iteration 51, loss = 1.13624859\n",
      "Iteration 52, loss = 1.13537769\n",
      "Iteration 53, loss = 1.13452585\n",
      "Iteration 54, loss = 1.13369274\n",
      "Iteration 55, loss = 1.13287800\n",
      "Iteration 56, loss = 1.13208128\n",
      "Iteration 57, loss = 1.13130223\n",
      "Iteration 58, loss = 1.13054048\n",
      "Iteration 59, loss = 1.12979569\n",
      "Iteration 60, loss = 1.12906752\n",
      "Iteration 61, loss = 1.12835560\n",
      "Iteration 62, loss = 1.12765961\n",
      "Iteration 63, loss = 1.12697920\n",
      "Iteration 64, loss = 1.12631404\n",
      "Iteration 65, loss = 1.12566379\n",
      "Iteration 66, loss = 1.12502813\n",
      "Iteration 67, loss = 1.12440675\n",
      "Iteration 68, loss = 1.12379931\n",
      "Iteration 69, loss = 1.12320553\n",
      "Iteration 70, loss = 1.12262507\n",
      "Iteration 71, loss = 1.12205766\n",
      "Iteration 72, loss = 1.12150299\n",
      "Iteration 73, loss = 1.12096078\n",
      "Iteration 74, loss = 1.12043074\n",
      "Iteration 75, loss = 1.11991260\n",
      "Iteration 76, loss = 1.11940608\n",
      "Iteration 77, loss = 1.11891091\n",
      "Iteration 78, loss = 1.11842685\n",
      "Iteration 79, loss = 1.11795363\n",
      "Iteration 80, loss = 1.11749100\n",
      "Iteration 81, loss = 1.11703871\n",
      "Iteration 82, loss = 1.11659654\n",
      "Iteration 83, loss = 1.11616425\n",
      "Iteration 84, loss = 1.11574160\n",
      "Iteration 85, loss = 1.11532837\n",
      "Iteration 86, loss = 1.11492436\n",
      "Iteration 87, loss = 1.11452933\n",
      "Iteration 88, loss = 1.11414309\n",
      "Iteration 89, loss = 1.11376543\n",
      "Iteration 90, loss = 1.11339616\n",
      "Iteration 91, loss = 1.11303507\n",
      "Iteration 92, loss = 1.11268198\n",
      "Iteration 93, loss = 1.11233670\n",
      "Iteration 94, loss = 1.11199905\n",
      "Iteration 95, loss = 1.11166885\n",
      "Iteration 96, loss = 1.11134594\n",
      "Iteration 97, loss = 1.11103014\n",
      "Iteration 98, loss = 1.11072129\n",
      "Iteration 99, loss = 1.11041923\n",
      "Iteration 100, loss = 1.11012380\n",
      "Iteration 101, loss = 1.10983484\n",
      "Iteration 102, loss = 1.10955222\n",
      "Iteration 103, loss = 1.10927577\n",
      "Iteration 104, loss = 1.10900536\n",
      "Iteration 105, loss = 1.10874086\n",
      "Iteration 106, loss = 1.10848211\n",
      "Iteration 107, loss = 1.10822900\n",
      "Iteration 108, loss = 1.10798139\n",
      "Iteration 109, loss = 1.10773915\n",
      "Iteration 110, loss = 1.10750216\n",
      "Iteration 111, loss = 1.10727031\n",
      "Iteration 112, loss = 1.10704347\n",
      "Iteration 113, loss = 1.10682152\n",
      "Iteration 114, loss = 1.10660436\n",
      "Iteration 115, loss = 1.10639188\n",
      "Iteration 116, loss = 1.10618397\n",
      "Iteration 117, loss = 1.10598052\n",
      "Iteration 118, loss = 1.10578144\n",
      "Iteration 119, loss = 1.10558661\n",
      "Iteration 120, loss = 1.10539596\n",
      "Iteration 121, loss = 1.10520937\n",
      "Iteration 122, loss = 1.10502677\n",
      "Iteration 123, loss = 1.10484805\n",
      "Iteration 124, loss = 1.10467313\n",
      "Iteration 125, loss = 1.10450193\n",
      "Iteration 126, loss = 1.10433436\n",
      "Iteration 127, loss = 1.10417033\n",
      "Iteration 128, loss = 1.10400977\n",
      "Iteration 129, loss = 1.10385260\n",
      "Iteration 130, loss = 1.10369874\n",
      "Iteration 131, loss = 1.10354812\n",
      "Iteration 132, loss = 1.10340066\n",
      "Iteration 133, loss = 1.10325630\n",
      "Iteration 134, loss = 1.10311496\n",
      "Iteration 135, loss = 1.10297657\n",
      "Iteration 136, loss = 1.10284107\n",
      "Iteration 137, loss = 1.10270840\n",
      "Iteration 138, loss = 1.10257849\n",
      "Iteration 139, loss = 1.10245128\n",
      "Iteration 140, loss = 1.10232671\n",
      "Iteration 141, loss = 1.10220471\n",
      "Iteration 142, loss = 1.10208524\n",
      "Iteration 143, loss = 1.10196823\n",
      "Iteration 144, loss = 1.10185364\n",
      "Iteration 145, loss = 1.10174140\n",
      "Iteration 146, loss = 1.10163146\n",
      "Iteration 147, loss = 1.10152378\n",
      "Iteration 148, loss = 1.10141830\n",
      "Iteration 149, loss = 1.10131498\n",
      "Iteration 150, loss = 1.10121376\n",
      "Iteration 151, loss = 1.10111460\n",
      "Iteration 152, loss = 1.10101745\n",
      "Iteration 153, loss = 1.10092228\n",
      "Iteration 154, loss = 1.10082903\n",
      "Iteration 155, loss = 1.10073766\n",
      "Iteration 156, loss = 1.10064814\n",
      "Iteration 157, loss = 1.10056041\n",
      "Iteration 158, loss = 1.10047445\n",
      "Iteration 159, loss = 1.10039021\n",
      "Iteration 160, loss = 1.10030765\n",
      "Iteration 161, loss = 1.10022674\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# 5-fold cross validation\n",
    "cv_results = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "\n",
    "msg = f\"Average Accuracy on cross-validation: {cv_results.mean()} ({cv_results.std()})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th rowspan=\"2\">Hidden Neurons</th>\n",
    "            <th colspan=\"3\">Learning Rates</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>0.001</th>\n",
    "            <th>0.01</th>\n",
    "            <th>0.1</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th>1</th>\n",
    "            <td>0.4377</td>\n",
    "            <td>0.5445</td>\n",
    "            <td>0.6757</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>3</th>\n",
    "            <td>0.6278</td>\n",
    "            <td>0.8767</td>\n",
    "            <td>0.9534</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>5</th>\n",
    "            <td>0.7144</td>\n",
    "            <td>0.9678</td>\n",
    "            <td>0.9567</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network Configuration Selected: \n",
    "- Hidden Nuerons: 5\n",
    "- Learning Rate: 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhruv/indexCode/cmpsc445/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-23 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-23 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-23 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-23 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-23 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-23 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-23 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-23 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-23 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-23 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-23 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-23 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-23 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-23 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-23 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-23 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-23 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-23 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-23 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-23 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-23 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-23 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-23\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(5,),\n",
       "              learning_rate_init=0.01, max_iter=1000, solver=&#x27;sgd&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" checked><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(5,),\n",
       "              learning_rate_init=0.01, max_iter=1000, solver=&#x27;sgd&#x27;)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(activation='logistic', hidden_layer_sizes=(5,),\n",
       "              learning_rate_init=0.01, max_iter=1000, solver='sgd')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_nuerons = 5\n",
    "learning_rate = 0.01\n",
    "\n",
    "clf_fin = MLPClassifier(\n",
    "    solver = 'sgd', \n",
    "    activation = 'logistic',                 \n",
    "    learning_rate_init = learning_rate, \n",
    "    learning_rate = 'constant', \n",
    "    max_iter = 1000, \n",
    "    verbose = False,\n",
    "    hidden_layer_sizes = (hidden_nuerons,)\n",
    ")\n",
    "\n",
    "clf_fin.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "Testing on entire test dataset `y_test`\n",
    "\n",
    "Hidden Layer Size: 5\n",
    "<br>\n",
    "Learning Rate: 0.01\n",
    "\n",
    "Mean Reported Accuracy: 98.333%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.967\n",
      "Accuracy Percentage: 96.667%\n"
     ]
    }
   ],
   "source": [
    "# predictions with the selected neural network classifier with respective hidden layers and learning rate\n",
    "results = clf_fin.predict(X_test)\n",
    "acc_score = accuracy_score(y_test, results)\n",
    "print(f\"Accuracy Score: {acc_score:.3f}\")\n",
    "print(f\"Accuracy Percentage: {acc_score*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Calculations\n",
    "\n",
    "Neural Network Questions\n",
    "\n",
    "1. Output of the hidden node H, when feed-forwarding with sigmoid function (activation function)\n",
    "\n",
    "    $ H_{out} = g(y) = \\sigma(y) = \\frac{1}{1+e^{-y}} $\n",
    "    \n",
    "    $ H_{in} = (\\sum_{i=1}^n w_i x_i) + b$\n",
    "    \n",
    "    We know: $b=0$ and $[X_1, X_2] = [0.1, 0.2]$\n",
    "\n",
    "    $ \\rarr 0.1(1) + 0.2(2) + 0 = 0.5 \\rarr H_{in} = 0.5 $\n",
    "    \n",
    "    $ \\rarr g(0.5) = \\sigma(0.5) = \\frac{1}{1+e^{-0.5}} = 0.62246 $\n",
    "\n",
    "    $ H_{out} = 0.6225 $\n",
    "    \n",
    "<br>\n",
    "\n",
    "2. $\\Delta w_{o1}$ using backpropagation\n",
    "\n",
    "    We know: $[O_1, O_2] = [0, 1]$\n",
    "\n",
    "    $ \\frac{\\partial E_{O_1}}{\\partial w_{o1}} = (\\frac{\\partial E_{O_1}}{\\partial out_{O_1}})(\\frac{\\partial out_{O_1}}{\\partial O_1})(\\frac{\\partial O_1}{\\partial w_{o1}})$ \n",
    "\n",
    "    $ E_{O_1} = \\frac{1}{2} (target_{O_1} - out_{O_1})^2 \\rarr \\frac{\\partial E_{O_1}}{\\partial out_{O_1}} = -(target_{O_1} - out_{O_1}) $\n",
    "\n",
    "    $ out_{O_1} = \\sigma (O_1) \\rarr \\frac{\\partial}{\\partial O_1} \\sigma (O_1) = \\sigma (O_1) (1 - \\sigma (O_1)) $\n",
    "\n",
    "    $ O_1 = H_{out}w_{o1} + b \\rarr \\frac{\\partial O_1}{\\partial w_{o1}} = H_{out} $\n",
    "\n",
    "    $ \\rarr \\Delta w_{o1} = [(-(target_{O_1} - out_{O_1}))(\\sigma (O_1) (1 - \\sigma (O_1)))(H_{out})] $\n",
    "\n",
    "    $ \\rarr (-(0 - 0.7764))(0.7764(1 - 0.7764))(0.6225) = 0.0839 $\n",
    "\n",
    "    $ \\Delta w_{o1} = 0.0839 $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmpsc445",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
