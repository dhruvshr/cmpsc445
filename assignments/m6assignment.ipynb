{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMPSC 445 - M6 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "dataset = pd.read_csv(url, names=names)\n",
    "\n",
    "X = dataset.values[:,0:4].astype(float)\n",
    "Y = dataset.values[:,4].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition Training and Testing Data\n",
    "Training size = 40% of Iris dataset instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Model\n",
    "\n",
    "- Multilayer Neural Network Model\n",
    "- Learning Rate: 0.001\n",
    "- 3 hidden neurons in 1 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_nuerons = 1\n",
    "alpha = 0.01\n",
    "\n",
    "clf = MLPClassifier(\n",
    "    solver = 'sgd', \n",
    "    activation = 'logistic',                 \n",
    "    learning_rate_init = alpha, \n",
    "    learning_rate = 'constant', \n",
    "    max_iter = 1000, \n",
    "    verbose = True,\n",
    "    hidden_layer_sizes = (hidden_nuerons,)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.33108626\n",
      "Iteration 2, loss = 1.32470140\n",
      "Iteration 3, loss = 1.31581477\n",
      "Iteration 4, loss = 1.30489604\n",
      "Iteration 5, loss = 1.29240300\n",
      "Iteration 6, loss = 1.27877051\n",
      "Iteration 7, loss = 1.26440181\n",
      "Iteration 8, loss = 1.24966224\n",
      "Iteration 9, loss = 1.23487474\n",
      "Iteration 10, loss = 1.22031725\n",
      "Iteration 11, loss = 1.20622170\n",
      "Iteration 12, loss = 1.19277442\n",
      "Iteration 13, loss = 1.18011802\n",
      "Iteration 14, loss = 1.16835416\n",
      "Iteration 15, loss = 1.15754736\n",
      "Iteration 16, loss = 1.14772932\n",
      "Iteration 17, loss = 1.13890361\n",
      "Iteration 18, loss = 1.13105056\n",
      "Iteration 19, loss = 1.12413192\n",
      "Iteration 20, loss = 1.11809543\n",
      "Iteration 21, loss = 1.11287880\n",
      "Iteration 22, loss = 1.10841338\n",
      "Iteration 23, loss = 1.10462724\n",
      "Iteration 24, loss = 1.10144767\n",
      "Iteration 25, loss = 1.09880326\n",
      "Iteration 26, loss = 1.09662542\n",
      "Iteration 27, loss = 1.09484952\n",
      "Iteration 28, loss = 1.09341569\n",
      "Iteration 29, loss = 1.09226921\n",
      "Iteration 30, loss = 1.09136077\n",
      "Iteration 31, loss = 1.09064645\n",
      "Iteration 32, loss = 1.09008759\n",
      "Iteration 33, loss = 1.08965053\n",
      "Iteration 34, loss = 1.08930629\n",
      "Iteration 35, loss = 1.08903019\n",
      "Iteration 36, loss = 1.08880147\n",
      "Iteration 37, loss = 1.08860283\n",
      "Iteration 38, loss = 1.08842011\n",
      "Iteration 39, loss = 1.08824183\n",
      "Iteration 40, loss = 1.08805886\n",
      "Iteration 41, loss = 1.08786407\n",
      "Iteration 42, loss = 1.08765203\n",
      "Iteration 43, loss = 1.08741871\n",
      "Iteration 44, loss = 1.08716125\n",
      "Iteration 45, loss = 1.08687770\n",
      "Iteration 46, loss = 1.08656688\n",
      "Iteration 47, loss = 1.08622816\n",
      "Iteration 48, loss = 1.08586133\n",
      "Iteration 49, loss = 1.08546651\n",
      "Iteration 50, loss = 1.08504398\n",
      "Iteration 51, loss = 1.08459416\n",
      "Iteration 52, loss = 1.08411750\n",
      "Iteration 53, loss = 1.08361445\n",
      "Iteration 54, loss = 1.08308542\n",
      "Iteration 55, loss = 1.08253072\n",
      "Iteration 56, loss = 1.08195058\n",
      "Iteration 57, loss = 1.08134511\n",
      "Iteration 58, loss = 1.08071431\n",
      "Iteration 59, loss = 1.08005808\n",
      "Iteration 60, loss = 1.07937618\n",
      "Iteration 61, loss = 1.07866831\n",
      "Iteration 62, loss = 1.07793409\n",
      "Iteration 63, loss = 1.07717304\n",
      "Iteration 64, loss = 1.07638468\n",
      "Iteration 65, loss = 1.07556849\n",
      "Iteration 66, loss = 1.07472394\n",
      "Iteration 67, loss = 1.07385053\n",
      "Iteration 68, loss = 1.07294779\n",
      "Iteration 69, loss = 1.07201532\n",
      "Iteration 70, loss = 1.07105278\n",
      "Iteration 71, loss = 1.07005994\n",
      "Iteration 72, loss = 1.06903666\n",
      "Iteration 73, loss = 1.06798289\n",
      "Iteration 74, loss = 1.06689873\n",
      "Iteration 75, loss = 1.06578436\n",
      "Iteration 76, loss = 1.06464006\n",
      "Iteration 77, loss = 1.06346622\n",
      "Iteration 78, loss = 1.06226330\n",
      "Iteration 79, loss = 1.06103180\n",
      "Iteration 80, loss = 1.05977227\n",
      "Iteration 81, loss = 1.05848524\n",
      "Iteration 82, loss = 1.05717125\n",
      "Iteration 83, loss = 1.05583075\n",
      "Iteration 84, loss = 1.05446416\n",
      "Iteration 85, loss = 1.05307177\n",
      "Iteration 86, loss = 1.05165379\n",
      "Iteration 87, loss = 1.05021028\n",
      "Iteration 88, loss = 1.04874120\n",
      "Iteration 89, loss = 1.04724639\n",
      "Iteration 90, loss = 1.04572556\n",
      "Iteration 91, loss = 1.04417836\n",
      "Iteration 92, loss = 1.04260434\n",
      "Iteration 93, loss = 1.04100303\n",
      "Iteration 94, loss = 1.03937392\n",
      "Iteration 95, loss = 1.03771652\n",
      "Iteration 96, loss = 1.03603038\n",
      "Iteration 97, loss = 1.03431512\n",
      "Iteration 98, loss = 1.03257044\n",
      "Iteration 99, loss = 1.03079614\n",
      "Iteration 100, loss = 1.02899212\n",
      "Iteration 101, loss = 1.02715842\n",
      "Iteration 102, loss = 1.02529518\n",
      "Iteration 103, loss = 1.02340268\n",
      "Iteration 104, loss = 1.02148129\n",
      "Iteration 105, loss = 1.01953145\n",
      "Iteration 106, loss = 1.01755372\n",
      "Iteration 107, loss = 1.01554871\n",
      "Iteration 108, loss = 1.01351706\n",
      "Iteration 109, loss = 1.01145945\n",
      "Iteration 110, loss = 1.00937660\n",
      "Iteration 111, loss = 1.00726922\n",
      "Iteration 112, loss = 1.00513801\n",
      "Iteration 113, loss = 1.00298368\n",
      "Iteration 114, loss = 1.00080692\n",
      "Iteration 115, loss = 0.99860843\n",
      "Iteration 116, loss = 0.99638887\n",
      "Iteration 117, loss = 0.99414891\n",
      "Iteration 118, loss = 0.99188921\n",
      "Iteration 119, loss = 0.98961045\n",
      "Iteration 120, loss = 0.98731328\n",
      "Iteration 121, loss = 0.98499839\n",
      "Iteration 122, loss = 0.98266647\n",
      "Iteration 123, loss = 0.98031821\n",
      "Iteration 124, loss = 0.97795434\n",
      "Iteration 125, loss = 0.97557559\n",
      "Iteration 126, loss = 0.97318270\n",
      "Iteration 127, loss = 0.97077643\n",
      "Iteration 128, loss = 0.96835755\n",
      "Iteration 129, loss = 0.96592682\n",
      "Iteration 130, loss = 0.96348502\n",
      "Iteration 131, loss = 0.96103294\n",
      "Iteration 132, loss = 0.95857135\n",
      "Iteration 133, loss = 0.95610101\n",
      "Iteration 134, loss = 0.95362270\n",
      "Iteration 135, loss = 0.95113717\n",
      "Iteration 136, loss = 0.94864518\n",
      "Iteration 137, loss = 0.94614746\n",
      "Iteration 138, loss = 0.94364474\n",
      "Iteration 139, loss = 0.94113774\n",
      "Iteration 140, loss = 0.93862717\n",
      "Iteration 141, loss = 0.93611372\n",
      "Iteration 142, loss = 0.93359808\n",
      "Iteration 143, loss = 0.93108092\n",
      "Iteration 144, loss = 0.92856289\n",
      "Iteration 145, loss = 0.92604465\n",
      "Iteration 146, loss = 0.92352682\n",
      "Iteration 147, loss = 0.92101003\n",
      "Iteration 148, loss = 0.91849488\n",
      "Iteration 149, loss = 0.91598196\n",
      "Iteration 150, loss = 0.91347185\n",
      "Iteration 151, loss = 0.91096510\n",
      "Iteration 152, loss = 0.90846227\n",
      "Iteration 153, loss = 0.90596387\n",
      "Iteration 154, loss = 0.90347042\n",
      "Iteration 155, loss = 0.90098242\n",
      "Iteration 156, loss = 0.89850035\n",
      "Iteration 157, loss = 0.89602465\n",
      "Iteration 158, loss = 0.89355579\n",
      "Iteration 159, loss = 0.89109419\n",
      "Iteration 160, loss = 0.88864025\n",
      "Iteration 161, loss = 0.88619438\n",
      "Iteration 162, loss = 0.88375696\n",
      "Iteration 163, loss = 0.88132835\n",
      "Iteration 164, loss = 0.87890890\n",
      "Iteration 165, loss = 0.87649894\n",
      "Iteration 166, loss = 0.87409879\n",
      "Iteration 167, loss = 0.87170876\n",
      "Iteration 168, loss = 0.86932913\n",
      "Iteration 169, loss = 0.86696018\n",
      "Iteration 170, loss = 0.86460218\n",
      "Iteration 171, loss = 0.86225537\n",
      "Iteration 172, loss = 0.85991998\n",
      "Iteration 173, loss = 0.85759624\n",
      "Iteration 174, loss = 0.85528435\n",
      "Iteration 175, loss = 0.85298452\n",
      "Iteration 176, loss = 0.85069693\n",
      "Iteration 177, loss = 0.84842175\n",
      "Iteration 178, loss = 0.84615915\n",
      "Iteration 179, loss = 0.84390928\n",
      "Iteration 180, loss = 0.84167227\n",
      "Iteration 181, loss = 0.83944827\n",
      "Iteration 182, loss = 0.83723738\n",
      "Iteration 183, loss = 0.83503973\n",
      "Iteration 184, loss = 0.83285541\n",
      "Iteration 185, loss = 0.83068452\n",
      "Iteration 186, loss = 0.82852714\n",
      "Iteration 187, loss = 0.82638335\n",
      "Iteration 188, loss = 0.82425322\n",
      "Iteration 189, loss = 0.82213680\n",
      "Iteration 190, loss = 0.82003416\n",
      "Iteration 191, loss = 0.81794533\n",
      "Iteration 192, loss = 0.81587035\n",
      "Iteration 193, loss = 0.81380926\n",
      "Iteration 194, loss = 0.81176209\n",
      "Iteration 195, loss = 0.80972885\n",
      "Iteration 196, loss = 0.80770956\n",
      "Iteration 197, loss = 0.80570422\n",
      "Iteration 198, loss = 0.80371284\n",
      "Iteration 199, loss = 0.80173542\n",
      "Iteration 200, loss = 0.79977195\n",
      "Iteration 201, loss = 0.79782241\n",
      "Iteration 202, loss = 0.79588680\n",
      "Iteration 203, loss = 0.79396508\n",
      "Iteration 204, loss = 0.79205724\n",
      "Iteration 205, loss = 0.79016325\n",
      "Iteration 206, loss = 0.78828308\n",
      "Iteration 207, loss = 0.78641668\n",
      "Iteration 208, loss = 0.78456402\n",
      "Iteration 209, loss = 0.78272506\n",
      "Iteration 210, loss = 0.78089974\n",
      "Iteration 211, loss = 0.77908803\n",
      "Iteration 212, loss = 0.77728987\n",
      "Iteration 213, loss = 0.77550521\n",
      "Iteration 214, loss = 0.77373398\n",
      "Iteration 215, loss = 0.77197614\n",
      "Iteration 216, loss = 0.77023162\n",
      "Iteration 217, loss = 0.76850035\n",
      "Iteration 218, loss = 0.76678227\n",
      "Iteration 219, loss = 0.76507732\n",
      "Iteration 220, loss = 0.76338542\n",
      "Iteration 221, loss = 0.76170651\n",
      "Iteration 222, loss = 0.76004051\n",
      "Iteration 223, loss = 0.75838734\n",
      "Iteration 224, loss = 0.75674695\n",
      "Iteration 225, loss = 0.75511924\n",
      "Iteration 226, loss = 0.75350414\n",
      "Iteration 227, loss = 0.75190157\n",
      "Iteration 228, loss = 0.75031146\n",
      "Iteration 229, loss = 0.74873372\n",
      "Iteration 230, loss = 0.74716827\n",
      "Iteration 231, loss = 0.74561503\n",
      "Iteration 232, loss = 0.74407392\n",
      "Iteration 233, loss = 0.74254485\n",
      "Iteration 234, loss = 0.74102775\n",
      "Iteration 235, loss = 0.73952252\n",
      "Iteration 236, loss = 0.73802908\n",
      "Iteration 237, loss = 0.73654734\n",
      "Iteration 238, loss = 0.73507723\n",
      "Iteration 239, loss = 0.73361865\n",
      "Iteration 240, loss = 0.73217152\n",
      "Iteration 241, loss = 0.73073575\n",
      "Iteration 242, loss = 0.72931125\n",
      "Iteration 243, loss = 0.72789795\n",
      "Iteration 244, loss = 0.72649574\n",
      "Iteration 245, loss = 0.72510456\n",
      "Iteration 246, loss = 0.72372430\n",
      "Iteration 247, loss = 0.72235488\n",
      "Iteration 248, loss = 0.72099622\n",
      "Iteration 249, loss = 0.71964823\n",
      "Iteration 250, loss = 0.71831081\n",
      "Iteration 251, loss = 0.71698390\n",
      "Iteration 252, loss = 0.71566739\n",
      "Iteration 253, loss = 0.71436121\n",
      "Iteration 254, loss = 0.71306527\n",
      "Iteration 255, loss = 0.71177948\n",
      "Iteration 256, loss = 0.71050376\n",
      "Iteration 257, loss = 0.70923802\n",
      "Iteration 258, loss = 0.70798218\n",
      "Iteration 259, loss = 0.70673615\n",
      "Iteration 260, loss = 0.70549986\n",
      "Iteration 261, loss = 0.70427321\n",
      "Iteration 262, loss = 0.70305613\n",
      "Iteration 263, loss = 0.70184853\n",
      "Iteration 264, loss = 0.70065032\n",
      "Iteration 265, loss = 0.69946144\n",
      "Iteration 266, loss = 0.69828179\n",
      "Iteration 267, loss = 0.69711130\n",
      "Iteration 268, loss = 0.69594988\n",
      "Iteration 269, loss = 0.69479746\n",
      "Iteration 270, loss = 0.69365395\n",
      "Iteration 271, loss = 0.69251928\n",
      "Iteration 272, loss = 0.69139337\n",
      "Iteration 273, loss = 0.69027614\n",
      "Iteration 274, loss = 0.68916752\n",
      "Iteration 275, loss = 0.68806742\n",
      "Iteration 276, loss = 0.68697576\n",
      "Iteration 277, loss = 0.68589249\n",
      "Iteration 278, loss = 0.68481751\n",
      "Iteration 279, loss = 0.68375076\n",
      "Iteration 280, loss = 0.68269216\n",
      "Iteration 281, loss = 0.68164163\n",
      "Iteration 282, loss = 0.68059911\n",
      "Iteration 283, loss = 0.67956452\n",
      "Iteration 284, loss = 0.67853779\n",
      "Iteration 285, loss = 0.67751884\n",
      "Iteration 286, loss = 0.67650762\n",
      "Iteration 287, loss = 0.67550404\n",
      "Iteration 288, loss = 0.67450804\n",
      "Iteration 289, loss = 0.67351955\n",
      "Iteration 290, loss = 0.67253849\n",
      "Iteration 291, loss = 0.67156481\n",
      "Iteration 292, loss = 0.67059844\n",
      "Iteration 293, loss = 0.66963930\n",
      "Iteration 294, loss = 0.66868734\n",
      "Iteration 295, loss = 0.66774248\n",
      "Iteration 296, loss = 0.66680466\n",
      "Iteration 297, loss = 0.66587382\n",
      "Iteration 298, loss = 0.66494990\n",
      "Iteration 299, loss = 0.66403282\n",
      "Iteration 300, loss = 0.66312253\n",
      "Iteration 301, loss = 0.66221897\n",
      "Iteration 302, loss = 0.66132207\n",
      "Iteration 303, loss = 0.66043177\n",
      "Iteration 304, loss = 0.65954801\n",
      "Iteration 305, loss = 0.65867074\n",
      "Iteration 306, loss = 0.65779988\n",
      "Iteration 307, loss = 0.65693539\n",
      "Iteration 308, loss = 0.65607721\n",
      "Iteration 309, loss = 0.65522527\n",
      "Iteration 310, loss = 0.65437953\n",
      "Iteration 311, loss = 0.65353992\n",
      "Iteration 312, loss = 0.65270638\n",
      "Iteration 313, loss = 0.65187887\n",
      "Iteration 314, loss = 0.65105733\n",
      "Iteration 315, loss = 0.65024170\n",
      "Iteration 316, loss = 0.64943192\n",
      "Iteration 317, loss = 0.64862796\n",
      "Iteration 318, loss = 0.64782974\n",
      "Iteration 319, loss = 0.64703722\n",
      "Iteration 320, loss = 0.64625035\n",
      "Iteration 321, loss = 0.64546908\n",
      "Iteration 322, loss = 0.64469335\n",
      "Iteration 323, loss = 0.64392312\n",
      "Iteration 324, loss = 0.64315833\n",
      "Iteration 325, loss = 0.64239894\n",
      "Iteration 326, loss = 0.64164490\n",
      "Iteration 327, loss = 0.64089615\n",
      "Iteration 328, loss = 0.64015265\n",
      "Iteration 329, loss = 0.63941436\n",
      "Iteration 330, loss = 0.63868122\n",
      "Iteration 331, loss = 0.63795319\n",
      "Iteration 332, loss = 0.63723022\n",
      "Iteration 333, loss = 0.63651227\n",
      "Iteration 334, loss = 0.63579929\n",
      "Iteration 335, loss = 0.63509123\n",
      "Iteration 336, loss = 0.63438806\n",
      "Iteration 337, loss = 0.63368973\n",
      "Iteration 338, loss = 0.63299620\n",
      "Iteration 339, loss = 0.63230741\n",
      "Iteration 340, loss = 0.63162334\n",
      "Iteration 341, loss = 0.63094394\n",
      "Iteration 342, loss = 0.63026916\n",
      "Iteration 343, loss = 0.62959896\n",
      "Iteration 344, loss = 0.62893331\n",
      "Iteration 345, loss = 0.62827216\n",
      "Iteration 346, loss = 0.62761548\n",
      "Iteration 347, loss = 0.62696321\n",
      "Iteration 348, loss = 0.62631534\n",
      "Iteration 349, loss = 0.62567180\n",
      "Iteration 350, loss = 0.62503258\n",
      "Iteration 351, loss = 0.62439762\n",
      "Iteration 352, loss = 0.62376690\n",
      "Iteration 353, loss = 0.62314037\n",
      "Iteration 354, loss = 0.62251799\n",
      "Iteration 355, loss = 0.62189974\n",
      "Iteration 356, loss = 0.62128557\n",
      "Iteration 357, loss = 0.62067545\n",
      "Iteration 358, loss = 0.62006934\n",
      "Iteration 359, loss = 0.61946721\n",
      "Iteration 360, loss = 0.61886902\n",
      "Iteration 361, loss = 0.61827474\n",
      "Iteration 362, loss = 0.61768433\n",
      "Iteration 363, loss = 0.61709777\n",
      "Iteration 364, loss = 0.61651501\n",
      "Iteration 365, loss = 0.61593603\n",
      "Iteration 366, loss = 0.61536079\n",
      "Iteration 367, loss = 0.61478925\n",
      "Iteration 368, loss = 0.61422140\n",
      "Iteration 369, loss = 0.61365719\n",
      "Iteration 370, loss = 0.61309659\n",
      "Iteration 371, loss = 0.61253958\n",
      "Iteration 372, loss = 0.61198612\n",
      "Iteration 373, loss = 0.61143618\n",
      "Iteration 374, loss = 0.61088974\n",
      "Iteration 375, loss = 0.61034675\n",
      "Iteration 376, loss = 0.60980720\n",
      "Iteration 377, loss = 0.60927105\n",
      "Iteration 378, loss = 0.60873827\n",
      "Iteration 379, loss = 0.60820884\n",
      "Iteration 380, loss = 0.60768273\n",
      "Iteration 381, loss = 0.60715990\n",
      "Iteration 382, loss = 0.60664034\n",
      "Iteration 383, loss = 0.60612401\n",
      "Iteration 384, loss = 0.60561088\n",
      "Iteration 385, loss = 0.60510094\n",
      "Iteration 386, loss = 0.60459414\n",
      "Iteration 387, loss = 0.60409048\n",
      "Iteration 388, loss = 0.60358991\n",
      "Iteration 389, loss = 0.60309242\n",
      "Iteration 390, loss = 0.60259797\n",
      "Iteration 391, loss = 0.60210655\n",
      "Iteration 392, loss = 0.60161813\n",
      "Iteration 393, loss = 0.60113268\n",
      "Iteration 394, loss = 0.60065018\n",
      "Iteration 395, loss = 0.60017060\n",
      "Iteration 396, loss = 0.59969392\n",
      "Iteration 397, loss = 0.59922011\n",
      "Iteration 398, loss = 0.59874916\n",
      "Iteration 399, loss = 0.59828104\n",
      "Iteration 400, loss = 0.59781572\n",
      "Iteration 401, loss = 0.59735318\n",
      "Iteration 402, loss = 0.59689341\n",
      "Iteration 403, loss = 0.59643637\n",
      "Iteration 404, loss = 0.59598204\n",
      "Iteration 405, loss = 0.59553041\n",
      "Iteration 406, loss = 0.59508145\n",
      "Iteration 407, loss = 0.59463513\n",
      "Iteration 408, loss = 0.59419145\n",
      "Iteration 409, loss = 0.59375037\n",
      "Iteration 410, loss = 0.59331188\n",
      "Iteration 411, loss = 0.59287595\n",
      "Iteration 412, loss = 0.59244256\n",
      "Iteration 413, loss = 0.59201170\n",
      "Iteration 414, loss = 0.59158334\n",
      "Iteration 415, loss = 0.59115746\n",
      "Iteration 416, loss = 0.59073405\n",
      "Iteration 417, loss = 0.59031308\n",
      "Iteration 418, loss = 0.58989453\n",
      "Iteration 419, loss = 0.58947839\n",
      "Iteration 420, loss = 0.58906463\n",
      "Iteration 421, loss = 0.58865324\n",
      "Iteration 422, loss = 0.58824420\n",
      "Iteration 423, loss = 0.58783749\n",
      "Iteration 424, loss = 0.58743309\n",
      "Iteration 425, loss = 0.58703099\n",
      "Iteration 426, loss = 0.58663115\n",
      "Iteration 427, loss = 0.58623358\n",
      "Iteration 428, loss = 0.58583824\n",
      "Iteration 429, loss = 0.58544513\n",
      "Iteration 430, loss = 0.58505422\n",
      "Iteration 431, loss = 0.58466550\n",
      "Iteration 432, loss = 0.58427895\n",
      "Iteration 433, loss = 0.58389455\n",
      "Iteration 434, loss = 0.58351229\n",
      "Iteration 435, loss = 0.58313215\n",
      "Iteration 436, loss = 0.58275411\n",
      "Iteration 437, loss = 0.58237817\n",
      "Iteration 438, loss = 0.58200429\n",
      "Iteration 439, loss = 0.58163247\n",
      "Iteration 440, loss = 0.58126269\n",
      "Iteration 441, loss = 0.58089493\n",
      "Iteration 442, loss = 0.58052919\n",
      "Iteration 443, loss = 0.58016544\n",
      "Iteration 444, loss = 0.57980366\n",
      "Iteration 445, loss = 0.57944385\n",
      "Iteration 446, loss = 0.57908599\n",
      "Iteration 447, loss = 0.57873006\n",
      "Iteration 448, loss = 0.57837605\n",
      "Iteration 449, loss = 0.57802395\n",
      "Iteration 450, loss = 0.57767374\n",
      "Iteration 451, loss = 0.57732540\n",
      "Iteration 452, loss = 0.57697893\n",
      "Iteration 453, loss = 0.57663430\n",
      "Iteration 454, loss = 0.57629151\n",
      "Iteration 455, loss = 0.57595054\n",
      "Iteration 456, loss = 0.57561137\n",
      "Iteration 457, loss = 0.57527400\n",
      "Iteration 458, loss = 0.57493842\n",
      "Iteration 459, loss = 0.57460459\n",
      "Iteration 460, loss = 0.57427253\n",
      "Iteration 461, loss = 0.57394220\n",
      "Iteration 462, loss = 0.57361360\n",
      "Iteration 463, loss = 0.57328672\n",
      "Iteration 464, loss = 0.57296154\n",
      "Iteration 465, loss = 0.57263805\n",
      "Iteration 466, loss = 0.57231624\n",
      "Iteration 467, loss = 0.57199610\n",
      "Iteration 468, loss = 0.57167761\n",
      "Iteration 469, loss = 0.57136076\n",
      "Iteration 470, loss = 0.57104554\n",
      "Iteration 471, loss = 0.57073194\n",
      "Iteration 472, loss = 0.57041995\n",
      "Iteration 473, loss = 0.57010955\n",
      "Iteration 474, loss = 0.56980073\n",
      "Iteration 475, loss = 0.56949349\n",
      "Iteration 476, loss = 0.56918780\n",
      "Iteration 477, loss = 0.56888367\n",
      "Iteration 478, loss = 0.56858107\n",
      "Iteration 479, loss = 0.56828000\n",
      "Iteration 480, loss = 0.56798045\n",
      "Iteration 481, loss = 0.56768241\n",
      "Iteration 482, loss = 0.56738585\n",
      "Iteration 483, loss = 0.56709079\n",
      "Iteration 484, loss = 0.56679719\n",
      "Iteration 485, loss = 0.56650506\n",
      "Iteration 486, loss = 0.56621438\n",
      "Iteration 487, loss = 0.56592515\n",
      "Iteration 488, loss = 0.56563735\n",
      "Iteration 489, loss = 0.56535097\n",
      "Iteration 490, loss = 0.56506600\n",
      "Iteration 491, loss = 0.56478243\n",
      "Iteration 492, loss = 0.56450026\n",
      "Iteration 493, loss = 0.56421947\n",
      "Iteration 494, loss = 0.56394006\n",
      "Iteration 495, loss = 0.56366201\n",
      "Iteration 496, loss = 0.56338531\n",
      "Iteration 497, loss = 0.56310996\n",
      "Iteration 498, loss = 0.56283594\n",
      "Iteration 499, loss = 0.56256325\n",
      "Iteration 500, loss = 0.56229188\n",
      "Iteration 501, loss = 0.56202182\n",
      "Iteration 502, loss = 0.56175305\n",
      "Iteration 503, loss = 0.56148558\n",
      "Iteration 504, loss = 0.56121939\n",
      "Iteration 505, loss = 0.56095447\n",
      "Iteration 506, loss = 0.56069081\n",
      "Iteration 507, loss = 0.56042841\n",
      "Iteration 508, loss = 0.56016726\n",
      "Iteration 509, loss = 0.55990734\n",
      "Iteration 510, loss = 0.55964866\n",
      "Iteration 511, loss = 0.55939120\n",
      "Iteration 512, loss = 0.55913495\n",
      "Iteration 513, loss = 0.55887991\n",
      "Iteration 514, loss = 0.55862606\n",
      "Iteration 515, loss = 0.55837341\n",
      "Iteration 516, loss = 0.55812193\n",
      "Iteration 517, loss = 0.55787163\n",
      "Iteration 518, loss = 0.55762249\n",
      "Iteration 519, loss = 0.55737451\n",
      "Iteration 520, loss = 0.55712769\n",
      "Iteration 521, loss = 0.55688200\n",
      "Iteration 522, loss = 0.55663745\n",
      "Iteration 523, loss = 0.55639403\n",
      "Iteration 524, loss = 0.55615172\n",
      "Iteration 525, loss = 0.55591053\n",
      "Iteration 526, loss = 0.55567045\n",
      "Iteration 527, loss = 0.55543146\n",
      "Iteration 528, loss = 0.55519356\n",
      "Iteration 529, loss = 0.55495675\n",
      "Iteration 530, loss = 0.55472101\n",
      "Iteration 531, loss = 0.55448635\n",
      "Iteration 532, loss = 0.55425274\n",
      "Iteration 533, loss = 0.55402019\n",
      "Iteration 534, loss = 0.55378869\n",
      "Iteration 535, loss = 0.55355823\n",
      "Iteration 536, loss = 0.55332881\n",
      "Iteration 537, loss = 0.55310041\n",
      "Iteration 538, loss = 0.55287304\n",
      "Iteration 539, loss = 0.55264668\n",
      "Iteration 540, loss = 0.55242133\n",
      "Iteration 541, loss = 0.55219698\n",
      "Iteration 542, loss = 0.55197363\n",
      "Iteration 543, loss = 0.55175127\n",
      "Iteration 544, loss = 0.55152989\n",
      "Iteration 545, loss = 0.55130949\n",
      "Iteration 546, loss = 0.55109006\n",
      "Iteration 547, loss = 0.55087159\n",
      "Iteration 548, loss = 0.55065408\n",
      "Iteration 549, loss = 0.55043752\n",
      "Iteration 550, loss = 0.55022191\n",
      "Iteration 551, loss = 0.55000724\n",
      "Iteration 552, loss = 0.54979350\n",
      "Iteration 553, loss = 0.54958070\n",
      "Iteration 554, loss = 0.54936881\n",
      "Iteration 555, loss = 0.54915784\n",
      "Iteration 556, loss = 0.54894778\n",
      "Iteration 557, loss = 0.54873863\n",
      "Iteration 558, loss = 0.54853038\n",
      "Iteration 559, loss = 0.54832302\n",
      "Iteration 560, loss = 0.54811655\n",
      "Iteration 561, loss = 0.54791096\n",
      "Iteration 562, loss = 0.54770625\n",
      "Iteration 563, loss = 0.54750241\n",
      "Iteration 564, loss = 0.54729944\n",
      "Iteration 565, loss = 0.54709733\n",
      "Iteration 566, loss = 0.54689608\n",
      "Iteration 567, loss = 0.54669567\n",
      "Iteration 568, loss = 0.54649612\n",
      "Iteration 569, loss = 0.54629740\n",
      "Iteration 570, loss = 0.54609952\n",
      "Iteration 571, loss = 0.54590246\n",
      "Iteration 572, loss = 0.54570624\n",
      "Iteration 573, loss = 0.54551083\n",
      "Iteration 574, loss = 0.54531624\n",
      "Iteration 575, loss = 0.54512245\n",
      "Iteration 576, loss = 0.54492948\n",
      "Iteration 577, loss = 0.54473730\n",
      "Iteration 578, loss = 0.54454592\n",
      "Iteration 579, loss = 0.54435533\n",
      "Iteration 580, loss = 0.54416552\n",
      "Iteration 581, loss = 0.54397650\n",
      "Iteration 582, loss = 0.54378825\n",
      "Iteration 583, loss = 0.54360077\n",
      "Iteration 584, loss = 0.54341406\n",
      "Iteration 585, loss = 0.54322812\n",
      "Iteration 586, loss = 0.54304293\n",
      "Iteration 587, loss = 0.54285849\n",
      "Iteration 588, loss = 0.54267481\n",
      "Iteration 589, loss = 0.54249186\n",
      "Iteration 590, loss = 0.54230966\n",
      "Iteration 591, loss = 0.54212819\n",
      "Iteration 592, loss = 0.54194746\n",
      "Iteration 593, loss = 0.54176745\n",
      "Iteration 594, loss = 0.54158817\n",
      "Iteration 595, loss = 0.54140960\n",
      "Iteration 596, loss = 0.54123175\n",
      "Iteration 597, loss = 0.54105461\n",
      "Iteration 598, loss = 0.54087817\n",
      "Iteration 599, loss = 0.54070243\n",
      "Iteration 600, loss = 0.54052740\n",
      "Iteration 601, loss = 0.54035305\n",
      "Iteration 602, loss = 0.54017940\n",
      "Iteration 603, loss = 0.54000643\n",
      "Iteration 604, loss = 0.53983415\n",
      "Iteration 605, loss = 0.53966254\n",
      "Iteration 606, loss = 0.53949160\n",
      "Iteration 607, loss = 0.53932133\n",
      "Iteration 608, loss = 0.53915173\n",
      "Iteration 609, loss = 0.53898280\n",
      "Iteration 610, loss = 0.53881452\n",
      "Iteration 611, loss = 0.53864689\n",
      "Iteration 612, loss = 0.53847992\n",
      "Iteration 613, loss = 0.53831359\n",
      "Iteration 614, loss = 0.53814791\n",
      "Iteration 615, loss = 0.53798286\n",
      "Iteration 616, loss = 0.53781846\n",
      "Iteration 617, loss = 0.53765468\n",
      "Iteration 618, loss = 0.53749153\n",
      "Iteration 619, loss = 0.53732901\n",
      "Iteration 620, loss = 0.53716711\n",
      "Iteration 621, loss = 0.53700583\n",
      "Iteration 622, loss = 0.53684516\n",
      "Iteration 623, loss = 0.53668510\n",
      "Iteration 624, loss = 0.53652566\n",
      "Iteration 625, loss = 0.53636681\n",
      "Iteration 626, loss = 0.53620857\n",
      "Iteration 627, loss = 0.53605092\n",
      "Iteration 628, loss = 0.53589387\n",
      "Iteration 629, loss = 0.53573741\n",
      "Iteration 630, loss = 0.53558154\n",
      "Iteration 631, loss = 0.53542625\n",
      "Iteration 632, loss = 0.53527154\n",
      "Iteration 633, loss = 0.53511741\n",
      "Iteration 634, loss = 0.53496385\n",
      "Iteration 635, loss = 0.53481087\n",
      "Iteration 636, loss = 0.53465845\n",
      "Iteration 637, loss = 0.53450660\n",
      "Iteration 638, loss = 0.53435531\n",
      "Iteration 639, loss = 0.53420458\n",
      "Iteration 640, loss = 0.53405441\n",
      "Iteration 641, loss = 0.53390479\n",
      "Iteration 642, loss = 0.53375572\n",
      "Iteration 643, loss = 0.53360719\n",
      "Iteration 644, loss = 0.53345921\n",
      "Iteration 645, loss = 0.53331177\n",
      "Iteration 646, loss = 0.53316487\n",
      "Iteration 647, loss = 0.53301850\n",
      "Iteration 648, loss = 0.53287266\n",
      "Iteration 649, loss = 0.53272735\n",
      "Iteration 650, loss = 0.53258257\n",
      "Iteration 651, loss = 0.53243831\n",
      "Iteration 652, loss = 0.53229458\n",
      "Iteration 653, loss = 0.53215136\n",
      "Iteration 654, loss = 0.53200865\n",
      "Iteration 655, loss = 0.53186646\n",
      "Iteration 656, loss = 0.53172478\n",
      "Iteration 657, loss = 0.53158360\n",
      "Iteration 658, loss = 0.53144293\n",
      "Iteration 659, loss = 0.53130275\n",
      "Iteration 660, loss = 0.53116308\n",
      "Iteration 661, loss = 0.53102390\n",
      "Iteration 662, loss = 0.53088522\n",
      "Iteration 663, loss = 0.53074703\n",
      "Iteration 664, loss = 0.53060932\n",
      "Iteration 665, loss = 0.53047210\n",
      "Iteration 666, loss = 0.53033536\n",
      "Iteration 667, loss = 0.53019910\n",
      "Iteration 668, loss = 0.53006332\n",
      "Iteration 669, loss = 0.52992802\n",
      "Iteration 670, loss = 0.52979319\n",
      "Iteration 671, loss = 0.52965882\n",
      "Iteration 672, loss = 0.52952493\n",
      "Iteration 673, loss = 0.52939150\n",
      "Iteration 674, loss = 0.52925853\n",
      "Iteration 675, loss = 0.52912603\n",
      "Iteration 676, loss = 0.52899398\n",
      "Iteration 677, loss = 0.52886238\n",
      "Iteration 678, loss = 0.52873124\n",
      "Iteration 679, loss = 0.52860055\n",
      "Iteration 680, loss = 0.52847031\n",
      "Iteration 681, loss = 0.52834052\n",
      "Iteration 682, loss = 0.52821117\n",
      "Iteration 683, loss = 0.52808226\n",
      "Iteration 684, loss = 0.52795379\n",
      "Iteration 685, loss = 0.52782575\n",
      "Iteration 686, loss = 0.52769815\n",
      "Iteration 687, loss = 0.52757098\n",
      "Iteration 688, loss = 0.52744425\n",
      "Iteration 689, loss = 0.52731794\n",
      "Iteration 690, loss = 0.52719205\n",
      "Iteration 691, loss = 0.52706659\n",
      "Iteration 692, loss = 0.52694156\n",
      "Iteration 693, loss = 0.52681694\n",
      "Iteration 694, loss = 0.52669273\n",
      "Iteration 695, loss = 0.52656895\n",
      "Iteration 696, loss = 0.52644557\n",
      "Iteration 697, loss = 0.52632261\n",
      "Iteration 698, loss = 0.52620005\n",
      "Iteration 699, loss = 0.52607790\n",
      "Iteration 700, loss = 0.52595616\n",
      "Iteration 701, loss = 0.52583481\n",
      "Iteration 702, loss = 0.52571387\n",
      "Iteration 703, loss = 0.52559333\n",
      "Iteration 704, loss = 0.52547318\n",
      "Iteration 705, loss = 0.52535343\n",
      "Iteration 706, loss = 0.52523406\n",
      "Iteration 707, loss = 0.52511509\n",
      "Iteration 708, loss = 0.52499651\n",
      "Iteration 709, loss = 0.52487831\n",
      "Iteration 710, loss = 0.52476050\n",
      "Iteration 711, loss = 0.52464307\n",
      "Iteration 712, loss = 0.52452602\n",
      "Iteration 713, loss = 0.52440935\n",
      "Iteration 714, loss = 0.52429306\n",
      "Iteration 715, loss = 0.52417714\n",
      "Iteration 716, loss = 0.52406159\n",
      "Iteration 717, loss = 0.52394642\n",
      "Iteration 718, loss = 0.52383161\n",
      "Iteration 719, loss = 0.52371717\n",
      "Iteration 720, loss = 0.52360310\n",
      "Iteration 721, loss = 0.52348939\n",
      "Iteration 722, loss = 0.52337605\n",
      "Iteration 723, loss = 0.52326306\n",
      "Iteration 724, loss = 0.52315043\n",
      "Iteration 725, loss = 0.52303816\n",
      "Iteration 726, loss = 0.52292624\n",
      "Iteration 727, loss = 0.52281468\n",
      "Iteration 728, loss = 0.52270347\n",
      "Iteration 729, loss = 0.52259261\n",
      "Iteration 730, loss = 0.52248209\n",
      "Iteration 731, loss = 0.52237192\n",
      "Iteration 732, loss = 0.52226210\n",
      "Iteration 733, loss = 0.52215262\n",
      "Iteration 734, loss = 0.52204348\n",
      "Iteration 735, loss = 0.52193468\n",
      "Iteration 736, loss = 0.52182622\n",
      "Iteration 737, loss = 0.52171809\n",
      "Iteration 738, loss = 0.52161030\n",
      "Iteration 739, loss = 0.52150284\n",
      "Iteration 740, loss = 0.52139571\n",
      "Iteration 741, loss = 0.52128892\n",
      "Iteration 742, loss = 0.52118245\n",
      "Iteration 743, loss = 0.52107630\n",
      "Iteration 744, loss = 0.52097049\n",
      "Iteration 745, loss = 0.52086499\n",
      "Iteration 746, loss = 0.52075982\n",
      "Iteration 747, loss = 0.52065497\n",
      "Iteration 748, loss = 0.52055043\n",
      "Iteration 749, loss = 0.52044621\n",
      "Iteration 750, loss = 0.52034231\n",
      "Iteration 751, loss = 0.52023873\n",
      "Iteration 752, loss = 0.52013545\n",
      "Iteration 753, loss = 0.52003249\n",
      "Iteration 754, loss = 0.51992983\n",
      "Iteration 755, loss = 0.51982749\n",
      "Iteration 756, loss = 0.51972545\n",
      "Iteration 757, loss = 0.51962372\n",
      "Iteration 758, loss = 0.51952229\n",
      "Iteration 759, loss = 0.51942116\n",
      "Iteration 760, loss = 0.51932033\n",
      "Iteration 761, loss = 0.51921980\n",
      "Iteration 762, loss = 0.51911957\n",
      "Iteration 763, loss = 0.51901964\n",
      "Iteration 764, loss = 0.51892000\n",
      "Iteration 765, loss = 0.51882066\n",
      "Iteration 766, loss = 0.51872161\n",
      "Iteration 767, loss = 0.51862284\n",
      "Iteration 768, loss = 0.51852437\n",
      "Iteration 769, loss = 0.51842619\n",
      "Iteration 770, loss = 0.51832829\n",
      "Iteration 771, loss = 0.51823068\n",
      "Iteration 772, loss = 0.51813335\n",
      "Iteration 773, loss = 0.51803631\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.32833822\n",
      "Iteration 2, loss = 1.32473323\n",
      "Iteration 3, loss = 1.31963481\n",
      "Iteration 4, loss = 1.31323724\n",
      "Iteration 5, loss = 1.30572495\n",
      "Iteration 6, loss = 1.29727237\n",
      "Iteration 7, loss = 1.28804403\n",
      "Iteration 8, loss = 1.27819498\n",
      "Iteration 9, loss = 1.26787131\n",
      "Iteration 10, loss = 1.25721074\n",
      "Iteration 11, loss = 1.24634310\n",
      "Iteration 12, loss = 1.23539081\n",
      "Iteration 13, loss = 1.22446894\n",
      "Iteration 14, loss = 1.21368506\n",
      "Iteration 15, loss = 1.20313854\n",
      "Iteration 16, loss = 1.19291949\n",
      "Iteration 17, loss = 1.18310719\n",
      "Iteration 18, loss = 1.17376838\n",
      "Iteration 19, loss = 1.16495552\n",
      "Iteration 20, loss = 1.15670546\n",
      "Iteration 21, loss = 1.14903875\n",
      "Iteration 22, loss = 1.14195991\n",
      "Iteration 23, loss = 1.13545867\n",
      "Iteration 24, loss = 1.12951203\n",
      "Iteration 25, loss = 1.12408695\n",
      "Iteration 26, loss = 1.11914326\n",
      "Iteration 27, loss = 1.11463652\n",
      "Iteration 28, loss = 1.11052046\n",
      "Iteration 29, loss = 1.10674905\n",
      "Iteration 30, loss = 1.10327794\n",
      "Iteration 31, loss = 1.10006545\n",
      "Iteration 32, loss = 1.09707312\n",
      "Iteration 33, loss = 1.09426599\n",
      "Iteration 34, loss = 1.09161264\n",
      "Iteration 35, loss = 1.08908511\n",
      "Iteration 36, loss = 1.08665872\n",
      "Iteration 37, loss = 1.08431198\n",
      "Iteration 38, loss = 1.08202636\n",
      "Iteration 39, loss = 1.07978623\n",
      "Iteration 40, loss = 1.07757874\n",
      "Iteration 41, loss = 1.07539374\n",
      "Iteration 42, loss = 1.07322371\n",
      "Iteration 43, loss = 1.07106364\n",
      "Iteration 44, loss = 1.06891096\n",
      "Iteration 45, loss = 1.06676532\n",
      "Iteration 46, loss = 1.06462830\n",
      "Iteration 47, loss = 1.06250316\n",
      "Iteration 48, loss = 1.06039433\n",
      "Iteration 49, loss = 1.05830704\n",
      "Iteration 50, loss = 1.05624674\n",
      "Iteration 51, loss = 1.05421864\n",
      "Iteration 52, loss = 1.05222726\n",
      "Iteration 53, loss = 1.05027603\n",
      "Iteration 54, loss = 1.04836700\n",
      "Iteration 55, loss = 1.04650068\n",
      "Iteration 56, loss = 1.04467602\n",
      "Iteration 57, loss = 1.04289045\n",
      "Iteration 58, loss = 1.04114004\n",
      "Iteration 59, loss = 1.03941977\n",
      "Iteration 60, loss = 1.03772378\n",
      "Iteration 61, loss = 1.03604570\n",
      "Iteration 62, loss = 1.03437896\n",
      "Iteration 63, loss = 1.03271702\n",
      "Iteration 64, loss = 1.03105371\n",
      "Iteration 65, loss = 1.02938334\n",
      "Iteration 66, loss = 1.02770092\n",
      "Iteration 67, loss = 1.02600222\n",
      "Iteration 68, loss = 1.02428386\n",
      "Iteration 69, loss = 1.02254330\n",
      "Iteration 70, loss = 1.02077881\n",
      "Iteration 71, loss = 1.01898942\n",
      "Iteration 72, loss = 1.01717483\n",
      "Iteration 73, loss = 1.01533528\n",
      "Iteration 74, loss = 1.01347147\n",
      "Iteration 75, loss = 1.01158443\n",
      "Iteration 76, loss = 1.00967539\n",
      "Iteration 77, loss = 1.00774568\n",
      "Iteration 78, loss = 1.00579663\n",
      "Iteration 79, loss = 1.00382952\n",
      "Iteration 80, loss = 1.00184547\n",
      "Iteration 81, loss = 0.99984547\n",
      "Iteration 82, loss = 0.99783030\n",
      "Iteration 83, loss = 0.99580056\n",
      "Iteration 84, loss = 0.99375666\n",
      "Iteration 85, loss = 0.99169890\n",
      "Iteration 86, loss = 0.98962742\n",
      "Iteration 87, loss = 0.98754231\n",
      "Iteration 88, loss = 0.98544361\n",
      "Iteration 89, loss = 0.98333136\n",
      "Iteration 90, loss = 0.98120562\n",
      "Iteration 91, loss = 0.97906650\n",
      "Iteration 92, loss = 0.97691420\n",
      "Iteration 93, loss = 0.97474898\n",
      "Iteration 94, loss = 0.97257119\n",
      "Iteration 95, loss = 0.97038126\n",
      "Iteration 96, loss = 0.96817968\n",
      "Iteration 97, loss = 0.96596701\n",
      "Iteration 98, loss = 0.96374386\n",
      "Iteration 99, loss = 0.96151085\n",
      "Iteration 100, loss = 0.95926862\n",
      "Iteration 101, loss = 0.95701782\n",
      "Iteration 102, loss = 0.95475909\n",
      "Iteration 103, loss = 0.95249305\n",
      "Iteration 104, loss = 0.95022028\n",
      "Iteration 105, loss = 0.94794138\n",
      "Iteration 106, loss = 0.94565689\n",
      "Iteration 107, loss = 0.94336734\n",
      "Iteration 108, loss = 0.94107324\n",
      "Iteration 109, loss = 0.93877508\n",
      "Iteration 110, loss = 0.93647334\n",
      "Iteration 111, loss = 0.93416849\n",
      "Iteration 112, loss = 0.93186099\n",
      "Iteration 113, loss = 0.92955131\n",
      "Iteration 114, loss = 0.92723990\n",
      "Iteration 115, loss = 0.92492722\n",
      "Iteration 116, loss = 0.92261371\n",
      "Iteration 117, loss = 0.92029984\n",
      "Iteration 118, loss = 0.91798605\n",
      "Iteration 119, loss = 0.91567278\n",
      "Iteration 120, loss = 0.91336048\n",
      "Iteration 121, loss = 0.91104956\n",
      "Iteration 122, loss = 0.90874047\n",
      "Iteration 123, loss = 0.90643360\n",
      "Iteration 124, loss = 0.90412937\n",
      "Iteration 125, loss = 0.90182817\n",
      "Iteration 126, loss = 0.89953038\n",
      "Iteration 127, loss = 0.89723638\n",
      "Iteration 128, loss = 0.89494652\n",
      "Iteration 129, loss = 0.89266115\n",
      "Iteration 130, loss = 0.89038061\n",
      "Iteration 131, loss = 0.88810524\n",
      "Iteration 132, loss = 0.88583535\n",
      "Iteration 133, loss = 0.88357124\n",
      "Iteration 134, loss = 0.88131323\n",
      "Iteration 135, loss = 0.87906160\n",
      "Iteration 136, loss = 0.87681664\n",
      "Iteration 137, loss = 0.87457860\n",
      "Iteration 138, loss = 0.87234777\n",
      "Iteration 139, loss = 0.87012439\n",
      "Iteration 140, loss = 0.86790871\n",
      "Iteration 141, loss = 0.86570096\n",
      "Iteration 142, loss = 0.86350138\n",
      "Iteration 143, loss = 0.86131018\n",
      "Iteration 144, loss = 0.85912758\n",
      "Iteration 145, loss = 0.85695377\n",
      "Iteration 146, loss = 0.85478895\n",
      "Iteration 147, loss = 0.85263331\n",
      "Iteration 148, loss = 0.85048701\n",
      "Iteration 149, loss = 0.84835024\n",
      "Iteration 150, loss = 0.84622314\n",
      "Iteration 151, loss = 0.84410588\n",
      "Iteration 152, loss = 0.84199860\n",
      "Iteration 153, loss = 0.83990143\n",
      "Iteration 154, loss = 0.83781451\n",
      "Iteration 155, loss = 0.83573796\n",
      "Iteration 156, loss = 0.83367190\n",
      "Iteration 157, loss = 0.83161643\n",
      "Iteration 158, loss = 0.82957167\n",
      "Iteration 159, loss = 0.82753770\n",
      "Iteration 160, loss = 0.82551463\n",
      "Iteration 161, loss = 0.82350253\n",
      "Iteration 162, loss = 0.82150148\n",
      "Iteration 163, loss = 0.81951155\n",
      "Iteration 164, loss = 0.81753282\n",
      "Iteration 165, loss = 0.81556535\n",
      "Iteration 166, loss = 0.81360919\n",
      "Iteration 167, loss = 0.81166440\n",
      "Iteration 168, loss = 0.80973101\n",
      "Iteration 169, loss = 0.80780908\n",
      "Iteration 170, loss = 0.80589864\n",
      "Iteration 171, loss = 0.80399973\n",
      "Iteration 172, loss = 0.80211237\n",
      "Iteration 173, loss = 0.80023658\n",
      "Iteration 174, loss = 0.79837239\n",
      "Iteration 175, loss = 0.79651982\n",
      "Iteration 176, loss = 0.79467887\n",
      "Iteration 177, loss = 0.79284955\n",
      "Iteration 178, loss = 0.79103188\n",
      "Iteration 179, loss = 0.78922584\n",
      "Iteration 180, loss = 0.78743144\n",
      "Iteration 181, loss = 0.78564868\n",
      "Iteration 182, loss = 0.78387755\n",
      "Iteration 183, loss = 0.78211803\n",
      "Iteration 184, loss = 0.78037011\n",
      "Iteration 185, loss = 0.77863377\n",
      "Iteration 186, loss = 0.77690900\n",
      "Iteration 187, loss = 0.77519578\n",
      "Iteration 188, loss = 0.77349408\n",
      "Iteration 189, loss = 0.77180386\n",
      "Iteration 190, loss = 0.77012512\n",
      "Iteration 191, loss = 0.76845780\n",
      "Iteration 192, loss = 0.76680189\n",
      "Iteration 193, loss = 0.76515734\n",
      "Iteration 194, loss = 0.76352412\n",
      "Iteration 195, loss = 0.76190219\n",
      "Iteration 196, loss = 0.76029151\n",
      "Iteration 197, loss = 0.75869204\n",
      "Iteration 198, loss = 0.75710372\n",
      "Iteration 199, loss = 0.75552653\n",
      "Iteration 200, loss = 0.75396041\n",
      "Iteration 201, loss = 0.75240531\n",
      "Iteration 202, loss = 0.75086118\n",
      "Iteration 203, loss = 0.74932798\n",
      "Iteration 204, loss = 0.74780565\n",
      "Iteration 205, loss = 0.74629414\n",
      "Iteration 206, loss = 0.74479339\n",
      "Iteration 207, loss = 0.74330335\n",
      "Iteration 208, loss = 0.74182396\n",
      "Iteration 209, loss = 0.74035518\n",
      "Iteration 210, loss = 0.73889693\n",
      "Iteration 211, loss = 0.73744916\n",
      "Iteration 212, loss = 0.73601181\n",
      "Iteration 213, loss = 0.73458483\n",
      "Iteration 214, loss = 0.73316814\n",
      "Iteration 215, loss = 0.73176170\n",
      "Iteration 216, loss = 0.73036543\n",
      "Iteration 217, loss = 0.72897929\n",
      "Iteration 218, loss = 0.72760319\n",
      "Iteration 219, loss = 0.72623709\n",
      "Iteration 220, loss = 0.72488091\n",
      "Iteration 221, loss = 0.72353460\n",
      "Iteration 222, loss = 0.72219809\n",
      "Iteration 223, loss = 0.72087131\n",
      "Iteration 224, loss = 0.71955421\n",
      "Iteration 225, loss = 0.71824670\n",
      "Iteration 226, loss = 0.71694874\n",
      "Iteration 227, loss = 0.71566026\n",
      "Iteration 228, loss = 0.71438118\n",
      "Iteration 229, loss = 0.71311145\n",
      "Iteration 230, loss = 0.71185100\n",
      "Iteration 231, loss = 0.71059976\n",
      "Iteration 232, loss = 0.70935766\n",
      "Iteration 233, loss = 0.70812465\n",
      "Iteration 234, loss = 0.70690065\n",
      "Iteration 235, loss = 0.70568560\n",
      "Iteration 236, loss = 0.70447944\n",
      "Iteration 237, loss = 0.70328210\n",
      "Iteration 238, loss = 0.70209350\n",
      "Iteration 239, loss = 0.70091360\n",
      "Iteration 240, loss = 0.69974231\n",
      "Iteration 241, loss = 0.69857958\n",
      "Iteration 242, loss = 0.69742535\n",
      "Iteration 243, loss = 0.69627954\n",
      "Iteration 244, loss = 0.69514209\n",
      "Iteration 245, loss = 0.69401294\n",
      "Iteration 246, loss = 0.69289202\n",
      "Iteration 247, loss = 0.69177927\n",
      "Iteration 248, loss = 0.69067462\n",
      "Iteration 249, loss = 0.68957802\n",
      "Iteration 250, loss = 0.68848939\n",
      "Iteration 251, loss = 0.68740868\n",
      "Iteration 252, loss = 0.68633581\n",
      "Iteration 253, loss = 0.68527074\n",
      "Iteration 254, loss = 0.68421339\n",
      "Iteration 255, loss = 0.68316371\n",
      "Iteration 256, loss = 0.68212163\n",
      "Iteration 257, loss = 0.68108709\n",
      "Iteration 258, loss = 0.68006003\n",
      "Iteration 259, loss = 0.67904039\n",
      "Iteration 260, loss = 0.67802810\n",
      "Iteration 261, loss = 0.67702312\n",
      "Iteration 262, loss = 0.67602537\n",
      "Iteration 263, loss = 0.67503481\n",
      "Iteration 264, loss = 0.67405136\n",
      "Iteration 265, loss = 0.67307497\n",
      "Iteration 266, loss = 0.67210559\n",
      "Iteration 267, loss = 0.67114315\n",
      "Iteration 268, loss = 0.67018760\n",
      "Iteration 269, loss = 0.66923888\n",
      "Iteration 270, loss = 0.66829693\n",
      "Iteration 271, loss = 0.66736170\n",
      "Iteration 272, loss = 0.66643313\n",
      "Iteration 273, loss = 0.66551116\n",
      "Iteration 274, loss = 0.66459575\n",
      "Iteration 275, loss = 0.66368682\n",
      "Iteration 276, loss = 0.66278434\n",
      "Iteration 277, loss = 0.66188824\n",
      "Iteration 278, loss = 0.66099848\n",
      "Iteration 279, loss = 0.66011499\n",
      "Iteration 280, loss = 0.65923773\n",
      "Iteration 281, loss = 0.65836664\n",
      "Iteration 282, loss = 0.65750167\n",
      "Iteration 283, loss = 0.65664277\n",
      "Iteration 284, loss = 0.65578988\n",
      "Iteration 285, loss = 0.65494296\n",
      "Iteration 286, loss = 0.65410196\n",
      "Iteration 287, loss = 0.65326683\n",
      "Iteration 288, loss = 0.65243750\n",
      "Iteration 289, loss = 0.65161395\n",
      "Iteration 290, loss = 0.65079611\n",
      "Iteration 291, loss = 0.64998394\n",
      "Iteration 292, loss = 0.64917739\n",
      "Iteration 293, loss = 0.64837641\n",
      "Iteration 294, loss = 0.64758095\n",
      "Iteration 295, loss = 0.64679098\n",
      "Iteration 296, loss = 0.64600643\n",
      "Iteration 297, loss = 0.64522727\n",
      "Iteration 298, loss = 0.64445345\n",
      "Iteration 299, loss = 0.64368492\n",
      "Iteration 300, loss = 0.64292164\n",
      "Iteration 301, loss = 0.64216356\n",
      "Iteration 302, loss = 0.64141064\n",
      "Iteration 303, loss = 0.64066283\n",
      "Iteration 304, loss = 0.63992010\n",
      "Iteration 305, loss = 0.63918239\n",
      "Iteration 306, loss = 0.63844967\n",
      "Iteration 307, loss = 0.63772189\n",
      "Iteration 308, loss = 0.63699901\n",
      "Iteration 309, loss = 0.63628099\n",
      "Iteration 310, loss = 0.63556779\n",
      "Iteration 311, loss = 0.63485937\n",
      "Iteration 312, loss = 0.63415568\n",
      "Iteration 313, loss = 0.63345668\n",
      "Iteration 314, loss = 0.63276234\n",
      "Iteration 315, loss = 0.63207262\n",
      "Iteration 316, loss = 0.63138747\n",
      "Iteration 317, loss = 0.63070686\n",
      "Iteration 318, loss = 0.63003075\n",
      "Iteration 319, loss = 0.62935910\n",
      "Iteration 320, loss = 0.62869187\n",
      "Iteration 321, loss = 0.62802903\n",
      "Iteration 322, loss = 0.62737054\n",
      "Iteration 323, loss = 0.62671636\n",
      "Iteration 324, loss = 0.62606645\n",
      "Iteration 325, loss = 0.62542079\n",
      "Iteration 326, loss = 0.62477932\n",
      "Iteration 327, loss = 0.62414203\n",
      "Iteration 328, loss = 0.62350886\n",
      "Iteration 329, loss = 0.62287979\n",
      "Iteration 330, loss = 0.62225479\n",
      "Iteration 331, loss = 0.62163382\n",
      "Iteration 332, loss = 0.62101684\n",
      "Iteration 333, loss = 0.62040382\n",
      "Iteration 334, loss = 0.61979473\n",
      "Iteration 335, loss = 0.61918954\n",
      "Iteration 336, loss = 0.61858820\n",
      "Iteration 337, loss = 0.61799070\n",
      "Iteration 338, loss = 0.61739700\n",
      "Iteration 339, loss = 0.61680706\n",
      "Iteration 340, loss = 0.61622086\n",
      "Iteration 341, loss = 0.61563836\n",
      "Iteration 342, loss = 0.61505953\n",
      "Iteration 343, loss = 0.61448435\n",
      "Iteration 344, loss = 0.61391277\n",
      "Iteration 345, loss = 0.61334478\n",
      "Iteration 346, loss = 0.61278035\n",
      "Iteration 347, loss = 0.61221943\n",
      "Iteration 348, loss = 0.61166201\n",
      "Iteration 349, loss = 0.61110805\n",
      "Iteration 350, loss = 0.61055753\n",
      "Iteration 351, loss = 0.61001041\n",
      "Iteration 352, loss = 0.60946668\n",
      "Iteration 353, loss = 0.60892629\n",
      "Iteration 354, loss = 0.60838923\n",
      "Iteration 355, loss = 0.60785547\n",
      "Iteration 356, loss = 0.60732498\n",
      "Iteration 357, loss = 0.60679772\n",
      "Iteration 358, loss = 0.60627369\n",
      "Iteration 359, loss = 0.60575284\n",
      "Iteration 360, loss = 0.60523516\n",
      "Iteration 361, loss = 0.60472062\n",
      "Iteration 362, loss = 0.60420918\n",
      "Iteration 363, loss = 0.60370084\n",
      "Iteration 364, loss = 0.60319556\n",
      "Iteration 365, loss = 0.60269331\n",
      "Iteration 366, loss = 0.60219408\n",
      "Iteration 367, loss = 0.60169783\n",
      "Iteration 368, loss = 0.60120455\n",
      "Iteration 369, loss = 0.60071420\n",
      "Iteration 370, loss = 0.60022677\n",
      "Iteration 371, loss = 0.59974224\n",
      "Iteration 372, loss = 0.59926057\n",
      "Iteration 373, loss = 0.59878175\n",
      "Iteration 374, loss = 0.59830576\n",
      "Iteration 375, loss = 0.59783256\n",
      "Iteration 376, loss = 0.59736214\n",
      "Iteration 377, loss = 0.59689448\n",
      "Iteration 378, loss = 0.59642955\n",
      "Iteration 379, loss = 0.59596733\n",
      "Iteration 380, loss = 0.59550780\n",
      "Iteration 381, loss = 0.59505094\n",
      "Iteration 382, loss = 0.59459673\n",
      "Iteration 383, loss = 0.59414515\n",
      "Iteration 384, loss = 0.59369617\n",
      "Iteration 385, loss = 0.59324978\n",
      "Iteration 386, loss = 0.59280595\n",
      "Iteration 387, loss = 0.59236467\n",
      "Iteration 388, loss = 0.59192591\n",
      "Iteration 389, loss = 0.59148965\n",
      "Iteration 390, loss = 0.59105588\n",
      "Iteration 391, loss = 0.59062458\n",
      "Iteration 392, loss = 0.59019572\n",
      "Iteration 393, loss = 0.58976929\n",
      "Iteration 394, loss = 0.58934526\n",
      "Iteration 395, loss = 0.58892363\n",
      "Iteration 396, loss = 0.58850436\n",
      "Iteration 397, loss = 0.58808745\n",
      "Iteration 398, loss = 0.58767287\n",
      "Iteration 399, loss = 0.58726061\n",
      "Iteration 400, loss = 0.58685064\n",
      "Iteration 401, loss = 0.58644295\n",
      "Iteration 402, loss = 0.58603753\n",
      "Iteration 403, loss = 0.58563435\n",
      "Iteration 404, loss = 0.58523340\n",
      "Iteration 405, loss = 0.58483466\n",
      "Iteration 406, loss = 0.58443811\n",
      "Iteration 407, loss = 0.58404374\n",
      "Iteration 408, loss = 0.58365153\n",
      "Iteration 409, loss = 0.58326146\n",
      "Iteration 410, loss = 0.58287351\n",
      "Iteration 411, loss = 0.58248768\n",
      "Iteration 412, loss = 0.58210395\n",
      "Iteration 413, loss = 0.58172229\n",
      "Iteration 414, loss = 0.58134269\n",
      "Iteration 415, loss = 0.58096515\n",
      "Iteration 416, loss = 0.58058963\n",
      "Iteration 417, loss = 0.58021613\n",
      "Iteration 418, loss = 0.57984463\n",
      "Iteration 419, loss = 0.57947512\n",
      "Iteration 420, loss = 0.57910758\n",
      "Iteration 421, loss = 0.57874199\n",
      "Iteration 422, loss = 0.57837835\n",
      "Iteration 423, loss = 0.57801663\n",
      "Iteration 424, loss = 0.57765682\n",
      "Iteration 425, loss = 0.57729892\n",
      "Iteration 426, loss = 0.57694289\n",
      "Iteration 427, loss = 0.57658874\n",
      "Iteration 428, loss = 0.57623644\n",
      "Iteration 429, loss = 0.57588599\n",
      "Iteration 430, loss = 0.57553737\n",
      "Iteration 431, loss = 0.57519055\n",
      "Iteration 432, loss = 0.57484555\n",
      "Iteration 433, loss = 0.57450233\n",
      "Iteration 434, loss = 0.57416088\n",
      "Iteration 435, loss = 0.57382120\n",
      "Iteration 436, loss = 0.57348326\n",
      "Iteration 437, loss = 0.57314707\n",
      "Iteration 438, loss = 0.57281259\n",
      "Iteration 439, loss = 0.57247983\n",
      "Iteration 440, loss = 0.57214876\n",
      "Iteration 441, loss = 0.57181938\n",
      "Iteration 442, loss = 0.57149168\n",
      "Iteration 443, loss = 0.57116563\n",
      "Iteration 444, loss = 0.57084124\n",
      "Iteration 445, loss = 0.57051848\n",
      "Iteration 446, loss = 0.57019735\n",
      "Iteration 447, loss = 0.56987783\n",
      "Iteration 448, loss = 0.56955992\n",
      "Iteration 449, loss = 0.56924359\n",
      "Iteration 450, loss = 0.56892885\n",
      "Iteration 451, loss = 0.56861567\n",
      "Iteration 452, loss = 0.56830405\n",
      "Iteration 453, loss = 0.56799397\n",
      "Iteration 454, loss = 0.56768543\n",
      "Iteration 455, loss = 0.56737841\n",
      "Iteration 456, loss = 0.56707291\n",
      "Iteration 457, loss = 0.56676890\n",
      "Iteration 458, loss = 0.56646639\n",
      "Iteration 459, loss = 0.56616536\n",
      "Iteration 460, loss = 0.56586579\n",
      "Iteration 461, loss = 0.56556769\n",
      "Iteration 462, loss = 0.56527103\n",
      "Iteration 463, loss = 0.56497582\n",
      "Iteration 464, loss = 0.56468203\n",
      "Iteration 465, loss = 0.56438966\n",
      "Iteration 466, loss = 0.56409870\n",
      "Iteration 467, loss = 0.56380914\n",
      "Iteration 468, loss = 0.56352097\n",
      "Iteration 469, loss = 0.56323417\n",
      "Iteration 470, loss = 0.56294875\n",
      "Iteration 471, loss = 0.56266469\n",
      "Iteration 472, loss = 0.56238197\n",
      "Iteration 473, loss = 0.56210060\n",
      "Iteration 474, loss = 0.56182056\n",
      "Iteration 475, loss = 0.56154184\n",
      "Iteration 476, loss = 0.56126443\n",
      "Iteration 477, loss = 0.56098833\n",
      "Iteration 478, loss = 0.56071353\n",
      "Iteration 479, loss = 0.56044000\n",
      "Iteration 480, loss = 0.56016776\n",
      "Iteration 481, loss = 0.55989679\n",
      "Iteration 482, loss = 0.55962707\n",
      "Iteration 483, loss = 0.55935861\n",
      "Iteration 484, loss = 0.55909139\n",
      "Iteration 485, loss = 0.55882540\n",
      "Iteration 486, loss = 0.55856064\n",
      "Iteration 487, loss = 0.55829709\n",
      "Iteration 488, loss = 0.55803475\n",
      "Iteration 489, loss = 0.55777362\n",
      "Iteration 490, loss = 0.55751367\n",
      "Iteration 491, loss = 0.55725491\n",
      "Iteration 492, loss = 0.55699733\n",
      "Iteration 493, loss = 0.55674092\n",
      "Iteration 494, loss = 0.55648566\n",
      "Iteration 495, loss = 0.55623156\n",
      "Iteration 496, loss = 0.55597860\n",
      "Iteration 497, loss = 0.55572678\n",
      "Iteration 498, loss = 0.55547609\n",
      "Iteration 499, loss = 0.55522652\n",
      "Iteration 500, loss = 0.55497806\n",
      "Iteration 501, loss = 0.55473071\n",
      "Iteration 502, loss = 0.55448446\n",
      "Iteration 503, loss = 0.55423931\n",
      "Iteration 504, loss = 0.55399523\n",
      "Iteration 505, loss = 0.55375224\n",
      "Iteration 506, loss = 0.55351031\n",
      "Iteration 507, loss = 0.55326945\n",
      "Iteration 508, loss = 0.55302964\n",
      "Iteration 509, loss = 0.55279088\n",
      "Iteration 510, loss = 0.55255316\n",
      "Iteration 511, loss = 0.55231648\n",
      "Iteration 512, loss = 0.55208083\n",
      "Iteration 513, loss = 0.55184620\n",
      "Iteration 514, loss = 0.55161258\n",
      "Iteration 515, loss = 0.55137997\n",
      "Iteration 516, loss = 0.55114836\n",
      "Iteration 517, loss = 0.55091775\n",
      "Iteration 518, loss = 0.55068812\n",
      "Iteration 519, loss = 0.55045948\n",
      "Iteration 520, loss = 0.55023181\n",
      "Iteration 521, loss = 0.55000511\n",
      "Iteration 522, loss = 0.54977938\n",
      "Iteration 523, loss = 0.54955459\n",
      "Iteration 524, loss = 0.54933076\n",
      "Iteration 525, loss = 0.54910788\n",
      "Iteration 526, loss = 0.54888593\n",
      "Iteration 527, loss = 0.54866491\n",
      "Iteration 528, loss = 0.54844482\n",
      "Iteration 529, loss = 0.54822564\n",
      "Iteration 530, loss = 0.54800738\n",
      "Iteration 531, loss = 0.54779003\n",
      "Iteration 532, loss = 0.54757358\n",
      "Iteration 533, loss = 0.54735802\n",
      "Iteration 534, loss = 0.54714336\n",
      "Iteration 535, loss = 0.54692958\n",
      "Iteration 536, loss = 0.54671668\n",
      "Iteration 537, loss = 0.54650465\n",
      "Iteration 538, loss = 0.54629349\n",
      "Iteration 539, loss = 0.54608319\n",
      "Iteration 540, loss = 0.54587374\n",
      "Iteration 541, loss = 0.54566515\n",
      "Iteration 542, loss = 0.54545740\n",
      "Iteration 543, loss = 0.54525050\n",
      "Iteration 544, loss = 0.54504442\n",
      "Iteration 545, loss = 0.54483918\n",
      "Iteration 546, loss = 0.54463476\n",
      "Iteration 547, loss = 0.54443116\n",
      "Iteration 548, loss = 0.54422837\n",
      "Iteration 549, loss = 0.54402639\n",
      "Iteration 550, loss = 0.54382522\n",
      "Iteration 551, loss = 0.54362484\n",
      "Iteration 552, loss = 0.54342525\n",
      "Iteration 553, loss = 0.54322646\n",
      "Iteration 554, loss = 0.54302844\n",
      "Iteration 555, loss = 0.54283121\n",
      "Iteration 556, loss = 0.54263474\n",
      "Iteration 557, loss = 0.54243905\n",
      "Iteration 558, loss = 0.54224412\n",
      "Iteration 559, loss = 0.54204994\n",
      "Iteration 560, loss = 0.54185652\n",
      "Iteration 561, loss = 0.54166385\n",
      "Iteration 562, loss = 0.54147193\n",
      "Iteration 563, loss = 0.54128074\n",
      "Iteration 564, loss = 0.54109029\n",
      "Iteration 565, loss = 0.54090057\n",
      "Iteration 566, loss = 0.54071157\n",
      "Iteration 567, loss = 0.54052330\n",
      "Iteration 568, loss = 0.54033574\n",
      "Iteration 569, loss = 0.54014890\n",
      "Iteration 570, loss = 0.53996276\n",
      "Iteration 571, loss = 0.53977733\n",
      "Iteration 572, loss = 0.53959259\n",
      "Iteration 573, loss = 0.53940855\n",
      "Iteration 574, loss = 0.53922520\n",
      "Iteration 575, loss = 0.53904254\n",
      "Iteration 576, loss = 0.53886056\n",
      "Iteration 577, loss = 0.53867925\n",
      "Iteration 578, loss = 0.53849862\n",
      "Iteration 579, loss = 0.53831866\n",
      "Iteration 580, loss = 0.53813936\n",
      "Iteration 581, loss = 0.53796072\n",
      "Iteration 582, loss = 0.53778274\n",
      "Iteration 583, loss = 0.53760542\n",
      "Iteration 584, loss = 0.53742874\n",
      "Iteration 585, loss = 0.53725270\n",
      "Iteration 586, loss = 0.53707731\n",
      "Iteration 587, loss = 0.53690255\n",
      "Iteration 588, loss = 0.53672843\n",
      "Iteration 589, loss = 0.53655493\n",
      "Iteration 590, loss = 0.53638206\n",
      "Iteration 591, loss = 0.53620981\n",
      "Iteration 592, loss = 0.53603818\n",
      "Iteration 593, loss = 0.53586716\n",
      "Iteration 594, loss = 0.53569675\n",
      "Iteration 595, loss = 0.53552695\n",
      "Iteration 596, loss = 0.53535775\n",
      "Iteration 597, loss = 0.53518914\n",
      "Iteration 598, loss = 0.53502113\n",
      "Iteration 599, loss = 0.53485372\n",
      "Iteration 600, loss = 0.53468689\n",
      "Iteration 601, loss = 0.53452064\n",
      "Iteration 602, loss = 0.53435497\n",
      "Iteration 603, loss = 0.53418988\n",
      "Iteration 604, loss = 0.53402537\n",
      "Iteration 605, loss = 0.53386142\n",
      "Iteration 606, loss = 0.53369804\n",
      "Iteration 607, loss = 0.53353522\n",
      "Iteration 608, loss = 0.53337296\n",
      "Iteration 609, loss = 0.53321126\n",
      "Iteration 610, loss = 0.53305011\n",
      "Iteration 611, loss = 0.53288951\n",
      "Iteration 612, loss = 0.53272945\n",
      "Iteration 613, loss = 0.53256994\n",
      "Iteration 614, loss = 0.53241096\n",
      "Iteration 615, loss = 0.53225252\n",
      "Iteration 616, loss = 0.53209461\n",
      "Iteration 617, loss = 0.53193723\n",
      "Iteration 618, loss = 0.53178038\n",
      "Iteration 619, loss = 0.53162405\n",
      "Iteration 620, loss = 0.53146824\n",
      "Iteration 621, loss = 0.53131294\n",
      "Iteration 622, loss = 0.53115816\n",
      "Iteration 623, loss = 0.53100389\n",
      "Iteration 624, loss = 0.53085012\n",
      "Iteration 625, loss = 0.53069686\n",
      "Iteration 626, loss = 0.53054410\n",
      "Iteration 627, loss = 0.53039183\n",
      "Iteration 628, loss = 0.53024006\n",
      "Iteration 629, loss = 0.53008878\n",
      "Iteration 630, loss = 0.52993799\n",
      "Iteration 631, loss = 0.52978769\n",
      "Iteration 632, loss = 0.52963786\n",
      "Iteration 633, loss = 0.52948852\n",
      "Iteration 634, loss = 0.52933965\n",
      "Iteration 635, loss = 0.52919126\n",
      "Iteration 636, loss = 0.52904333\n",
      "Iteration 637, loss = 0.52889588\n",
      "Iteration 638, loss = 0.52874889\n",
      "Iteration 639, loss = 0.52860236\n",
      "Iteration 640, loss = 0.52845629\n",
      "Iteration 641, loss = 0.52831067\n",
      "Iteration 642, loss = 0.52816551\n",
      "Iteration 643, loss = 0.52802080\n",
      "Iteration 644, loss = 0.52787654\n",
      "Iteration 645, loss = 0.52773272\n",
      "Iteration 646, loss = 0.52758935\n",
      "Iteration 647, loss = 0.52744641\n",
      "Iteration 648, loss = 0.52730391\n",
      "Iteration 649, loss = 0.52716185\n",
      "Iteration 650, loss = 0.52702021\n",
      "Iteration 651, loss = 0.52687901\n",
      "Iteration 652, loss = 0.52673823\n",
      "Iteration 653, loss = 0.52659787\n",
      "Iteration 654, loss = 0.52645794\n",
      "Iteration 655, loss = 0.52631842\n",
      "Iteration 656, loss = 0.52617932\n",
      "Iteration 657, loss = 0.52604063\n",
      "Iteration 658, loss = 0.52590236\n",
      "Iteration 659, loss = 0.52576449\n",
      "Iteration 660, loss = 0.52562702\n",
      "Iteration 661, loss = 0.52548996\n",
      "Iteration 662, loss = 0.52535330\n",
      "Iteration 663, loss = 0.52521703\n",
      "Iteration 664, loss = 0.52508116\n",
      "Iteration 665, loss = 0.52494569\n",
      "Iteration 666, loss = 0.52481060\n",
      "Iteration 667, loss = 0.52467590\n",
      "Iteration 668, loss = 0.52454159\n",
      "Iteration 669, loss = 0.52440766\n",
      "Iteration 670, loss = 0.52427411\n",
      "Iteration 671, loss = 0.52414094\n",
      "Iteration 672, loss = 0.52400814\n",
      "Iteration 673, loss = 0.52387572\n",
      "Iteration 674, loss = 0.52374366\n",
      "Iteration 675, loss = 0.52361198\n",
      "Iteration 676, loss = 0.52348066\n",
      "Iteration 677, loss = 0.52334971\n",
      "Iteration 678, loss = 0.52321912\n",
      "Iteration 679, loss = 0.52308888\n",
      "Iteration 680, loss = 0.52295901\n",
      "Iteration 681, loss = 0.52282949\n",
      "Iteration 682, loss = 0.52270032\n",
      "Iteration 683, loss = 0.52257150\n",
      "Iteration 684, loss = 0.52244302\n",
      "Iteration 685, loss = 0.52231490\n",
      "Iteration 686, loss = 0.52218711\n",
      "Iteration 687, loss = 0.52205967\n",
      "Iteration 688, loss = 0.52193257\n",
      "Iteration 689, loss = 0.52180580\n",
      "Iteration 690, loss = 0.52167937\n",
      "Iteration 691, loss = 0.52155327\n",
      "Iteration 692, loss = 0.52142750\n",
      "Iteration 693, loss = 0.52130206\n",
      "Iteration 694, loss = 0.52117694\n",
      "Iteration 695, loss = 0.52105215\n",
      "Iteration 696, loss = 0.52092768\n",
      "Iteration 697, loss = 0.52080352\n",
      "Iteration 698, loss = 0.52067969\n",
      "Iteration 699, loss = 0.52055617\n",
      "Iteration 700, loss = 0.52043296\n",
      "Iteration 701, loss = 0.52031006\n",
      "Iteration 702, loss = 0.52018748\n",
      "Iteration 703, loss = 0.52006519\n",
      "Iteration 704, loss = 0.51994322\n",
      "Iteration 705, loss = 0.51982154\n",
      "Iteration 706, loss = 0.51970017\n",
      "Iteration 707, loss = 0.51957909\n",
      "Iteration 708, loss = 0.51945832\n",
      "Iteration 709, loss = 0.51933783\n",
      "Iteration 710, loss = 0.51921764\n",
      "Iteration 711, loss = 0.51909774\n",
      "Iteration 712, loss = 0.51897813\n",
      "Iteration 713, loss = 0.51885880\n",
      "Iteration 714, loss = 0.51873976\n",
      "Iteration 715, loss = 0.51862100\n",
      "Iteration 716, loss = 0.51850252\n",
      "Iteration 717, loss = 0.51838432\n",
      "Iteration 718, loss = 0.51826639\n",
      "Iteration 719, loss = 0.51814874\n",
      "Iteration 720, loss = 0.51803136\n",
      "Iteration 721, loss = 0.51791426\n",
      "Iteration 722, loss = 0.51779742\n",
      "Iteration 723, loss = 0.51768085\n",
      "Iteration 724, loss = 0.51756455\n",
      "Iteration 725, loss = 0.51744850\n",
      "Iteration 726, loss = 0.51733272\n",
      "Iteration 727, loss = 0.51721720\n",
      "Iteration 728, loss = 0.51710194\n",
      "Iteration 729, loss = 0.51698693\n",
      "Iteration 730, loss = 0.51687217\n",
      "Iteration 731, loss = 0.51675767\n",
      "Iteration 732, loss = 0.51664342\n",
      "Iteration 733, loss = 0.51652941\n",
      "Iteration 734, loss = 0.51641565\n",
      "Iteration 735, loss = 0.51630214\n",
      "Iteration 736, loss = 0.51618887\n",
      "Iteration 737, loss = 0.51607584\n",
      "Iteration 738, loss = 0.51596304\n",
      "Iteration 739, loss = 0.51585049\n",
      "Iteration 740, loss = 0.51573817\n",
      "Iteration 741, loss = 0.51562609\n",
      "Iteration 742, loss = 0.51551423\n",
      "Iteration 743, loss = 0.51540261\n",
      "Iteration 744, loss = 0.51529121\n",
      "Iteration 745, loss = 0.51518005\n",
      "Iteration 746, loss = 0.51506910\n",
      "Iteration 747, loss = 0.51495838\n",
      "Iteration 748, loss = 0.51484788\n",
      "Iteration 749, loss = 0.51473760\n",
      "Iteration 750, loss = 0.51462754\n",
      "Iteration 751, loss = 0.51451770\n",
      "Iteration 752, loss = 0.51440806\n",
      "Iteration 753, loss = 0.51429864\n",
      "Iteration 754, loss = 0.51418944\n",
      "Iteration 755, loss = 0.51408044\n",
      "Iteration 756, loss = 0.51397165\n",
      "Iteration 757, loss = 0.51386306\n",
      "Iteration 758, loss = 0.51375468\n",
      "Iteration 759, loss = 0.51364650\n",
      "Iteration 760, loss = 0.51353852\n",
      "Iteration 761, loss = 0.51343074\n",
      "Iteration 762, loss = 0.51332316\n",
      "Iteration 763, loss = 0.51321577\n",
      "Iteration 764, loss = 0.51310858\n",
      "Iteration 765, loss = 0.51300158\n",
      "Iteration 766, loss = 0.51289477\n",
      "Iteration 767, loss = 0.51278815\n",
      "Iteration 768, loss = 0.51268172\n",
      "Iteration 769, loss = 0.51257547\n",
      "Iteration 770, loss = 0.51246941\n",
      "Iteration 771, loss = 0.51236352\n",
      "Iteration 772, loss = 0.51225782\n",
      "Iteration 773, loss = 0.51215230\n",
      "Iteration 774, loss = 0.51204696\n",
      "Iteration 775, loss = 0.51194179\n",
      "Iteration 776, loss = 0.51183680\n",
      "Iteration 777, loss = 0.51173198\n",
      "Iteration 778, loss = 0.51162733\n",
      "Iteration 779, loss = 0.51152285\n",
      "Iteration 780, loss = 0.51141854\n",
      "Iteration 781, loss = 0.51131439\n",
      "Iteration 782, loss = 0.51121041\n",
      "Iteration 783, loss = 0.51110659\n",
      "Iteration 784, loss = 0.51100293\n",
      "Iteration 785, loss = 0.51089944\n",
      "Iteration 786, loss = 0.51079610\n",
      "Iteration 787, loss = 0.51069292\n",
      "Iteration 788, loss = 0.51058989\n",
      "Iteration 789, loss = 0.51048702\n",
      "Iteration 790, loss = 0.51038430\n",
      "Iteration 791, loss = 0.51028173\n",
      "Iteration 792, loss = 0.51017931\n",
      "Iteration 793, loss = 0.51007703\n",
      "Iteration 794, loss = 0.50997491\n",
      "Iteration 795, loss = 0.50987292\n",
      "Iteration 796, loss = 0.50977108\n",
      "Iteration 797, loss = 0.50966939\n",
      "Iteration 798, loss = 0.50956783\n",
      "Iteration 799, loss = 0.50946641\n",
      "Iteration 800, loss = 0.50936512\n",
      "Iteration 801, loss = 0.50926397\n",
      "Iteration 802, loss = 0.50916296\n",
      "Iteration 803, loss = 0.50906207\n",
      "Iteration 804, loss = 0.50896132\n",
      "Iteration 805, loss = 0.50886070\n",
      "Iteration 806, loss = 0.50876020\n",
      "Iteration 807, loss = 0.50865983\n",
      "Iteration 808, loss = 0.50855959\n",
      "Iteration 809, loss = 0.50845947\n",
      "Iteration 810, loss = 0.50835947\n",
      "Iteration 811, loss = 0.50825959\n",
      "Iteration 812, loss = 0.50815982\n",
      "Iteration 813, loss = 0.50806018\n",
      "Iteration 814, loss = 0.50796065\n",
      "Iteration 815, loss = 0.50786124\n",
      "Iteration 816, loss = 0.50776194\n",
      "Iteration 817, loss = 0.50766274\n",
      "Iteration 818, loss = 0.50756366\n",
      "Iteration 819, loss = 0.50746469\n",
      "Iteration 820, loss = 0.50736583\n",
      "Iteration 821, loss = 0.50726707\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.12969075\n",
      "Iteration 2, loss = 1.12918502\n",
      "Iteration 3, loss = 1.12848099\n",
      "Iteration 4, loss = 1.12761521\n",
      "Iteration 5, loss = 1.12662261\n",
      "Iteration 6, loss = 1.12553560\n",
      "Iteration 7, loss = 1.12438338\n",
      "Iteration 8, loss = 1.12319161\n",
      "Iteration 9, loss = 1.12198232\n",
      "Iteration 10, loss = 1.12077399\n",
      "Iteration 11, loss = 1.11958169\n",
      "Iteration 12, loss = 1.11841744\n",
      "Iteration 13, loss = 1.11729048\n",
      "Iteration 14, loss = 1.11620766\n",
      "Iteration 15, loss = 1.11517375\n",
      "Iteration 16, loss = 1.11419183\n",
      "Iteration 17, loss = 1.11326354\n",
      "Iteration 18, loss = 1.11238939\n",
      "Iteration 19, loss = 1.11156899\n",
      "Iteration 20, loss = 1.11080124\n",
      "Iteration 21, loss = 1.11008452\n",
      "Iteration 22, loss = 1.10941684\n",
      "Iteration 23, loss = 1.10879593\n",
      "Iteration 24, loss = 1.10821937\n",
      "Iteration 25, loss = 1.10768465\n",
      "Iteration 26, loss = 1.10718921\n",
      "Iteration 27, loss = 1.10673054\n",
      "Iteration 28, loss = 1.10630615\n",
      "Iteration 29, loss = 1.10591364\n",
      "Iteration 30, loss = 1.10555071\n",
      "Iteration 31, loss = 1.10521517\n",
      "Iteration 32, loss = 1.10490493\n",
      "Iteration 33, loss = 1.10461803\n",
      "Iteration 34, loss = 1.10435263\n",
      "Iteration 35, loss = 1.10410702\n",
      "Iteration 36, loss = 1.10387959\n",
      "Iteration 37, loss = 1.10366886\n",
      "Iteration 38, loss = 1.10347344\n",
      "Iteration 39, loss = 1.10329207\n",
      "Iteration 40, loss = 1.10312357\n",
      "Iteration 41, loss = 1.10296686\n",
      "Iteration 42, loss = 1.10282094\n",
      "Iteration 43, loss = 1.10268490\n",
      "Iteration 44, loss = 1.10255790\n",
      "Iteration 45, loss = 1.10243918\n",
      "Iteration 46, loss = 1.10232804\n",
      "Iteration 47, loss = 1.10222383\n",
      "Iteration 48, loss = 1.10212597\n",
      "Iteration 49, loss = 1.10203392\n",
      "Iteration 50, loss = 1.10194720\n",
      "Iteration 51, loss = 1.10186537\n",
      "Iteration 52, loss = 1.10178802\n",
      "Iteration 53, loss = 1.10171478\n",
      "Iteration 54, loss = 1.10164532\n",
      "Iteration 55, loss = 1.10157934\n",
      "Iteration 56, loss = 1.10151656\n",
      "Iteration 57, loss = 1.10145672\n",
      "Iteration 58, loss = 1.10139961\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.22750525\n",
      "Iteration 2, loss = 1.22295810\n",
      "Iteration 3, loss = 1.21659925\n",
      "Iteration 4, loss = 1.20873578\n",
      "Iteration 5, loss = 1.19966387\n",
      "Iteration 6, loss = 1.18966305\n",
      "Iteration 7, loss = 1.17899219\n",
      "Iteration 8, loss = 1.16788698\n",
      "Iteration 9, loss = 1.15655860\n",
      "Iteration 10, loss = 1.14519346\n",
      "Iteration 11, loss = 1.13395365\n",
      "Iteration 12, loss = 1.12297794\n",
      "Iteration 13, loss = 1.11238296\n",
      "Iteration 14, loss = 1.10226450\n",
      "Iteration 15, loss = 1.09269849\n",
      "Iteration 16, loss = 1.08374199\n",
      "Iteration 17, loss = 1.07543387\n",
      "Iteration 18, loss = 1.06779567\n",
      "Iteration 19, loss = 1.06083251\n",
      "Iteration 20, loss = 1.05453443\n",
      "Iteration 21, loss = 1.04887806\n",
      "Iteration 22, loss = 1.04382866\n",
      "Iteration 23, loss = 1.03934233\n",
      "Iteration 24, loss = 1.03536830\n",
      "Iteration 25, loss = 1.03185122\n",
      "Iteration 26, loss = 1.02873316\n",
      "Iteration 27, loss = 1.02595545\n",
      "Iteration 28, loss = 1.02346033\n",
      "Iteration 29, loss = 1.02119230\n",
      "Iteration 30, loss = 1.01909928\n",
      "Iteration 31, loss = 1.01713363\n",
      "Iteration 32, loss = 1.01525282\n",
      "Iteration 33, loss = 1.01342001\n",
      "Iteration 34, loss = 1.01160436\n",
      "Iteration 35, loss = 1.00978107\n",
      "Iteration 36, loss = 1.00793129\n",
      "Iteration 37, loss = 1.00604177\n",
      "Iteration 38, loss = 1.00410439\n",
      "Iteration 39, loss = 1.00211551\n",
      "Iteration 40, loss = 1.00007524\n",
      "Iteration 41, loss = 0.99798667\n",
      "Iteration 42, loss = 0.99585503\n",
      "Iteration 43, loss = 0.99368696\n",
      "Iteration 44, loss = 0.99148971\n",
      "Iteration 45, loss = 0.98927056\n",
      "Iteration 46, loss = 0.98703631\n",
      "Iteration 47, loss = 0.98479288\n",
      "Iteration 48, loss = 0.98254505\n",
      "Iteration 49, loss = 0.98029639\n",
      "Iteration 50, loss = 0.97804922\n",
      "Iteration 51, loss = 0.97580471\n",
      "Iteration 52, loss = 0.97356304\n",
      "Iteration 53, loss = 0.97132359\n",
      "Iteration 54, loss = 0.96908516\n",
      "Iteration 55, loss = 0.96684621\n",
      "Iteration 56, loss = 0.96460506\n",
      "Iteration 57, loss = 0.96236003\n",
      "Iteration 58, loss = 0.96010963\n",
      "Iteration 59, loss = 0.95785265\n",
      "Iteration 60, loss = 0.95558820\n",
      "Iteration 61, loss = 0.95331578\n",
      "Iteration 62, loss = 0.95103523\n",
      "Iteration 63, loss = 0.94874677\n",
      "Iteration 64, loss = 0.94645087\n",
      "Iteration 65, loss = 0.94414826\n",
      "Iteration 66, loss = 0.94183985\n",
      "Iteration 67, loss = 0.93952664\n",
      "Iteration 68, loss = 0.93720968\n",
      "Iteration 69, loss = 0.93489002\n",
      "Iteration 70, loss = 0.93256866\n",
      "Iteration 71, loss = 0.93024650\n",
      "Iteration 72, loss = 0.92792435\n",
      "Iteration 73, loss = 0.92560292\n",
      "Iteration 74, loss = 0.92328279\n",
      "Iteration 75, loss = 0.92096444\n",
      "Iteration 76, loss = 0.91864825\n",
      "Iteration 77, loss = 0.91633453\n",
      "Iteration 78, loss = 0.91402353\n",
      "Iteration 79, loss = 0.91171548\n",
      "Iteration 80, loss = 0.90941057\n",
      "Iteration 81, loss = 0.90710897\n",
      "Iteration 82, loss = 0.90481090\n",
      "Iteration 83, loss = 0.90251655\n",
      "Iteration 84, loss = 0.90022614\n",
      "Iteration 85, loss = 0.89793992\n",
      "Iteration 86, loss = 0.89565815\n",
      "Iteration 87, loss = 0.89338110\n",
      "Iteration 88, loss = 0.89110907\n",
      "Iteration 89, loss = 0.88884236\n",
      "Iteration 90, loss = 0.88658126\n",
      "Iteration 91, loss = 0.88432607\n",
      "Iteration 92, loss = 0.88207709\n",
      "Iteration 93, loss = 0.87983459\n",
      "Iteration 94, loss = 0.87759886\n",
      "Iteration 95, loss = 0.87537014\n",
      "Iteration 96, loss = 0.87314870\n",
      "Iteration 97, loss = 0.87093475\n",
      "Iteration 98, loss = 0.86872853\n",
      "Iteration 99, loss = 0.86653024\n",
      "Iteration 100, loss = 0.86434008\n",
      "Iteration 101, loss = 0.86215825\n",
      "Iteration 102, loss = 0.85998492\n",
      "Iteration 103, loss = 0.85782027\n",
      "Iteration 104, loss = 0.85566448\n",
      "Iteration 105, loss = 0.85351770\n",
      "Iteration 106, loss = 0.85138009\n",
      "Iteration 107, loss = 0.84925181\n",
      "Iteration 108, loss = 0.84713300\n",
      "Iteration 109, loss = 0.84502381\n",
      "Iteration 110, loss = 0.84292437\n",
      "Iteration 111, loss = 0.84083482\n",
      "Iteration 112, loss = 0.83875529\n",
      "Iteration 113, loss = 0.83668589\n",
      "Iteration 114, loss = 0.83462675\n",
      "Iteration 115, loss = 0.83257797\n",
      "Iteration 116, loss = 0.83053965\n",
      "Iteration 117, loss = 0.82851190\n",
      "Iteration 118, loss = 0.82649480\n",
      "Iteration 119, loss = 0.82448843\n",
      "Iteration 120, loss = 0.82249289\n",
      "Iteration 121, loss = 0.82050824\n",
      "Iteration 122, loss = 0.81853455\n",
      "Iteration 123, loss = 0.81657189\n",
      "Iteration 124, loss = 0.81462031\n",
      "Iteration 125, loss = 0.81267987\n",
      "Iteration 126, loss = 0.81075061\n",
      "Iteration 127, loss = 0.80883259\n",
      "Iteration 128, loss = 0.80692583\n",
      "Iteration 129, loss = 0.80503038\n",
      "Iteration 130, loss = 0.80314626\n",
      "Iteration 131, loss = 0.80127351\n",
      "Iteration 132, loss = 0.79941215\n",
      "Iteration 133, loss = 0.79756219\n",
      "Iteration 134, loss = 0.79572365\n",
      "Iteration 135, loss = 0.79389655\n",
      "Iteration 136, loss = 0.79208090\n",
      "Iteration 137, loss = 0.79027669\n",
      "Iteration 138, loss = 0.78848394\n",
      "Iteration 139, loss = 0.78670263\n",
      "Iteration 140, loss = 0.78493277\n",
      "Iteration 141, loss = 0.78317436\n",
      "Iteration 142, loss = 0.78142737\n",
      "Iteration 143, loss = 0.77969180\n",
      "Iteration 144, loss = 0.77796763\n",
      "Iteration 145, loss = 0.77625485\n",
      "Iteration 146, loss = 0.77455343\n",
      "Iteration 147, loss = 0.77286336\n",
      "Iteration 148, loss = 0.77118460\n",
      "Iteration 149, loss = 0.76951713\n",
      "Iteration 150, loss = 0.76786092\n",
      "Iteration 151, loss = 0.76621594\n",
      "Iteration 152, loss = 0.76458216\n",
      "Iteration 153, loss = 0.76295954\n",
      "Iteration 154, loss = 0.76134804\n",
      "Iteration 155, loss = 0.75974763\n",
      "Iteration 156, loss = 0.75815826\n",
      "Iteration 157, loss = 0.75657990\n",
      "Iteration 158, loss = 0.75501250\n",
      "Iteration 159, loss = 0.75345602\n",
      "Iteration 160, loss = 0.75191040\n",
      "Iteration 161, loss = 0.75037561\n",
      "Iteration 162, loss = 0.74885160\n",
      "Iteration 163, loss = 0.74733831\n",
      "Iteration 164, loss = 0.74583570\n",
      "Iteration 165, loss = 0.74434370\n",
      "Iteration 166, loss = 0.74286228\n",
      "Iteration 167, loss = 0.74139138\n",
      "Iteration 168, loss = 0.73993093\n",
      "Iteration 169, loss = 0.73848089\n",
      "Iteration 170, loss = 0.73704121\n",
      "Iteration 171, loss = 0.73561181\n",
      "Iteration 172, loss = 0.73419265\n",
      "Iteration 173, loss = 0.73278367\n",
      "Iteration 174, loss = 0.73138480\n",
      "Iteration 175, loss = 0.72999599\n",
      "Iteration 176, loss = 0.72861718\n",
      "Iteration 177, loss = 0.72724831\n",
      "Iteration 178, loss = 0.72588931\n",
      "Iteration 179, loss = 0.72454013\n",
      "Iteration 180, loss = 0.72320070\n",
      "Iteration 181, loss = 0.72187096\n",
      "Iteration 182, loss = 0.72055085\n",
      "Iteration 183, loss = 0.71924030\n",
      "Iteration 184, loss = 0.71793925\n",
      "Iteration 185, loss = 0.71664765\n",
      "Iteration 186, loss = 0.71536542\n",
      "Iteration 187, loss = 0.71409249\n",
      "Iteration 188, loss = 0.71282882\n",
      "Iteration 189, loss = 0.71157433\n",
      "Iteration 190, loss = 0.71032896\n",
      "Iteration 191, loss = 0.70909264\n",
      "Iteration 192, loss = 0.70786532\n",
      "Iteration 193, loss = 0.70664692\n",
      "Iteration 194, loss = 0.70543738\n",
      "Iteration 195, loss = 0.70423664\n",
      "Iteration 196, loss = 0.70304464\n",
      "Iteration 197, loss = 0.70186130\n",
      "Iteration 198, loss = 0.70068658\n",
      "Iteration 199, loss = 0.69952039\n",
      "Iteration 200, loss = 0.69836268\n",
      "Iteration 201, loss = 0.69721338\n",
      "Iteration 202, loss = 0.69607244\n",
      "Iteration 203, loss = 0.69493978\n",
      "Iteration 204, loss = 0.69381535\n",
      "Iteration 205, loss = 0.69269908\n",
      "Iteration 206, loss = 0.69159090\n",
      "Iteration 207, loss = 0.69049076\n",
      "Iteration 208, loss = 0.68939859\n",
      "Iteration 209, loss = 0.68831434\n",
      "Iteration 210, loss = 0.68723793\n",
      "Iteration 211, loss = 0.68616930\n",
      "Iteration 212, loss = 0.68510841\n",
      "Iteration 213, loss = 0.68405517\n",
      "Iteration 214, loss = 0.68300954\n",
      "Iteration 215, loss = 0.68197145\n",
      "Iteration 216, loss = 0.68094084\n",
      "Iteration 217, loss = 0.67991766\n",
      "Iteration 218, loss = 0.67890183\n",
      "Iteration 219, loss = 0.67789331\n",
      "Iteration 220, loss = 0.67689203\n",
      "Iteration 221, loss = 0.67589793\n",
      "Iteration 222, loss = 0.67491096\n",
      "Iteration 223, loss = 0.67393106\n",
      "Iteration 224, loss = 0.67295817\n",
      "Iteration 225, loss = 0.67199223\n",
      "Iteration 226, loss = 0.67103318\n",
      "Iteration 227, loss = 0.67008097\n",
      "Iteration 228, loss = 0.66913555\n",
      "Iteration 229, loss = 0.66819685\n",
      "Iteration 230, loss = 0.66726482\n",
      "Iteration 231, loss = 0.66633940\n",
      "Iteration 232, loss = 0.66542054\n",
      "Iteration 233, loss = 0.66450819\n",
      "Iteration 234, loss = 0.66360229\n",
      "Iteration 235, loss = 0.66270278\n",
      "Iteration 236, loss = 0.66180962\n",
      "Iteration 237, loss = 0.66092275\n",
      "Iteration 238, loss = 0.66004212\n",
      "Iteration 239, loss = 0.65916767\n",
      "Iteration 240, loss = 0.65829936\n",
      "Iteration 241, loss = 0.65743712\n",
      "Iteration 242, loss = 0.65658092\n",
      "Iteration 243, loss = 0.65573070\n",
      "Iteration 244, loss = 0.65488641\n",
      "Iteration 245, loss = 0.65404799\n",
      "Iteration 246, loss = 0.65321541\n",
      "Iteration 247, loss = 0.65238860\n",
      "Iteration 248, loss = 0.65156753\n",
      "Iteration 249, loss = 0.65075214\n",
      "Iteration 250, loss = 0.64994238\n",
      "Iteration 251, loss = 0.64913822\n",
      "Iteration 252, loss = 0.64833959\n",
      "Iteration 253, loss = 0.64754645\n",
      "Iteration 254, loss = 0.64675877\n",
      "Iteration 255, loss = 0.64597648\n",
      "Iteration 256, loss = 0.64519954\n",
      "Iteration 257, loss = 0.64442792\n",
      "Iteration 258, loss = 0.64366156\n",
      "Iteration 259, loss = 0.64290042\n",
      "Iteration 260, loss = 0.64214445\n",
      "Iteration 261, loss = 0.64139361\n",
      "Iteration 262, loss = 0.64064787\n",
      "Iteration 263, loss = 0.63990716\n",
      "Iteration 264, loss = 0.63917146\n",
      "Iteration 265, loss = 0.63844072\n",
      "Iteration 266, loss = 0.63771489\n",
      "Iteration 267, loss = 0.63699394\n",
      "Iteration 268, loss = 0.63627782\n",
      "Iteration 269, loss = 0.63556650\n",
      "Iteration 270, loss = 0.63485992\n",
      "Iteration 271, loss = 0.63415806\n",
      "Iteration 272, loss = 0.63346087\n",
      "Iteration 273, loss = 0.63276832\n",
      "Iteration 274, loss = 0.63208036\n",
      "Iteration 275, loss = 0.63139695\n",
      "Iteration 276, loss = 0.63071805\n",
      "Iteration 277, loss = 0.63004364\n",
      "Iteration 278, loss = 0.62937366\n",
      "Iteration 279, loss = 0.62870809\n",
      "Iteration 280, loss = 0.62804688\n",
      "Iteration 281, loss = 0.62739001\n",
      "Iteration 282, loss = 0.62673742\n",
      "Iteration 283, loss = 0.62608909\n",
      "Iteration 284, loss = 0.62544497\n",
      "Iteration 285, loss = 0.62480505\n",
      "Iteration 286, loss = 0.62416927\n",
      "Iteration 287, loss = 0.62353760\n",
      "Iteration 288, loss = 0.62291002\n",
      "Iteration 289, loss = 0.62228648\n",
      "Iteration 290, loss = 0.62166695\n",
      "Iteration 291, loss = 0.62105140\n",
      "Iteration 292, loss = 0.62043979\n",
      "Iteration 293, loss = 0.61983210\n",
      "Iteration 294, loss = 0.61922828\n",
      "Iteration 295, loss = 0.61862831\n",
      "Iteration 296, loss = 0.61803216\n",
      "Iteration 297, loss = 0.61743979\n",
      "Iteration 298, loss = 0.61685116\n",
      "Iteration 299, loss = 0.61626626\n",
      "Iteration 300, loss = 0.61568505\n",
      "Iteration 301, loss = 0.61510749\n",
      "Iteration 302, loss = 0.61453357\n",
      "Iteration 303, loss = 0.61396324\n",
      "Iteration 304, loss = 0.61339647\n",
      "Iteration 305, loss = 0.61283325\n",
      "Iteration 306, loss = 0.61227353\n",
      "Iteration 307, loss = 0.61171730\n",
      "Iteration 308, loss = 0.61116452\n",
      "Iteration 309, loss = 0.61061515\n",
      "Iteration 310, loss = 0.61006919\n",
      "Iteration 311, loss = 0.60952659\n",
      "Iteration 312, loss = 0.60898733\n",
      "Iteration 313, loss = 0.60845138\n",
      "Iteration 314, loss = 0.60791872\n",
      "Iteration 315, loss = 0.60738931\n",
      "Iteration 316, loss = 0.60686314\n",
      "Iteration 317, loss = 0.60634016\n",
      "Iteration 318, loss = 0.60582037\n",
      "Iteration 319, loss = 0.60530373\n",
      "Iteration 320, loss = 0.60479022\n",
      "Iteration 321, loss = 0.60427981\n",
      "Iteration 322, loss = 0.60377247\n",
      "Iteration 323, loss = 0.60326819\n",
      "Iteration 324, loss = 0.60276693\n",
      "Iteration 325, loss = 0.60226868\n",
      "Iteration 326, loss = 0.60177340\n",
      "Iteration 327, loss = 0.60128107\n",
      "Iteration 328, loss = 0.60079168\n",
      "Iteration 329, loss = 0.60030519\n",
      "Iteration 330, loss = 0.59982159\n",
      "Iteration 331, loss = 0.59934084\n",
      "Iteration 332, loss = 0.59886293\n",
      "Iteration 333, loss = 0.59838784\n",
      "Iteration 334, loss = 0.59791554\n",
      "Iteration 335, loss = 0.59744601\n",
      "Iteration 336, loss = 0.59697922\n",
      "Iteration 337, loss = 0.59651516\n",
      "Iteration 338, loss = 0.59605380\n",
      "Iteration 339, loss = 0.59559513\n",
      "Iteration 340, loss = 0.59513911\n",
      "Iteration 341, loss = 0.59468574\n",
      "Iteration 342, loss = 0.59423498\n",
      "Iteration 343, loss = 0.59378683\n",
      "Iteration 344, loss = 0.59334125\n",
      "Iteration 345, loss = 0.59289822\n",
      "Iteration 346, loss = 0.59245774\n",
      "Iteration 347, loss = 0.59201977\n",
      "Iteration 348, loss = 0.59158430\n",
      "Iteration 349, loss = 0.59115131\n",
      "Iteration 350, loss = 0.59072077\n",
      "Iteration 351, loss = 0.59029268\n",
      "Iteration 352, loss = 0.58986700\n",
      "Iteration 353, loss = 0.58944373\n",
      "Iteration 354, loss = 0.58902284\n",
      "Iteration 355, loss = 0.58860431\n",
      "Iteration 356, loss = 0.58818813\n",
      "Iteration 357, loss = 0.58777427\n",
      "Iteration 358, loss = 0.58736273\n",
      "Iteration 359, loss = 0.58695348\n",
      "Iteration 360, loss = 0.58654650\n",
      "Iteration 361, loss = 0.58614178\n",
      "Iteration 362, loss = 0.58573929\n",
      "Iteration 363, loss = 0.58533903\n",
      "Iteration 364, loss = 0.58494097\n",
      "Iteration 365, loss = 0.58454510\n",
      "Iteration 366, loss = 0.58415141\n",
      "Iteration 367, loss = 0.58375986\n",
      "Iteration 368, loss = 0.58337046\n",
      "Iteration 369, loss = 0.58298317\n",
      "Iteration 370, loss = 0.58259800\n",
      "Iteration 371, loss = 0.58221491\n",
      "Iteration 372, loss = 0.58183389\n",
      "Iteration 373, loss = 0.58145494\n",
      "Iteration 374, loss = 0.58107802\n",
      "Iteration 375, loss = 0.58070314\n",
      "Iteration 376, loss = 0.58033026\n",
      "Iteration 377, loss = 0.57995938\n",
      "Iteration 378, loss = 0.57959049\n",
      "Iteration 379, loss = 0.57922356\n",
      "Iteration 380, loss = 0.57885858\n",
      "Iteration 381, loss = 0.57849554\n",
      "Iteration 382, loss = 0.57813442\n",
      "Iteration 383, loss = 0.57777521\n",
      "Iteration 384, loss = 0.57741789\n",
      "Iteration 385, loss = 0.57706246\n",
      "Iteration 386, loss = 0.57670889\n",
      "Iteration 387, loss = 0.57635717\n",
      "Iteration 388, loss = 0.57600729\n",
      "Iteration 389, loss = 0.57565924\n",
      "Iteration 390, loss = 0.57531300\n",
      "Iteration 391, loss = 0.57496856\n",
      "Iteration 392, loss = 0.57462590\n",
      "Iteration 393, loss = 0.57428501\n",
      "Iteration 394, loss = 0.57394588\n",
      "Iteration 395, loss = 0.57360850\n",
      "Iteration 396, loss = 0.57327285\n",
      "Iteration 397, loss = 0.57293892\n",
      "Iteration 398, loss = 0.57260670\n",
      "Iteration 399, loss = 0.57227617\n",
      "Iteration 400, loss = 0.57194733\n",
      "Iteration 401, loss = 0.57162016\n",
      "Iteration 402, loss = 0.57129465\n",
      "Iteration 403, loss = 0.57097078\n",
      "Iteration 404, loss = 0.57064855\n",
      "Iteration 405, loss = 0.57032794\n",
      "Iteration 406, loss = 0.57000894\n",
      "Iteration 407, loss = 0.56969154\n",
      "Iteration 408, loss = 0.56937573\n",
      "Iteration 409, loss = 0.56906150\n",
      "Iteration 410, loss = 0.56874883\n",
      "Iteration 411, loss = 0.56843771\n",
      "Iteration 412, loss = 0.56812814\n",
      "Iteration 413, loss = 0.56782010\n",
      "Iteration 414, loss = 0.56751358\n",
      "Iteration 415, loss = 0.56720858\n",
      "Iteration 416, loss = 0.56690507\n",
      "Iteration 417, loss = 0.56660305\n",
      "Iteration 418, loss = 0.56630251\n",
      "Iteration 419, loss = 0.56600343\n",
      "Iteration 420, loss = 0.56570581\n",
      "Iteration 421, loss = 0.56540965\n",
      "Iteration 422, loss = 0.56511491\n",
      "Iteration 423, loss = 0.56482161\n",
      "Iteration 424, loss = 0.56452972\n",
      "Iteration 425, loss = 0.56423923\n",
      "Iteration 426, loss = 0.56395015\n",
      "Iteration 427, loss = 0.56366245\n",
      "Iteration 428, loss = 0.56337613\n",
      "Iteration 429, loss = 0.56309118\n",
      "Iteration 430, loss = 0.56280758\n",
      "Iteration 431, loss = 0.56252534\n",
      "Iteration 432, loss = 0.56224443\n",
      "Iteration 433, loss = 0.56196485\n",
      "Iteration 434, loss = 0.56168660\n",
      "Iteration 435, loss = 0.56140965\n",
      "Iteration 436, loss = 0.56113401\n",
      "Iteration 437, loss = 0.56085967\n",
      "Iteration 438, loss = 0.56058660\n",
      "Iteration 439, loss = 0.56031482\n",
      "Iteration 440, loss = 0.56004430\n",
      "Iteration 441, loss = 0.55977504\n",
      "Iteration 442, loss = 0.55950703\n",
      "Iteration 443, loss = 0.55924026\n",
      "Iteration 444, loss = 0.55897472\n",
      "Iteration 445, loss = 0.55871041\n",
      "Iteration 446, loss = 0.55844731\n",
      "Iteration 447, loss = 0.55818542\n",
      "Iteration 448, loss = 0.55792473\n",
      "Iteration 449, loss = 0.55766524\n",
      "Iteration 450, loss = 0.55740692\n",
      "Iteration 451, loss = 0.55714978\n",
      "Iteration 452, loss = 0.55689381\n",
      "Iteration 453, loss = 0.55663900\n",
      "Iteration 454, loss = 0.55638534\n",
      "Iteration 455, loss = 0.55613283\n",
      "Iteration 456, loss = 0.55588145\n",
      "Iteration 457, loss = 0.55563119\n",
      "Iteration 458, loss = 0.55538206\n",
      "Iteration 459, loss = 0.55513405\n",
      "Iteration 460, loss = 0.55488714\n",
      "Iteration 461, loss = 0.55464132\n",
      "Iteration 462, loss = 0.55439660\n",
      "Iteration 463, loss = 0.55415297\n",
      "Iteration 464, loss = 0.55391041\n",
      "Iteration 465, loss = 0.55366892\n",
      "Iteration 466, loss = 0.55342849\n",
      "Iteration 467, loss = 0.55318912\n",
      "Iteration 468, loss = 0.55295080\n",
      "Iteration 469, loss = 0.55271351\n",
      "Iteration 470, loss = 0.55247727\n",
      "Iteration 471, loss = 0.55224205\n",
      "Iteration 472, loss = 0.55200785\n",
      "Iteration 473, loss = 0.55177467\n",
      "Iteration 474, loss = 0.55154249\n",
      "Iteration 475, loss = 0.55131132\n",
      "Iteration 476, loss = 0.55108114\n",
      "Iteration 477, loss = 0.55085195\n",
      "Iteration 478, loss = 0.55062373\n",
      "Iteration 479, loss = 0.55039650\n",
      "Iteration 480, loss = 0.55017023\n",
      "Iteration 481, loss = 0.54994493\n",
      "Iteration 482, loss = 0.54972058\n",
      "Iteration 483, loss = 0.54949718\n",
      "Iteration 484, loss = 0.54927473\n",
      "Iteration 485, loss = 0.54905322\n",
      "Iteration 486, loss = 0.54883263\n",
      "Iteration 487, loss = 0.54861297\n",
      "Iteration 488, loss = 0.54839423\n",
      "Iteration 489, loss = 0.54817641\n",
      "Iteration 490, loss = 0.54795949\n",
      "Iteration 491, loss = 0.54774348\n",
      "Iteration 492, loss = 0.54752836\n",
      "Iteration 493, loss = 0.54731413\n",
      "Iteration 494, loss = 0.54710078\n",
      "Iteration 495, loss = 0.54688832\n",
      "Iteration 496, loss = 0.54667673\n",
      "Iteration 497, loss = 0.54646600\n",
      "Iteration 498, loss = 0.54625614\n",
      "Iteration 499, loss = 0.54604713\n",
      "Iteration 500, loss = 0.54583898\n",
      "Iteration 501, loss = 0.54563167\n",
      "Iteration 502, loss = 0.54542520\n",
      "Iteration 503, loss = 0.54521957\n",
      "Iteration 504, loss = 0.54501477\n",
      "Iteration 505, loss = 0.54481079\n",
      "Iteration 506, loss = 0.54460763\n",
      "Iteration 507, loss = 0.54440529\n",
      "Iteration 508, loss = 0.54420375\n",
      "Iteration 509, loss = 0.54400302\n",
      "Iteration 510, loss = 0.54380309\n",
      "Iteration 511, loss = 0.54360395\n",
      "Iteration 512, loss = 0.54340560\n",
      "Iteration 513, loss = 0.54320803\n",
      "Iteration 514, loss = 0.54301124\n",
      "Iteration 515, loss = 0.54281523\n",
      "Iteration 516, loss = 0.54261998\n",
      "Iteration 517, loss = 0.54242550\n",
      "Iteration 518, loss = 0.54223178\n",
      "Iteration 519, loss = 0.54203881\n",
      "Iteration 520, loss = 0.54184659\n",
      "Iteration 521, loss = 0.54165512\n",
      "Iteration 522, loss = 0.54146439\n",
      "Iteration 523, loss = 0.54127440\n",
      "Iteration 524, loss = 0.54108513\n",
      "Iteration 525, loss = 0.54089660\n",
      "Iteration 526, loss = 0.54070878\n",
      "Iteration 527, loss = 0.54052168\n",
      "Iteration 528, loss = 0.54033530\n",
      "Iteration 529, loss = 0.54014963\n",
      "Iteration 530, loss = 0.53996466\n",
      "Iteration 531, loss = 0.53978039\n",
      "Iteration 532, loss = 0.53959681\n",
      "Iteration 533, loss = 0.53941393\n",
      "Iteration 534, loss = 0.53923174\n",
      "Iteration 535, loss = 0.53905022\n",
      "Iteration 536, loss = 0.53886939\n",
      "Iteration 537, loss = 0.53868923\n",
      "Iteration 538, loss = 0.53850974\n",
      "Iteration 539, loss = 0.53833092\n",
      "Iteration 540, loss = 0.53815276\n",
      "Iteration 541, loss = 0.53797526\n",
      "Iteration 542, loss = 0.53779841\n",
      "Iteration 543, loss = 0.53762221\n",
      "Iteration 544, loss = 0.53744666\n",
      "Iteration 545, loss = 0.53727175\n",
      "Iteration 546, loss = 0.53709748\n",
      "Iteration 547, loss = 0.53692384\n",
      "Iteration 548, loss = 0.53675083\n",
      "Iteration 549, loss = 0.53657845\n",
      "Iteration 550, loss = 0.53640669\n",
      "Iteration 551, loss = 0.53623555\n",
      "Iteration 552, loss = 0.53606503\n",
      "Iteration 553, loss = 0.53589512\n",
      "Iteration 554, loss = 0.53572581\n",
      "Iteration 555, loss = 0.53555711\n",
      "Iteration 556, loss = 0.53538901\n",
      "Iteration 557, loss = 0.53522150\n",
      "Iteration 558, loss = 0.53505459\n",
      "Iteration 559, loss = 0.53488826\n",
      "Iteration 560, loss = 0.53472253\n",
      "Iteration 561, loss = 0.53455737\n",
      "Iteration 562, loss = 0.53439279\n",
      "Iteration 563, loss = 0.53422879\n",
      "Iteration 564, loss = 0.53406536\n",
      "Iteration 565, loss = 0.53390250\n",
      "Iteration 566, loss = 0.53374020\n",
      "Iteration 567, loss = 0.53357846\n",
      "Iteration 568, loss = 0.53341728\n",
      "Iteration 569, loss = 0.53325665\n",
      "Iteration 570, loss = 0.53309658\n",
      "Iteration 571, loss = 0.53293705\n",
      "Iteration 572, loss = 0.53277807\n",
      "Iteration 573, loss = 0.53261962\n",
      "Iteration 574, loss = 0.53246172\n",
      "Iteration 575, loss = 0.53230435\n",
      "Iteration 576, loss = 0.53214751\n",
      "Iteration 577, loss = 0.53199120\n",
      "Iteration 578, loss = 0.53183541\n",
      "Iteration 579, loss = 0.53168014\n",
      "Iteration 580, loss = 0.53152540\n",
      "Iteration 581, loss = 0.53137116\n",
      "Iteration 582, loss = 0.53121744\n",
      "Iteration 583, loss = 0.53106423\n",
      "Iteration 584, loss = 0.53091153\n",
      "Iteration 585, loss = 0.53075932\n",
      "Iteration 586, loss = 0.53060762\n",
      "Iteration 587, loss = 0.53045641\n",
      "Iteration 588, loss = 0.53030570\n",
      "Iteration 589, loss = 0.53015548\n",
      "Iteration 590, loss = 0.53000574\n",
      "Iteration 591, loss = 0.52985649\n",
      "Iteration 592, loss = 0.52970772\n",
      "Iteration 593, loss = 0.52955944\n",
      "Iteration 594, loss = 0.52941162\n",
      "Iteration 595, loss = 0.52926428\n",
      "Iteration 596, loss = 0.52911741\n",
      "Iteration 597, loss = 0.52897101\n",
      "Iteration 598, loss = 0.52882507\n",
      "Iteration 599, loss = 0.52867959\n",
      "Iteration 600, loss = 0.52853457\n",
      "Iteration 601, loss = 0.52839001\n",
      "Iteration 602, loss = 0.52824590\n",
      "Iteration 603, loss = 0.52810224\n",
      "Iteration 604, loss = 0.52795903\n",
      "Iteration 605, loss = 0.52781626\n",
      "Iteration 606, loss = 0.52767394\n",
      "Iteration 607, loss = 0.52753206\n",
      "Iteration 608, loss = 0.52739061\n",
      "Iteration 609, loss = 0.52724960\n",
      "Iteration 610, loss = 0.52710901\n",
      "Iteration 611, loss = 0.52696886\n",
      "Iteration 612, loss = 0.52682913\n",
      "Iteration 613, loss = 0.52668983\n",
      "Iteration 614, loss = 0.52655095\n",
      "Iteration 615, loss = 0.52641248\n",
      "Iteration 616, loss = 0.52627444\n",
      "Iteration 617, loss = 0.52613680\n",
      "Iteration 618, loss = 0.52599958\n",
      "Iteration 619, loss = 0.52586276\n",
      "Iteration 620, loss = 0.52572635\n",
      "Iteration 621, loss = 0.52559034\n",
      "Iteration 622, loss = 0.52545474\n",
      "Iteration 623, loss = 0.52531953\n",
      "Iteration 624, loss = 0.52518472\n",
      "Iteration 625, loss = 0.52505030\n",
      "Iteration 626, loss = 0.52491627\n",
      "Iteration 627, loss = 0.52478263\n",
      "Iteration 628, loss = 0.52464938\n",
      "Iteration 629, loss = 0.52451651\n",
      "Iteration 630, loss = 0.52438402\n",
      "Iteration 631, loss = 0.52425191\n",
      "Iteration 632, loss = 0.52412018\n",
      "Iteration 633, loss = 0.52398882\n",
      "Iteration 634, loss = 0.52385783\n",
      "Iteration 635, loss = 0.52372721\n",
      "Iteration 636, loss = 0.52359696\n",
      "Iteration 637, loss = 0.52346708\n",
      "Iteration 638, loss = 0.52333756\n",
      "Iteration 639, loss = 0.52320839\n",
      "Iteration 640, loss = 0.52307959\n",
      "Iteration 641, loss = 0.52295114\n",
      "Iteration 642, loss = 0.52282304\n",
      "Iteration 643, loss = 0.52269530\n",
      "Iteration 644, loss = 0.52256790\n",
      "Iteration 645, loss = 0.52244086\n",
      "Iteration 646, loss = 0.52231415\n",
      "Iteration 647, loss = 0.52218779\n",
      "Iteration 648, loss = 0.52206177\n",
      "Iteration 649, loss = 0.52193609\n",
      "Iteration 650, loss = 0.52181074\n",
      "Iteration 651, loss = 0.52168572\n",
      "Iteration 652, loss = 0.52156104\n",
      "Iteration 653, loss = 0.52143669\n",
      "Iteration 654, loss = 0.52131266\n",
      "Iteration 655, loss = 0.52118896\n",
      "Iteration 656, loss = 0.52106559\n",
      "Iteration 657, loss = 0.52094253\n",
      "Iteration 658, loss = 0.52081979\n",
      "Iteration 659, loss = 0.52069737\n",
      "Iteration 660, loss = 0.52057527\n",
      "Iteration 661, loss = 0.52045348\n",
      "Iteration 662, loss = 0.52033199\n",
      "Iteration 663, loss = 0.52021082\n",
      "Iteration 664, loss = 0.52008995\n",
      "Iteration 665, loss = 0.51996939\n",
      "Iteration 666, loss = 0.51984913\n",
      "Iteration 667, loss = 0.51972917\n",
      "Iteration 668, loss = 0.51960951\n",
      "Iteration 669, loss = 0.51949014\n",
      "Iteration 670, loss = 0.51937107\n",
      "Iteration 671, loss = 0.51925230\n",
      "Iteration 672, loss = 0.51913381\n",
      "Iteration 673, loss = 0.51901561\n",
      "Iteration 674, loss = 0.51889770\n",
      "Iteration 675, loss = 0.51878007\n",
      "Iteration 676, loss = 0.51866273\n",
      "Iteration 677, loss = 0.51854566\n",
      "Iteration 678, loss = 0.51842888\n",
      "Iteration 679, loss = 0.51831237\n",
      "Iteration 680, loss = 0.51819614\n",
      "Iteration 681, loss = 0.51808018\n",
      "Iteration 682, loss = 0.51796449\n",
      "Iteration 683, loss = 0.51784908\n",
      "Iteration 684, loss = 0.51773393\n",
      "Iteration 685, loss = 0.51761904\n",
      "Iteration 686, loss = 0.51750442\n",
      "Iteration 687, loss = 0.51739006\n",
      "Iteration 688, loss = 0.51727596\n",
      "Iteration 689, loss = 0.51716212\n",
      "Iteration 690, loss = 0.51704854\n",
      "Iteration 691, loss = 0.51693521\n",
      "Iteration 692, loss = 0.51682213\n",
      "Iteration 693, loss = 0.51670931\n",
      "Iteration 694, loss = 0.51659673\n",
      "Iteration 695, loss = 0.51648440\n",
      "Iteration 696, loss = 0.51637232\n",
      "Iteration 697, loss = 0.51626048\n",
      "Iteration 698, loss = 0.51614888\n",
      "Iteration 699, loss = 0.51603752\n",
      "Iteration 700, loss = 0.51592641\n",
      "Iteration 701, loss = 0.51581552\n",
      "Iteration 702, loss = 0.51570488\n",
      "Iteration 703, loss = 0.51559446\n",
      "Iteration 704, loss = 0.51548428\n",
      "Iteration 705, loss = 0.51537433\n",
      "Iteration 706, loss = 0.51526460\n",
      "Iteration 707, loss = 0.51515511\n",
      "Iteration 708, loss = 0.51504583\n",
      "Iteration 709, loss = 0.51493678\n",
      "Iteration 710, loss = 0.51482795\n",
      "Iteration 711, loss = 0.51471934\n",
      "Iteration 712, loss = 0.51461095\n",
      "Iteration 713, loss = 0.51450277\n",
      "Iteration 714, loss = 0.51439481\n",
      "Iteration 715, loss = 0.51428706\n",
      "Iteration 716, loss = 0.51417952\n",
      "Iteration 717, loss = 0.51407219\n",
      "Iteration 718, loss = 0.51396507\n",
      "Iteration 719, loss = 0.51385816\n",
      "Iteration 720, loss = 0.51375145\n",
      "Iteration 721, loss = 0.51364494\n",
      "Iteration 722, loss = 0.51353863\n",
      "Iteration 723, loss = 0.51343252\n",
      "Iteration 724, loss = 0.51332661\n",
      "Iteration 725, loss = 0.51322090\n",
      "Iteration 726, loss = 0.51311538\n",
      "Iteration 727, loss = 0.51301005\n",
      "Iteration 728, loss = 0.51290491\n",
      "Iteration 729, loss = 0.51279997\n",
      "Iteration 730, loss = 0.51269521\n",
      "Iteration 731, loss = 0.51259063\n",
      "Iteration 732, loss = 0.51248625\n",
      "Iteration 733, loss = 0.51238204\n",
      "Iteration 734, loss = 0.51227802\n",
      "Iteration 735, loss = 0.51217418\n",
      "Iteration 736, loss = 0.51207051\n",
      "Iteration 737, loss = 0.51196702\n",
      "Iteration 738, loss = 0.51186371\n",
      "Iteration 739, loss = 0.51176057\n",
      "Iteration 740, loss = 0.51165760\n",
      "Iteration 741, loss = 0.51155481\n",
      "Iteration 742, loss = 0.51145218\n",
      "Iteration 743, loss = 0.51134972\n",
      "Iteration 744, loss = 0.51124743\n",
      "Iteration 745, loss = 0.51114529\n",
      "Iteration 746, loss = 0.51104333\n",
      "Iteration 747, loss = 0.51094152\n",
      "Iteration 748, loss = 0.51083987\n",
      "Iteration 749, loss = 0.51073838\n",
      "Iteration 750, loss = 0.51063705\n",
      "Iteration 751, loss = 0.51053587\n",
      "Iteration 752, loss = 0.51043485\n",
      "Iteration 753, loss = 0.51033398\n",
      "Iteration 754, loss = 0.51023326\n",
      "Iteration 755, loss = 0.51013268\n",
      "Iteration 756, loss = 0.51003226\n",
      "Iteration 757, loss = 0.50993198\n",
      "Iteration 758, loss = 0.50983184\n",
      "Iteration 759, loss = 0.50973185\n",
      "Iteration 760, loss = 0.50963200\n",
      "Iteration 761, loss = 0.50953228\n",
      "Iteration 762, loss = 0.50943271\n",
      "Iteration 763, loss = 0.50933327\n",
      "Iteration 764, loss = 0.50923397\n",
      "Iteration 765, loss = 0.50913481\n",
      "Iteration 766, loss = 0.50903577\n",
      "Iteration 767, loss = 0.50893687\n",
      "Iteration 768, loss = 0.50883809\n",
      "Iteration 769, loss = 0.50873944\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.15770975\n",
      "Iteration 2, loss = 1.15693595\n",
      "Iteration 3, loss = 1.15584618\n",
      "Iteration 4, loss = 1.15448596\n",
      "Iteration 5, loss = 1.15289865\n",
      "Iteration 6, loss = 1.15112492\n",
      "Iteration 7, loss = 1.14920251\n",
      "Iteration 8, loss = 1.14716596\n",
      "Iteration 9, loss = 1.14504661\n",
      "Iteration 10, loss = 1.14287258\n",
      "Iteration 11, loss = 1.14066881\n",
      "Iteration 12, loss = 1.13845722\n",
      "Iteration 13, loss = 1.13625682\n",
      "Iteration 14, loss = 1.13408392\n",
      "Iteration 15, loss = 1.13195229\n",
      "Iteration 16, loss = 1.12987339\n",
      "Iteration 17, loss = 1.12785655\n",
      "Iteration 18, loss = 1.12590920\n",
      "Iteration 19, loss = 1.12403704\n",
      "Iteration 20, loss = 1.12224424\n",
      "Iteration 21, loss = 1.12053365\n",
      "Iteration 22, loss = 1.11890692\n",
      "Iteration 23, loss = 1.11736470\n",
      "Iteration 24, loss = 1.11590676\n",
      "Iteration 25, loss = 1.11453216\n",
      "Iteration 26, loss = 1.11323932\n",
      "Iteration 27, loss = 1.11202618\n",
      "Iteration 28, loss = 1.11089027\n",
      "Iteration 29, loss = 1.10982879\n",
      "Iteration 30, loss = 1.10883872\n",
      "Iteration 31, loss = 1.10791686\n",
      "Iteration 32, loss = 1.10705988\n",
      "Iteration 33, loss = 1.10626442\n",
      "Iteration 34, loss = 1.10552705\n",
      "Iteration 35, loss = 1.10484440\n",
      "Iteration 36, loss = 1.10421310\n",
      "Iteration 37, loss = 1.10362989\n",
      "Iteration 38, loss = 1.10309155\n",
      "Iteration 39, loss = 1.10259501\n",
      "Iteration 40, loss = 1.10213727\n",
      "Iteration 41, loss = 1.10171548\n",
      "Iteration 42, loss = 1.10132692\n",
      "Iteration 43, loss = 1.10096898\n",
      "Iteration 44, loss = 1.10063920\n",
      "Iteration 45, loss = 1.10033525\n",
      "Iteration 46, loss = 1.10005493\n",
      "Iteration 47, loss = 1.09979617\n",
      "Iteration 48, loss = 1.09955701\n",
      "Iteration 49, loss = 1.09933565\n",
      "Iteration 50, loss = 1.09913037\n",
      "Iteration 51, loss = 1.09893956\n",
      "Iteration 52, loss = 1.09876175\n",
      "Iteration 53, loss = 1.09859553\n",
      "Iteration 54, loss = 1.09843960\n",
      "Iteration 55, loss = 1.09829276\n",
      "Iteration 56, loss = 1.09815386\n",
      "Iteration 57, loss = 1.09802185\n",
      "Iteration 58, loss = 1.09789574\n",
      "Iteration 59, loss = 1.09777461\n",
      "Iteration 60, loss = 1.09765759\n",
      "Iteration 61, loss = 1.09754386\n",
      "Iteration 62, loss = 1.09743266\n",
      "Iteration 63, loss = 1.09732324\n",
      "Iteration 64, loss = 1.09721493\n",
      "Iteration 65, loss = 1.09710707\n",
      "Iteration 66, loss = 1.09699900\n",
      "Iteration 67, loss = 1.09689013\n",
      "Iteration 68, loss = 1.09677984\n",
      "Iteration 69, loss = 1.09666757\n",
      "Iteration 70, loss = 1.09655271\n",
      "Iteration 71, loss = 1.09643471\n",
      "Iteration 72, loss = 1.09631297\n",
      "Iteration 73, loss = 1.09618691\n",
      "Iteration 74, loss = 1.09605594\n",
      "Iteration 75, loss = 1.09591944\n",
      "Iteration 76, loss = 1.09577676\n",
      "Iteration 77, loss = 1.09562726\n",
      "Iteration 78, loss = 1.09547023\n",
      "Iteration 79, loss = 1.09530495\n",
      "Iteration 80, loss = 1.09513063\n",
      "Iteration 81, loss = 1.09494647\n",
      "Iteration 82, loss = 1.09475157\n",
      "Iteration 83, loss = 1.09454502\n",
      "Iteration 84, loss = 1.09432579\n",
      "Iteration 85, loss = 1.09409281\n",
      "Iteration 86, loss = 1.09384494\n",
      "Iteration 87, loss = 1.09358091\n",
      "Iteration 88, loss = 1.09329939\n",
      "Iteration 89, loss = 1.09299895\n",
      "Iteration 90, loss = 1.09267805\n",
      "Iteration 91, loss = 1.09233502\n",
      "Iteration 92, loss = 1.09196812\n",
      "Iteration 93, loss = 1.09157546\n",
      "Iteration 94, loss = 1.09115505\n",
      "Iteration 95, loss = 1.09070481\n",
      "Iteration 96, loss = 1.09022252\n",
      "Iteration 97, loss = 1.08970592\n",
      "Iteration 98, loss = 1.08915267\n",
      "Iteration 99, loss = 1.08856039\n",
      "Iteration 100, loss = 1.08792671\n",
      "Iteration 101, loss = 1.08724932\n",
      "Iteration 102, loss = 1.08652600\n",
      "Iteration 103, loss = 1.08575473\n",
      "Iteration 104, loss = 1.08493375\n",
      "Iteration 105, loss = 1.08406165\n",
      "Iteration 106, loss = 1.08313748\n",
      "Iteration 107, loss = 1.08216082\n",
      "Iteration 108, loss = 1.08113191\n",
      "Iteration 109, loss = 1.08005173\n",
      "Iteration 110, loss = 1.07892202\n",
      "Iteration 111, loss = 1.07774536\n",
      "Iteration 112, loss = 1.07652515\n",
      "Iteration 113, loss = 1.07526552\n",
      "Iteration 114, loss = 1.07397127\n",
      "Iteration 115, loss = 1.07264769\n",
      "Iteration 116, loss = 1.07130038\n",
      "Iteration 117, loss = 1.06993499\n",
      "Iteration 118, loss = 1.06855704\n",
      "Iteration 119, loss = 1.06717160\n",
      "Iteration 120, loss = 1.06578316\n",
      "Iteration 121, loss = 1.06439540\n",
      "Iteration 122, loss = 1.06301109\n",
      "Iteration 123, loss = 1.06163201\n",
      "Iteration 124, loss = 1.06025894\n",
      "Iteration 125, loss = 1.05889172\n",
      "Iteration 126, loss = 1.05752932\n",
      "Iteration 127, loss = 1.05616997\n",
      "Iteration 128, loss = 1.05481133\n",
      "Iteration 129, loss = 1.05345061\n",
      "Iteration 130, loss = 1.05208474\n",
      "Iteration 131, loss = 1.05071054\n",
      "Iteration 132, loss = 1.04932485\n",
      "Iteration 133, loss = 1.04792464\n",
      "Iteration 134, loss = 1.04650713\n",
      "Iteration 135, loss = 1.04506983\n",
      "Iteration 136, loss = 1.04361066\n",
      "Iteration 137, loss = 1.04212792\n",
      "Iteration 138, loss = 1.04062031\n",
      "Iteration 139, loss = 1.03908698\n",
      "Iteration 140, loss = 1.03752745\n",
      "Iteration 141, loss = 1.03594160\n",
      "Iteration 142, loss = 1.03432962\n",
      "Iteration 143, loss = 1.03269197\n",
      "Iteration 144, loss = 1.03102933\n",
      "Iteration 145, loss = 1.02934250\n",
      "Iteration 146, loss = 1.02763238\n",
      "Iteration 147, loss = 1.02589994\n",
      "Iteration 148, loss = 1.02414610\n",
      "Iteration 149, loss = 1.02237177\n",
      "Iteration 150, loss = 1.02057775\n",
      "Iteration 151, loss = 1.01876475\n",
      "Iteration 152, loss = 1.01693338\n",
      "Iteration 153, loss = 1.01508412\n",
      "Iteration 154, loss = 1.01321732\n",
      "Iteration 155, loss = 1.01133327\n",
      "Iteration 156, loss = 1.00943213\n",
      "Iteration 157, loss = 1.00751401\n",
      "Iteration 158, loss = 1.00557900\n",
      "Iteration 159, loss = 1.00362715\n",
      "Iteration 160, loss = 1.00165850\n",
      "Iteration 161, loss = 0.99967313\n",
      "Iteration 162, loss = 0.99767117\n",
      "Iteration 163, loss = 0.99565276\n",
      "Iteration 164, loss = 0.99361813\n",
      "Iteration 165, loss = 0.99156755\n",
      "Iteration 166, loss = 0.98950136\n",
      "Iteration 167, loss = 0.98741995\n",
      "Iteration 168, loss = 0.98532374\n",
      "Iteration 169, loss = 0.98321323\n",
      "Iteration 170, loss = 0.98108891\n",
      "Iteration 171, loss = 0.97895131\n",
      "Iteration 172, loss = 0.97680098\n",
      "Iteration 173, loss = 0.97463847\n",
      "Iteration 174, loss = 0.97246433\n",
      "Iteration 175, loss = 0.97027909\n",
      "Iteration 176, loss = 0.96808331\n",
      "Iteration 177, loss = 0.96587750\n",
      "Iteration 178, loss = 0.96366219\n",
      "Iteration 179, loss = 0.96143790\n",
      "Iteration 180, loss = 0.95920512\n",
      "Iteration 181, loss = 0.95696436\n",
      "Iteration 182, loss = 0.95471611\n",
      "Iteration 183, loss = 0.95246087\n",
      "Iteration 184, loss = 0.95019912\n",
      "Iteration 185, loss = 0.94793137\n",
      "Iteration 186, loss = 0.94565811\n",
      "Iteration 187, loss = 0.94337983\n",
      "Iteration 188, loss = 0.94109703\n",
      "Iteration 189, loss = 0.93881020\n",
      "Iteration 190, loss = 0.93651983\n",
      "Iteration 191, loss = 0.93422641\n",
      "Iteration 192, loss = 0.93193043\n",
      "Iteration 193, loss = 0.92963237\n",
      "Iteration 194, loss = 0.92733271\n",
      "Iteration 195, loss = 0.92503191\n",
      "Iteration 196, loss = 0.92273043\n",
      "Iteration 197, loss = 0.92042873\n",
      "Iteration 198, loss = 0.91812725\n",
      "Iteration 199, loss = 0.91582642\n",
      "Iteration 200, loss = 0.91352666\n",
      "Iteration 201, loss = 0.91122840\n",
      "Iteration 202, loss = 0.90893204\n",
      "Iteration 203, loss = 0.90663798\n",
      "Iteration 204, loss = 0.90434659\n",
      "Iteration 205, loss = 0.90205827\n",
      "Iteration 206, loss = 0.89977338\n",
      "Iteration 207, loss = 0.89749227\n",
      "Iteration 208, loss = 0.89521531\n",
      "Iteration 209, loss = 0.89294282\n",
      "Iteration 210, loss = 0.89067515\n",
      "Iteration 211, loss = 0.88841261\n",
      "Iteration 212, loss = 0.88615552\n",
      "Iteration 213, loss = 0.88390418\n",
      "Iteration 214, loss = 0.88165889\n",
      "Iteration 215, loss = 0.87941993\n",
      "Iteration 216, loss = 0.87718757\n",
      "Iteration 217, loss = 0.87496209\n",
      "Iteration 218, loss = 0.87274374\n",
      "Iteration 219, loss = 0.87053276\n",
      "Iteration 220, loss = 0.86832940\n",
      "Iteration 221, loss = 0.86613389\n",
      "Iteration 222, loss = 0.86394645\n",
      "Iteration 223, loss = 0.86176728\n",
      "Iteration 224, loss = 0.85959660\n",
      "Iteration 225, loss = 0.85743460\n",
      "Iteration 226, loss = 0.85528146\n",
      "Iteration 227, loss = 0.85313737\n",
      "Iteration 228, loss = 0.85100249\n",
      "Iteration 229, loss = 0.84887699\n",
      "Iteration 230, loss = 0.84676102\n",
      "Iteration 231, loss = 0.84465474\n",
      "Iteration 232, loss = 0.84255828\n",
      "Iteration 233, loss = 0.84047177\n",
      "Iteration 234, loss = 0.83839535\n",
      "Iteration 235, loss = 0.83632913\n",
      "Iteration 236, loss = 0.83427323\n",
      "Iteration 237, loss = 0.83222774\n",
      "Iteration 238, loss = 0.83019279\n",
      "Iteration 239, loss = 0.82816845\n",
      "Iteration 240, loss = 0.82615481\n",
      "Iteration 241, loss = 0.82415196\n",
      "Iteration 242, loss = 0.82215998\n",
      "Iteration 243, loss = 0.82017893\n",
      "Iteration 244, loss = 0.81820888\n",
      "Iteration 245, loss = 0.81624989\n",
      "Iteration 246, loss = 0.81430202\n",
      "Iteration 247, loss = 0.81236531\n",
      "Iteration 248, loss = 0.81043982\n",
      "Iteration 249, loss = 0.80852558\n",
      "Iteration 250, loss = 0.80662263\n",
      "Iteration 251, loss = 0.80473100\n",
      "Iteration 252, loss = 0.80285071\n",
      "Iteration 253, loss = 0.80098181\n",
      "Iteration 254, loss = 0.79912429\n",
      "Iteration 255, loss = 0.79727818\n",
      "Iteration 256, loss = 0.79544350\n",
      "Iteration 257, loss = 0.79362024\n",
      "Iteration 258, loss = 0.79180842\n",
      "Iteration 259, loss = 0.79000804\n",
      "Iteration 260, loss = 0.78821909\n",
      "Iteration 261, loss = 0.78644157\n",
      "Iteration 262, loss = 0.78467548\n",
      "Iteration 263, loss = 0.78292080\n",
      "Iteration 264, loss = 0.78117752\n",
      "Iteration 265, loss = 0.77944563\n",
      "Iteration 266, loss = 0.77772510\n",
      "Iteration 267, loss = 0.77601593\n",
      "Iteration 268, loss = 0.77431807\n",
      "Iteration 269, loss = 0.77263152\n",
      "Iteration 270, loss = 0.77095623\n",
      "Iteration 271, loss = 0.76929219\n",
      "Iteration 272, loss = 0.76763935\n",
      "Iteration 273, loss = 0.76599769\n",
      "Iteration 274, loss = 0.76436718\n",
      "Iteration 275, loss = 0.76274777\n",
      "Iteration 276, loss = 0.76113942\n",
      "Iteration 277, loss = 0.75954210\n",
      "Iteration 278, loss = 0.75795576\n",
      "Iteration 279, loss = 0.75638036\n",
      "Iteration 280, loss = 0.75481585\n",
      "Iteration 281, loss = 0.75326219\n",
      "Iteration 282, loss = 0.75171934\n",
      "Iteration 283, loss = 0.75018724\n",
      "Iteration 284, loss = 0.74866584\n",
      "Iteration 285, loss = 0.74715509\n",
      "Iteration 286, loss = 0.74565494\n",
      "Iteration 287, loss = 0.74416534\n",
      "Iteration 288, loss = 0.74268623\n",
      "Iteration 289, loss = 0.74121757\n",
      "Iteration 290, loss = 0.73975928\n",
      "Iteration 291, loss = 0.73831133\n",
      "Iteration 292, loss = 0.73687364\n",
      "Iteration 293, loss = 0.73544617\n",
      "Iteration 294, loss = 0.73402885\n",
      "Iteration 295, loss = 0.73262163\n",
      "Iteration 296, loss = 0.73122444\n",
      "Iteration 297, loss = 0.72983723\n",
      "Iteration 298, loss = 0.72845994\n",
      "Iteration 299, loss = 0.72709250\n",
      "Iteration 300, loss = 0.72573485\n",
      "Iteration 301, loss = 0.72438694\n",
      "Iteration 302, loss = 0.72304870\n",
      "Iteration 303, loss = 0.72172006\n",
      "Iteration 304, loss = 0.72040097\n",
      "Iteration 305, loss = 0.71909136\n",
      "Iteration 306, loss = 0.71779118\n",
      "Iteration 307, loss = 0.71650035\n",
      "Iteration 308, loss = 0.71521881\n",
      "Iteration 309, loss = 0.71394650\n",
      "Iteration 310, loss = 0.71268336\n",
      "Iteration 311, loss = 0.71142932\n",
      "Iteration 312, loss = 0.71018432\n",
      "Iteration 313, loss = 0.70894830\n",
      "Iteration 314, loss = 0.70772118\n",
      "Iteration 315, loss = 0.70650292\n",
      "Iteration 316, loss = 0.70529344\n",
      "Iteration 317, loss = 0.70409267\n",
      "Iteration 318, loss = 0.70290057\n",
      "Iteration 319, loss = 0.70171705\n",
      "Iteration 320, loss = 0.70054207\n",
      "Iteration 321, loss = 0.69937555\n",
      "Iteration 322, loss = 0.69821744\n",
      "Iteration 323, loss = 0.69706766\n",
      "Iteration 324, loss = 0.69592616\n",
      "Iteration 325, loss = 0.69479287\n",
      "Iteration 326, loss = 0.69366774\n",
      "Iteration 327, loss = 0.69255069\n",
      "Iteration 328, loss = 0.69144167\n",
      "Iteration 329, loss = 0.69034062\n",
      "Iteration 330, loss = 0.68924746\n",
      "Iteration 331, loss = 0.68816215\n",
      "Iteration 332, loss = 0.68708462\n",
      "Iteration 333, loss = 0.68601481\n",
      "Iteration 334, loss = 0.68495265\n",
      "Iteration 335, loss = 0.68389809\n",
      "Iteration 336, loss = 0.68285107\n",
      "Iteration 337, loss = 0.68181153\n",
      "Iteration 338, loss = 0.68077940\n",
      "Iteration 339, loss = 0.67975464\n",
      "Iteration 340, loss = 0.67873717\n",
      "Iteration 341, loss = 0.67772694\n",
      "Iteration 342, loss = 0.67672389\n",
      "Iteration 343, loss = 0.67572797\n",
      "Iteration 344, loss = 0.67473911\n",
      "Iteration 345, loss = 0.67375726\n",
      "Iteration 346, loss = 0.67278237\n",
      "Iteration 347, loss = 0.67181437\n",
      "Iteration 348, loss = 0.67085320\n",
      "Iteration 349, loss = 0.66989882\n",
      "Iteration 350, loss = 0.66895117\n",
      "Iteration 351, loss = 0.66801019\n",
      "Iteration 352, loss = 0.66707582\n",
      "Iteration 353, loss = 0.66614801\n",
      "Iteration 354, loss = 0.66522672\n",
      "Iteration 355, loss = 0.66431187\n",
      "Iteration 356, loss = 0.66340343\n",
      "Iteration 357, loss = 0.66250133\n",
      "Iteration 358, loss = 0.66160553\n",
      "Iteration 359, loss = 0.66071597\n",
      "Iteration 360, loss = 0.65983260\n",
      "Iteration 361, loss = 0.65895536\n",
      "Iteration 362, loss = 0.65808421\n",
      "Iteration 363, loss = 0.65721910\n",
      "Iteration 364, loss = 0.65635998\n",
      "Iteration 365, loss = 0.65550679\n",
      "Iteration 366, loss = 0.65465948\n",
      "Iteration 367, loss = 0.65381801\n",
      "Iteration 368, loss = 0.65298233\n",
      "Iteration 369, loss = 0.65215238\n",
      "Iteration 370, loss = 0.65132813\n",
      "Iteration 371, loss = 0.65050952\n",
      "Iteration 372, loss = 0.64969650\n",
      "Iteration 373, loss = 0.64888903\n",
      "Iteration 374, loss = 0.64808706\n",
      "Iteration 375, loss = 0.64729054\n",
      "Iteration 376, loss = 0.64649944\n",
      "Iteration 377, loss = 0.64571369\n",
      "Iteration 378, loss = 0.64493326\n",
      "Iteration 379, loss = 0.64415811\n",
      "Iteration 380, loss = 0.64338818\n",
      "Iteration 381, loss = 0.64262344\n",
      "Iteration 382, loss = 0.64186384\n",
      "Iteration 383, loss = 0.64110933\n",
      "Iteration 384, loss = 0.64035988\n",
      "Iteration 385, loss = 0.63961544\n",
      "Iteration 386, loss = 0.63887597\n",
      "Iteration 387, loss = 0.63814142\n",
      "Iteration 388, loss = 0.63741176\n",
      "Iteration 389, loss = 0.63668694\n",
      "Iteration 390, loss = 0.63596693\n",
      "Iteration 391, loss = 0.63525168\n",
      "Iteration 392, loss = 0.63454115\n",
      "Iteration 393, loss = 0.63383530\n",
      "Iteration 394, loss = 0.63313409\n",
      "Iteration 395, loss = 0.63243749\n",
      "Iteration 396, loss = 0.63174545\n",
      "Iteration 397, loss = 0.63105794\n",
      "Iteration 398, loss = 0.63037492\n",
      "Iteration 399, loss = 0.62969635\n",
      "Iteration 400, loss = 0.62902219\n",
      "Iteration 401, loss = 0.62835240\n",
      "Iteration 402, loss = 0.62768696\n",
      "Iteration 403, loss = 0.62702581\n",
      "Iteration 404, loss = 0.62636894\n",
      "Iteration 405, loss = 0.62571629\n",
      "Iteration 406, loss = 0.62506784\n",
      "Iteration 407, loss = 0.62442354\n",
      "Iteration 408, loss = 0.62378338\n",
      "Iteration 409, loss = 0.62314730\n",
      "Iteration 410, loss = 0.62251528\n",
      "Iteration 411, loss = 0.62188727\n",
      "Iteration 412, loss = 0.62126326\n",
      "Iteration 413, loss = 0.62064320\n",
      "Iteration 414, loss = 0.62002707\n",
      "Iteration 415, loss = 0.61941482\n",
      "Iteration 416, loss = 0.61880643\n",
      "Iteration 417, loss = 0.61820186\n",
      "Iteration 418, loss = 0.61760109\n",
      "Iteration 419, loss = 0.61700408\n",
      "Iteration 420, loss = 0.61641079\n",
      "Iteration 421, loss = 0.61582121\n",
      "Iteration 422, loss = 0.61523529\n",
      "Iteration 423, loss = 0.61465301\n",
      "Iteration 424, loss = 0.61407434\n",
      "Iteration 425, loss = 0.61349924\n",
      "Iteration 426, loss = 0.61292770\n",
      "Iteration 427, loss = 0.61235967\n",
      "Iteration 428, loss = 0.61179513\n",
      "Iteration 429, loss = 0.61123405\n",
      "Iteration 430, loss = 0.61067640\n",
      "Iteration 431, loss = 0.61012216\n",
      "Iteration 432, loss = 0.60957129\n",
      "Iteration 433, loss = 0.60902377\n",
      "Iteration 434, loss = 0.60847957\n",
      "Iteration 435, loss = 0.60793866\n",
      "Iteration 436, loss = 0.60740102\n",
      "Iteration 437, loss = 0.60686662\n",
      "Iteration 438, loss = 0.60633543\n",
      "Iteration 439, loss = 0.60580743\n",
      "Iteration 440, loss = 0.60528259\n",
      "Iteration 441, loss = 0.60476088\n",
      "Iteration 442, loss = 0.60424228\n",
      "Iteration 443, loss = 0.60372677\n",
      "Iteration 444, loss = 0.60321431\n",
      "Iteration 445, loss = 0.60270489\n",
      "Iteration 446, loss = 0.60219848\n",
      "Iteration 447, loss = 0.60169505\n",
      "Iteration 448, loss = 0.60119458\n",
      "Iteration 449, loss = 0.60069705\n",
      "Iteration 450, loss = 0.60020243\n",
      "Iteration 451, loss = 0.59971070\n",
      "Iteration 452, loss = 0.59922184\n",
      "Iteration 453, loss = 0.59873582\n",
      "Iteration 454, loss = 0.59825262\n",
      "Iteration 455, loss = 0.59777222\n",
      "Iteration 456, loss = 0.59729459\n",
      "Iteration 457, loss = 0.59681971\n",
      "Iteration 458, loss = 0.59634757\n",
      "Iteration 459, loss = 0.59587813\n",
      "Iteration 460, loss = 0.59541138\n",
      "Iteration 461, loss = 0.59494730\n",
      "Iteration 462, loss = 0.59448586\n",
      "Iteration 463, loss = 0.59402705\n",
      "Iteration 464, loss = 0.59357083\n",
      "Iteration 465, loss = 0.59311720\n",
      "Iteration 466, loss = 0.59266613\n",
      "Iteration 467, loss = 0.59221760\n",
      "Iteration 468, loss = 0.59177159\n",
      "Iteration 469, loss = 0.59132808\n",
      "Iteration 470, loss = 0.59088705\n",
      "Iteration 471, loss = 0.59044849\n",
      "Iteration 472, loss = 0.59001236\n",
      "Iteration 473, loss = 0.58957866\n",
      "Iteration 474, loss = 0.58914736\n",
      "Iteration 475, loss = 0.58871845\n",
      "Iteration 476, loss = 0.58829190\n",
      "Iteration 477, loss = 0.58786770\n",
      "Iteration 478, loss = 0.58744583\n",
      "Iteration 479, loss = 0.58702628\n",
      "Iteration 480, loss = 0.58660901\n",
      "Iteration 481, loss = 0.58619402\n",
      "Iteration 482, loss = 0.58578128\n",
      "Iteration 483, loss = 0.58537079\n",
      "Iteration 484, loss = 0.58496252\n",
      "Iteration 485, loss = 0.58455645\n",
      "Iteration 486, loss = 0.58415257\n",
      "Iteration 487, loss = 0.58375086\n",
      "Iteration 488, loss = 0.58335131\n",
      "Iteration 489, loss = 0.58295389\n",
      "Iteration 490, loss = 0.58255860\n",
      "Iteration 491, loss = 0.58216541\n",
      "Iteration 492, loss = 0.58177430\n",
      "Iteration 493, loss = 0.58138527\n",
      "Iteration 494, loss = 0.58099830\n",
      "Iteration 495, loss = 0.58061337\n",
      "Iteration 496, loss = 0.58023046\n",
      "Iteration 497, loss = 0.57984956\n",
      "Iteration 498, loss = 0.57947066\n",
      "Iteration 499, loss = 0.57909373\n",
      "Iteration 500, loss = 0.57871877\n",
      "Iteration 501, loss = 0.57834576\n",
      "Iteration 502, loss = 0.57797468\n",
      "Iteration 503, loss = 0.57760552\n",
      "Iteration 504, loss = 0.57723827\n",
      "Iteration 505, loss = 0.57687291\n",
      "Iteration 506, loss = 0.57650942\n",
      "Iteration 507, loss = 0.57614779\n",
      "Iteration 508, loss = 0.57578802\n",
      "Iteration 509, loss = 0.57543007\n",
      "Iteration 510, loss = 0.57507395\n",
      "Iteration 511, loss = 0.57471963\n",
      "Iteration 512, loss = 0.57436710\n",
      "Iteration 513, loss = 0.57401636\n",
      "Iteration 514, loss = 0.57366738\n",
      "Iteration 515, loss = 0.57332015\n",
      "Iteration 516, loss = 0.57297466\n",
      "Iteration 517, loss = 0.57263090\n",
      "Iteration 518, loss = 0.57228885\n",
      "Iteration 519, loss = 0.57194851\n",
      "Iteration 520, loss = 0.57160985\n",
      "Iteration 521, loss = 0.57127287\n",
      "Iteration 522, loss = 0.57093755\n",
      "Iteration 523, loss = 0.57060388\n",
      "Iteration 524, loss = 0.57027185\n",
      "Iteration 525, loss = 0.56994144\n",
      "Iteration 526, loss = 0.56961265\n",
      "Iteration 527, loss = 0.56928546\n",
      "Iteration 528, loss = 0.56895987\n",
      "Iteration 529, loss = 0.56863585\n",
      "Iteration 530, loss = 0.56831340\n",
      "Iteration 531, loss = 0.56799250\n",
      "Iteration 532, loss = 0.56767315\n",
      "Iteration 533, loss = 0.56735533\n",
      "Iteration 534, loss = 0.56703903\n",
      "Iteration 535, loss = 0.56672424\n",
      "Iteration 536, loss = 0.56641096\n",
      "Iteration 537, loss = 0.56609916\n",
      "Iteration 538, loss = 0.56578883\n",
      "Iteration 539, loss = 0.56547998\n",
      "Iteration 540, loss = 0.56517258\n",
      "Iteration 541, loss = 0.56486662\n",
      "Iteration 542, loss = 0.56456211\n",
      "Iteration 543, loss = 0.56425901\n",
      "Iteration 544, loss = 0.56395733\n",
      "Iteration 545, loss = 0.56365705\n",
      "Iteration 546, loss = 0.56335817\n",
      "Iteration 547, loss = 0.56306067\n",
      "Iteration 548, loss = 0.56276454\n",
      "Iteration 549, loss = 0.56246977\n",
      "Iteration 550, loss = 0.56217636\n",
      "Iteration 551, loss = 0.56188430\n",
      "Iteration 552, loss = 0.56159356\n",
      "Iteration 553, loss = 0.56130416\n",
      "Iteration 554, loss = 0.56101606\n",
      "Iteration 555, loss = 0.56072928\n",
      "Iteration 556, loss = 0.56044378\n",
      "Iteration 557, loss = 0.56015958\n",
      "Iteration 558, loss = 0.55987665\n",
      "Iteration 559, loss = 0.55959499\n",
      "Iteration 560, loss = 0.55931459\n",
      "Iteration 561, loss = 0.55903544\n",
      "Iteration 562, loss = 0.55875753\n",
      "Iteration 563, loss = 0.55848086\n",
      "Iteration 564, loss = 0.55820541\n",
      "Iteration 565, loss = 0.55793117\n",
      "Iteration 566, loss = 0.55765814\n",
      "Iteration 567, loss = 0.55738630\n",
      "Iteration 568, loss = 0.55711566\n",
      "Iteration 569, loss = 0.55684619\n",
      "Iteration 570, loss = 0.55657790\n",
      "Iteration 571, loss = 0.55631077\n",
      "Iteration 572, loss = 0.55604480\n",
      "Iteration 573, loss = 0.55577998\n",
      "Iteration 574, loss = 0.55551629\n",
      "Iteration 575, loss = 0.55525373\n",
      "Iteration 576, loss = 0.55499230\n",
      "Iteration 577, loss = 0.55473199\n",
      "Iteration 578, loss = 0.55447278\n",
      "Iteration 579, loss = 0.55421467\n",
      "Iteration 580, loss = 0.55395765\n",
      "Iteration 581, loss = 0.55370171\n",
      "Iteration 582, loss = 0.55344686\n",
      "Iteration 583, loss = 0.55319307\n",
      "Iteration 584, loss = 0.55294034\n",
      "Iteration 585, loss = 0.55268866\n",
      "Iteration 586, loss = 0.55243804\n",
      "Iteration 587, loss = 0.55218845\n",
      "Iteration 588, loss = 0.55193989\n",
      "Iteration 589, loss = 0.55169236\n",
      "Iteration 590, loss = 0.55144584\n",
      "Iteration 591, loss = 0.55120033\n",
      "Iteration 592, loss = 0.55095583\n",
      "Iteration 593, loss = 0.55071232\n",
      "Iteration 594, loss = 0.55046980\n",
      "Iteration 595, loss = 0.55022827\n",
      "Iteration 596, loss = 0.54998770\n",
      "Iteration 597, loss = 0.54974811\n",
      "Iteration 598, loss = 0.54950948\n",
      "Iteration 599, loss = 0.54927180\n",
      "Iteration 600, loss = 0.54903507\n",
      "Iteration 601, loss = 0.54879928\n",
      "Iteration 602, loss = 0.54856443\n",
      "Iteration 603, loss = 0.54833050\n",
      "Iteration 604, loss = 0.54809750\n",
      "Iteration 605, loss = 0.54786541\n",
      "Iteration 606, loss = 0.54763423\n",
      "Iteration 607, loss = 0.54740395\n",
      "Iteration 608, loss = 0.54717457\n",
      "Iteration 609, loss = 0.54694608\n",
      "Iteration 610, loss = 0.54671847\n",
      "Iteration 611, loss = 0.54649174\n",
      "Iteration 612, loss = 0.54626588\n",
      "Iteration 613, loss = 0.54604089\n",
      "Iteration 614, loss = 0.54581675\n",
      "Iteration 615, loss = 0.54559347\n",
      "Iteration 616, loss = 0.54537104\n",
      "Iteration 617, loss = 0.54514945\n",
      "Iteration 618, loss = 0.54492869\n",
      "Iteration 619, loss = 0.54470876\n",
      "Iteration 620, loss = 0.54448966\n",
      "Iteration 621, loss = 0.54427137\n",
      "Iteration 622, loss = 0.54405390\n",
      "Iteration 623, loss = 0.54383723\n",
      "Iteration 624, loss = 0.54362136\n",
      "Iteration 625, loss = 0.54340629\n",
      "Iteration 626, loss = 0.54319201\n",
      "Iteration 627, loss = 0.54297852\n",
      "Iteration 628, loss = 0.54276580\n",
      "Iteration 629, loss = 0.54255385\n",
      "Iteration 630, loss = 0.54234268\n",
      "Iteration 631, loss = 0.54213226\n",
      "Iteration 632, loss = 0.54192261\n",
      "Iteration 633, loss = 0.54171370\n",
      "Iteration 634, loss = 0.54150554\n",
      "Iteration 635, loss = 0.54129813\n",
      "Iteration 636, loss = 0.54109145\n",
      "Iteration 637, loss = 0.54088550\n",
      "Iteration 638, loss = 0.54068027\n",
      "Iteration 639, loss = 0.54047577\n",
      "Iteration 640, loss = 0.54027198\n",
      "Iteration 641, loss = 0.54006891\n",
      "Iteration 642, loss = 0.53986654\n",
      "Iteration 643, loss = 0.53966487\n",
      "Iteration 644, loss = 0.53946389\n",
      "Iteration 645, loss = 0.53926361\n",
      "Iteration 646, loss = 0.53906401\n",
      "Iteration 647, loss = 0.53886510\n",
      "Iteration 648, loss = 0.53866686\n",
      "Iteration 649, loss = 0.53846929\n",
      "Iteration 650, loss = 0.53827239\n",
      "Iteration 651, loss = 0.53807615\n",
      "Iteration 652, loss = 0.53788057\n",
      "Iteration 653, loss = 0.53768564\n",
      "Iteration 654, loss = 0.53749136\n",
      "Iteration 655, loss = 0.53729773\n",
      "Iteration 656, loss = 0.53710473\n",
      "Iteration 657, loss = 0.53691236\n",
      "Iteration 658, loss = 0.53672063\n",
      "Iteration 659, loss = 0.53652952\n",
      "Iteration 660, loss = 0.53633904\n",
      "Iteration 661, loss = 0.53614917\n",
      "Iteration 662, loss = 0.53595991\n",
      "Iteration 663, loss = 0.53577126\n",
      "Iteration 664, loss = 0.53558321\n",
      "Iteration 665, loss = 0.53539576\n",
      "Iteration 666, loss = 0.53520891\n",
      "Iteration 667, loss = 0.53502265\n",
      "Iteration 668, loss = 0.53483697\n",
      "Iteration 669, loss = 0.53465188\n",
      "Iteration 670, loss = 0.53446736\n",
      "Iteration 671, loss = 0.53428342\n",
      "Iteration 672, loss = 0.53410005\n",
      "Iteration 673, loss = 0.53391724\n",
      "Iteration 674, loss = 0.53373500\n",
      "Iteration 675, loss = 0.53355331\n",
      "Iteration 676, loss = 0.53337217\n",
      "Iteration 677, loss = 0.53319159\n",
      "Iteration 678, loss = 0.53301155\n",
      "Iteration 679, loss = 0.53283205\n",
      "Iteration 680, loss = 0.53265308\n",
      "Iteration 681, loss = 0.53247465\n",
      "Iteration 682, loss = 0.53229676\n",
      "Iteration 683, loss = 0.53211938\n",
      "Iteration 684, loss = 0.53194253\n",
      "Iteration 685, loss = 0.53176619\n",
      "Iteration 686, loss = 0.53159037\n",
      "Iteration 687, loss = 0.53141506\n",
      "Iteration 688, loss = 0.53124026\n",
      "Iteration 689, loss = 0.53106596\n",
      "Iteration 690, loss = 0.53089216\n",
      "Iteration 691, loss = 0.53071885\n",
      "Iteration 692, loss = 0.53054603\n",
      "Iteration 693, loss = 0.53037371\n",
      "Iteration 694, loss = 0.53020186\n",
      "Iteration 695, loss = 0.53003050\n",
      "Iteration 696, loss = 0.52985961\n",
      "Iteration 697, loss = 0.52968920\n",
      "Iteration 698, loss = 0.52951926\n",
      "Iteration 699, loss = 0.52934978\n",
      "Iteration 700, loss = 0.52918077\n",
      "Iteration 701, loss = 0.52901221\n",
      "Iteration 702, loss = 0.52884411\n",
      "Iteration 703, loss = 0.52867646\n",
      "Iteration 704, loss = 0.52850927\n",
      "Iteration 705, loss = 0.52834251\n",
      "Iteration 706, loss = 0.52817620\n",
      "Iteration 707, loss = 0.52801033\n",
      "Iteration 708, loss = 0.52784489\n",
      "Iteration 709, loss = 0.52767988\n",
      "Iteration 710, loss = 0.52751531\n",
      "Iteration 711, loss = 0.52735115\n",
      "Iteration 712, loss = 0.52718742\n",
      "Iteration 713, loss = 0.52702411\n",
      "Iteration 714, loss = 0.52686121\n",
      "Iteration 715, loss = 0.52669872\n",
      "Iteration 716, loss = 0.52653665\n",
      "Iteration 717, loss = 0.52637497\n",
      "Iteration 718, loss = 0.52621370\n",
      "Iteration 719, loss = 0.52605283\n",
      "Iteration 720, loss = 0.52589236\n",
      "Iteration 721, loss = 0.52573227\n",
      "Iteration 722, loss = 0.52557258\n",
      "Iteration 723, loss = 0.52541327\n",
      "Iteration 724, loss = 0.52525435\n",
      "Iteration 725, loss = 0.52509580\n",
      "Iteration 726, loss = 0.52493764\n",
      "Iteration 727, loss = 0.52477984\n",
      "Iteration 728, loss = 0.52462242\n",
      "Iteration 729, loss = 0.52446536\n",
      "Iteration 730, loss = 0.52430867\n",
      "Iteration 731, loss = 0.52415235\n",
      "Iteration 732, loss = 0.52399637\n",
      "Iteration 733, loss = 0.52384076\n",
      "Iteration 734, loss = 0.52368550\n",
      "Iteration 735, loss = 0.52353059\n",
      "Iteration 736, loss = 0.52337602\n",
      "Iteration 737, loss = 0.52322180\n",
      "Iteration 738, loss = 0.52306792\n",
      "Iteration 739, loss = 0.52291438\n",
      "Iteration 740, loss = 0.52276117\n",
      "Iteration 741, loss = 0.52260829\n",
      "Iteration 742, loss = 0.52245575\n",
      "Iteration 743, loss = 0.52230353\n",
      "Iteration 744, loss = 0.52215163\n",
      "Iteration 745, loss = 0.52200006\n",
      "Iteration 746, loss = 0.52184880\n",
      "Iteration 747, loss = 0.52169786\n",
      "Iteration 748, loss = 0.52154723\n",
      "Iteration 749, loss = 0.52139691\n",
      "Iteration 750, loss = 0.52124690\n",
      "Iteration 751, loss = 0.52109719\n",
      "Iteration 752, loss = 0.52094779\n",
      "Iteration 753, loss = 0.52079868\n",
      "Iteration 754, loss = 0.52064987\n",
      "Iteration 755, loss = 0.52050135\n",
      "Iteration 756, loss = 0.52035312\n",
      "Iteration 757, loss = 0.52020518\n",
      "Iteration 758, loss = 0.52005753\n",
      "Iteration 759, loss = 0.51991016\n",
      "Iteration 760, loss = 0.51976307\n",
      "Iteration 761, loss = 0.51961625\n",
      "Iteration 762, loss = 0.51946972\n",
      "Iteration 763, loss = 0.51932345\n",
      "Iteration 764, loss = 0.51917745\n",
      "Iteration 765, loss = 0.51903172\n",
      "Iteration 766, loss = 0.51888626\n",
      "Iteration 767, loss = 0.51874105\n",
      "Iteration 768, loss = 0.51859611\n",
      "Iteration 769, loss = 0.51845142\n",
      "Iteration 770, loss = 0.51830699\n",
      "Iteration 771, loss = 0.51816281\n",
      "Iteration 772, loss = 0.51801887\n",
      "Iteration 773, loss = 0.51787519\n",
      "Iteration 774, loss = 0.51773175\n",
      "Iteration 775, loss = 0.51758855\n",
      "Iteration 776, loss = 0.51744559\n",
      "Iteration 777, loss = 0.51730287\n",
      "Iteration 778, loss = 0.51716038\n",
      "Iteration 779, loss = 0.51701812\n",
      "Iteration 780, loss = 0.51687610\n",
      "Iteration 781, loss = 0.51673430\n",
      "Iteration 782, loss = 0.51659272\n",
      "Iteration 783, loss = 0.51645137\n",
      "Iteration 784, loss = 0.51631024\n",
      "Iteration 785, loss = 0.51616933\n",
      "Iteration 786, loss = 0.51602863\n",
      "Iteration 787, loss = 0.51588814\n",
      "Iteration 788, loss = 0.51574787\n",
      "Iteration 789, loss = 0.51560780\n",
      "Iteration 790, loss = 0.51546794\n",
      "Iteration 791, loss = 0.51532829\n",
      "Iteration 792, loss = 0.51518883\n",
      "Iteration 793, loss = 0.51504958\n",
      "Iteration 794, loss = 0.51491052\n",
      "Iteration 795, loss = 0.51477166\n",
      "Iteration 796, loss = 0.51463299\n",
      "Iteration 797, loss = 0.51449450\n",
      "Iteration 798, loss = 0.51435621\n",
      "Iteration 799, loss = 0.51421810\n",
      "Iteration 800, loss = 0.51408018\n",
      "Iteration 801, loss = 0.51394244\n",
      "Iteration 802, loss = 0.51380487\n",
      "Iteration 803, loss = 0.51366749\n",
      "Iteration 804, loss = 0.51353028\n",
      "Iteration 805, loss = 0.51339324\n",
      "Iteration 806, loss = 0.51325637\n",
      "Iteration 807, loss = 0.51311967\n",
      "Iteration 808, loss = 0.51298313\n",
      "Iteration 809, loss = 0.51284676\n",
      "Iteration 810, loss = 0.51271055\n",
      "Iteration 811, loss = 0.51257450\n",
      "Iteration 812, loss = 0.51243861\n",
      "Iteration 813, loss = 0.51230288\n",
      "Iteration 814, loss = 0.51216729\n",
      "Iteration 815, loss = 0.51203186\n",
      "Iteration 816, loss = 0.51189658\n",
      "Iteration 817, loss = 0.51176145\n",
      "Iteration 818, loss = 0.51162646\n",
      "Iteration 819, loss = 0.51149161\n",
      "Iteration 820, loss = 0.51135690\n",
      "Iteration 821, loss = 0.51122234\n",
      "Iteration 822, loss = 0.51108791\n",
      "Iteration 823, loss = 0.51095361\n",
      "Iteration 824, loss = 0.51081945\n",
      "Iteration 825, loss = 0.51068542\n",
      "Iteration 826, loss = 0.51055152\n",
      "Iteration 827, loss = 0.51041775\n",
      "Iteration 828, loss = 0.51028410\n",
      "Iteration 829, loss = 0.51015057\n",
      "Iteration 830, loss = 0.51001717\n",
      "Iteration 831, loss = 0.50988388\n",
      "Iteration 832, loss = 0.50975071\n",
      "Iteration 833, loss = 0.50961766\n",
      "Iteration 834, loss = 0.50948472\n",
      "Iteration 835, loss = 0.50935189\n",
      "Iteration 836, loss = 0.50921917\n",
      "Iteration 837, loss = 0.50908656\n",
      "Iteration 838, loss = 0.50895406\n",
      "Iteration 839, loss = 0.50882166\n",
      "Iteration 840, loss = 0.50868936\n",
      "Iteration 841, loss = 0.50855716\n",
      "Iteration 842, loss = 0.50842506\n",
      "Iteration 843, loss = 0.50829306\n",
      "Iteration 844, loss = 0.50816115\n",
      "Iteration 845, loss = 0.50802933\n",
      "Iteration 846, loss = 0.50789761\n",
      "Iteration 847, loss = 0.50776597\n",
      "Iteration 848, loss = 0.50763443\n",
      "Iteration 849, loss = 0.50750296\n",
      "Iteration 850, loss = 0.50737159\n",
      "Iteration 851, loss = 0.50724029\n",
      "Iteration 852, loss = 0.50710907\n",
      "Iteration 853, loss = 0.50697794\n",
      "Iteration 854, loss = 0.50684688\n",
      "Iteration 855, loss = 0.50671589\n",
      "Iteration 856, loss = 0.50658498\n",
      "Iteration 857, loss = 0.50645414\n",
      "Iteration 858, loss = 0.50632337\n",
      "Iteration 859, loss = 0.50619267\n",
      "Iteration 860, loss = 0.50606204\n",
      "Iteration 861, loss = 0.50593146\n",
      "Iteration 862, loss = 0.50580096\n",
      "Iteration 863, loss = 0.50567051\n",
      "Iteration 864, loss = 0.50554013\n",
      "Iteration 865, loss = 0.50540980\n",
      "Iteration 866, loss = 0.50527953\n",
      "Iteration 867, loss = 0.50514931\n",
      "Iteration 868, loss = 0.50501915\n",
      "Iteration 869, loss = 0.50488904\n",
      "Iteration 870, loss = 0.50475898\n",
      "Iteration 871, loss = 0.50462896\n",
      "Iteration 872, loss = 0.50449900\n",
      "Iteration 873, loss = 0.50436908\n",
      "Iteration 874, loss = 0.50423920\n",
      "Iteration 875, loss = 0.50410936\n",
      "Iteration 876, loss = 0.50397957\n",
      "Iteration 877, loss = 0.50384981\n",
      "Iteration 878, loss = 0.50372009\n",
      "Iteration 879, loss = 0.50359041\n",
      "Iteration 880, loss = 0.50346076\n",
      "Iteration 881, loss = 0.50333115\n",
      "Iteration 882, loss = 0.50320156\n",
      "Iteration 883, loss = 0.50307201\n",
      "Iteration 884, loss = 0.50294248\n",
      "Iteration 885, loss = 0.50281298\n",
      "Iteration 886, loss = 0.50268350\n",
      "Iteration 887, loss = 0.50255405\n",
      "Iteration 888, loss = 0.50242462\n",
      "Iteration 889, loss = 0.50229522\n",
      "Iteration 890, loss = 0.50216583\n",
      "Iteration 891, loss = 0.50203646\n",
      "Iteration 892, loss = 0.50190710\n",
      "Iteration 893, loss = 0.50177776\n",
      "Iteration 894, loss = 0.50164844\n",
      "Iteration 895, loss = 0.50151912\n",
      "Iteration 896, loss = 0.50138982\n",
      "Iteration 897, loss = 0.50126053\n",
      "Iteration 898, loss = 0.50113124\n",
      "Iteration 899, loss = 0.50100196\n",
      "Iteration 900, loss = 0.50087269\n",
      "Iteration 901, loss = 0.50074342\n",
      "Iteration 902, loss = 0.50061415\n",
      "Iteration 903, loss = 0.50048488\n",
      "Iteration 904, loss = 0.50035562\n",
      "Iteration 905, loss = 0.50022635\n",
      "Iteration 906, loss = 0.50009708\n",
      "Iteration 907, loss = 0.49996780\n",
      "Iteration 908, loss = 0.49983852\n",
      "Iteration 909, loss = 0.49970924\n",
      "Iteration 910, loss = 0.49957994\n",
      "Iteration 911, loss = 0.49945064\n",
      "Iteration 912, loss = 0.49932132\n",
      "Iteration 913, loss = 0.49919200\n",
      "Iteration 914, loss = 0.49906266\n",
      "Iteration 915, loss = 0.49893330\n",
      "Iteration 916, loss = 0.49880393\n",
      "Iteration 917, loss = 0.49867455\n",
      "Iteration 918, loss = 0.49854514\n",
      "Iteration 919, loss = 0.49841572\n",
      "Iteration 920, loss = 0.49828628\n",
      "Iteration 921, loss = 0.49815681\n",
      "Iteration 922, loss = 0.49802732\n",
      "Iteration 923, loss = 0.49789781\n",
      "Iteration 924, loss = 0.49776827\n",
      "Iteration 925, loss = 0.49763871\n",
      "Iteration 926, loss = 0.49750912\n",
      "Iteration 927, loss = 0.49737950\n",
      "Iteration 928, loss = 0.49724985\n",
      "Iteration 929, loss = 0.49712017\n",
      "Iteration 930, loss = 0.49699045\n",
      "Iteration 931, loss = 0.49686071\n",
      "Iteration 932, loss = 0.49673093\n",
      "Iteration 933, loss = 0.49660111\n",
      "Iteration 934, loss = 0.49647126\n",
      "Iteration 935, loss = 0.49634137\n",
      "Iteration 936, loss = 0.49621145\n",
      "Iteration 937, loss = 0.49608148\n",
      "Iteration 938, loss = 0.49595147\n",
      "Iteration 939, loss = 0.49582142\n",
      "Iteration 940, loss = 0.49569133\n",
      "Iteration 941, loss = 0.49556119\n",
      "Iteration 942, loss = 0.49543101\n",
      "Iteration 943, loss = 0.49530079\n",
      "Iteration 944, loss = 0.49517052\n",
      "Iteration 945, loss = 0.49504019\n",
      "Iteration 946, loss = 0.49490983\n",
      "Iteration 947, loss = 0.49477941\n",
      "Iteration 948, loss = 0.49464894\n",
      "Iteration 949, loss = 0.49451842\n",
      "Iteration 950, loss = 0.49438784\n",
      "Iteration 951, loss = 0.49425722\n",
      "Iteration 952, loss = 0.49412653\n",
      "Iteration 953, loss = 0.49399580\n",
      "Iteration 954, loss = 0.49386501\n",
      "Iteration 955, loss = 0.49373416\n",
      "Iteration 956, loss = 0.49360325\n",
      "Iteration 957, loss = 0.49347228\n",
      "Iteration 958, loss = 0.49334125\n",
      "Iteration 959, loss = 0.49321017\n",
      "Iteration 960, loss = 0.49307902\n",
      "Iteration 961, loss = 0.49294781\n",
      "Iteration 962, loss = 0.49281653\n",
      "Iteration 963, loss = 0.49268520\n",
      "Iteration 964, loss = 0.49255379\n",
      "Iteration 965, loss = 0.49242233\n",
      "Iteration 966, loss = 0.49229079\n",
      "Iteration 967, loss = 0.49215919\n",
      "Iteration 968, loss = 0.49202752\n",
      "Iteration 969, loss = 0.49189578\n",
      "Iteration 970, loss = 0.49176397\n",
      "Iteration 971, loss = 0.49163209\n",
      "Iteration 972, loss = 0.49150014\n",
      "Iteration 973, loss = 0.49136812\n",
      "Iteration 974, loss = 0.49123603\n",
      "Iteration 975, loss = 0.49110386\n",
      "Iteration 976, loss = 0.49097162\n",
      "Iteration 977, loss = 0.49083930\n",
      "Iteration 978, loss = 0.49070691\n",
      "Iteration 979, loss = 0.49057444\n",
      "Iteration 980, loss = 0.49044190\n",
      "Iteration 981, loss = 0.49030928\n",
      "Iteration 982, loss = 0.49017658\n",
      "Iteration 983, loss = 0.49004380\n",
      "Iteration 984, loss = 0.48991094\n",
      "Iteration 985, loss = 0.48977801\n",
      "Iteration 986, loss = 0.48964499\n",
      "Iteration 987, loss = 0.48951189\n",
      "Iteration 988, loss = 0.48937871\n",
      "Iteration 989, loss = 0.48924544\n",
      "Iteration 990, loss = 0.48911210\n",
      "Iteration 991, loss = 0.48897867\n",
      "Iteration 992, loss = 0.48884515\n",
      "Iteration 993, loss = 0.48871155\n",
      "Iteration 994, loss = 0.48857787\n",
      "Iteration 995, loss = 0.48844410\n",
      "Iteration 996, loss = 0.48831024\n",
      "Iteration 997, loss = 0.48817630\n",
      "Iteration 998, loss = 0.48804226\n",
      "Iteration 999, loss = 0.48790814\n",
      "Iteration 1000, loss = 0.48777394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhruv/indexCode/cmpsc445/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 5-fold cross validation\n",
    "cv_results = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "msg = \"Average Accuracy on cross-validation: {cv_results.mean():.4f} ({cv_results.std():.4f})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: There were many instances where the optimization did not converge. But the most recent value stored in `msg`.\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th rowspan=\"2\">Hidden Neurons</th>\n",
    "            <th colspan=\"3\">Learning Rates</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>0.001</th>\n",
    "            <th>0.01</th>\n",
    "            <th>0.1</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th>1</th>\n",
    "            <td>0.4500 (0.1862)</td>\n",
    "            <td>0.6578 (0.2389)</td>\n",
    "            <td>0.7178 (0.2284)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>3</th>\n",
    "            <td>0.6333 (0.1922)</td>\n",
    "            <td>0.9122 (0.1220)</td>\n",
    "            <td>0.9722 (0.0322)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>5</th>\n",
    "            <td>0.6922 (0.1919)</td>\n",
    "            <td>0.9822 (0.0252)</td>\n",
    "            <td>0.9789 (0.0267)</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network Configuration Selected: \n",
    "- Hidden Nuerons: 5\n",
    "- Learning Rate: 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhruv/indexCode/cmpsc445/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(5,),\n",
       "              learning_rate_init=0.01, max_iter=1000, solver=&#x27;sgd&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(5,),\n",
       "              learning_rate_init=0.01, max_iter=1000, solver=&#x27;sgd&#x27;)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(activation='logistic', hidden_layer_sizes=(5,),\n",
       "              learning_rate_init=0.01, max_iter=1000, solver='sgd')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_nuerons = 5\n",
    "learning_rate = 0.01\n",
    "\n",
    "clf_fin = MLPClassifier(\n",
    "    solver = 'sgd', \n",
    "    activation = 'logistic',                 \n",
    "    learning_rate_init = learning_rate, \n",
    "    learning_rate = 'constant', \n",
    "    max_iter = 1000, \n",
    "    verbose = False,\n",
    "    hidden_layer_sizes = (hidden_nuerons,)\n",
    ")\n",
    "\n",
    "clf_fin.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "Testing on entire test dataset `y_test`\n",
    "\n",
    "Hidden Layer Size: 5\n",
    "<br>\n",
    "Learning Rate: 0.01\n",
    "\n",
    "Reported Accuracy: 98.333%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.983\n",
      "Accuracy Percentage: 98.333%\n"
     ]
    }
   ],
   "source": [
    "# predictions with the selected neural network classifier with respective hidden layers and learning rate\n",
    "results = clf_fin.predict(X_test)\n",
    "acc_score = accuracy_score(y_test, results)\n",
    "print(f\"Accuracy Score: {acc_score:.3f}\")\n",
    "print(f\"Accuracy Percentage: {acc_score*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Calculations\n",
    "\n",
    "Neural Network Questions\n",
    "\n",
    "1. Output of the hidden node H, when feed-forwarding with sigmoid function (activation function)\n",
    "\n",
    "    $ H_{out} = g(y) = \\sigma(y) = \\frac{1}{1+e^{-y}} $\n",
    "    \n",
    "    $ H_{in} = (\\sum_{i=1}^n w_i x_i) + b$\n",
    "    \n",
    "    We know: $b=0$ and $[X_1, X_2] = [0.1, 0.2]$\n",
    "\n",
    "    $ \\rarr 0.1(1) + 0.2(2) + 0 = 0.5 \\rarr H_{in} = 0.5 $\n",
    "    \n",
    "    $ \\rarr g(0.5) = \\sigma(0.5) = \\frac{1}{1+e^{-0.5}} = 0.62246 $\n",
    "\n",
    "    $ H_{out} = 0.6225 $\n",
    "    \n",
    "<br>\n",
    "\n",
    "2. $\\Delta w_{o1}$ using backpropagation\n",
    "\n",
    "    We know: $[O_1, O_2] = [0, 1]$\n",
    "\n",
    "    $ \\frac{\\partial E_{O_1}}{\\partial w_{o1}} = (\\frac{\\partial E_{O_1}}{\\partial out_{O_1}})(\\frac{\\partial out_{O_1}}{\\partial O_1})(\\frac{\\partial O_1}{\\partial w_{o1}})$ \n",
    "\n",
    "    $ E_{O_1} = \\frac{1}{2} (target_{O_1} - out_{O_1})^2 \\rarr \\frac{\\partial E_{O_1}}{\\partial out_{O_1}} = -(target_{O_1} - out_{O_1}) $\n",
    "\n",
    "    $ out_{O_1} = \\sigma (O_1) \\rarr \\frac{\\partial}{\\partial O_1} \\sigma (O_1) = \\sigma (O_1) (1 - \\sigma (O_1)) $\n",
    "\n",
    "    $ O_1 = H_{out}w_{o1} + b \\rarr \\frac{\\partial O_1}{\\partial w_{o1}} = H_{out} $\n",
    "\n",
    "    $ \\rarr \\Delta w_{o1} = [(-(target_{O_1} - out_{O_1}))(\\sigma (O_1) (1 - \\sigma (O_1)))(H_{out})] $\n",
    "\n",
    "    $ \\rarr (-(0 - 0.7764))(0.7764(1 - 0.7764))(0.6225) = 0.0839 $\n",
    "\n",
    "    $ \\Delta w_{o1} = 0.0839 $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
