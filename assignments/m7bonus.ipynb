{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M7 Bonus \n",
    "#### Training a Convolutional Neural Net on the CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# configuring machine to utilize GPU with cuda\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar10/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar10/cifar-10-python.tar.gz to ./data/cifar10\n"
     ]
    }
   ],
   "source": [
    "# loading cifar-10 training and testing with transforming it\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "# loading cifar-10 training data\n",
    "cifar10_training_data = datasets.CIFAR10(\n",
    "    './data/cifar10', \n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# loading cifar-10 test data\n",
    "cifar10_testing_data = datasets.CIFAR10(\n",
    "    './data/cifar10', \n",
    "    train=False, \n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining cnn model\n",
    "\n",
    "class ConvNeuralNet(nn.Module):\n",
    "    \"\"\"    \n",
    "    Convolutional Neural Network for classifying CIFAR-10 dataset.\n",
    "\n",
    "    Attributes:\n",
    "        convolution_layer (nn.Sequential): A sequential container of convolutional layers\n",
    "            that includes convolution, batch normalization, ReLU activations, and pooling.\n",
    "        fullyconnected_layer (nn.Sequential): A sequential container of fully connected\n",
    "            layers with dropout for regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolution_layer = nn.Sequential(\n",
    "            # first block : (input: 3 channels, output: 32 channels)\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # second block : (input: 64 channels, output: 128 channels)\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.05),\n",
    "\n",
    "            # third block : (input: 128 channels, output: 256 channels)\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.fullyconnected_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(4096, 1024),  # (input: 4096 features, output: 1024 features)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),   # (input: 1024 features, output: 512 features)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 10)  # final layer (input: 512 features, output: 10 classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # convolution\n",
    "        x = self.convolution_layer(x)\n",
    "\n",
    "        # flatten view\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # fully connected layer\n",
    "        x = self.fullyconnected_layer(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# model = ConvNeuralNet()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def training(model, train_dataloader, optimizer, criterion, epoch, device, print_freq=10):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    for batch_idx, (data, labels) in enumerate(train_dataloader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # propagation\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print stats\n",
    "        training_loss += loss.item() * data.shape[0]\n",
    "\n",
    "\n",
    "        if not (batch_idx % print_freq):\n",
    "            print(f\"Batch: {batch_idx}/{len(train_dataloader)} | \", \n",
    "                  f\"Training Loss: {loss.item():.4f} | \"\n",
    "            )\n",
    "    return training_loss / len(train_dataloader.dataset)\n",
    "\n",
    "# testing function\n",
    "def testing(model, test_dataloader, criterion, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            # move data and target to the specified device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # forward pass \n",
    "            output = model(data)\n",
    "\n",
    "            # calculate loss\n",
    "            test_loss += criterion(output, target).item() * data.size(0)\n",
    "\n",
    "            # get predictions\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    # average loss\n",
    "    average_loss = test_loss / total\n",
    "    # calculate accuracy\n",
    "    accuracy = correct / total * 100\n",
    "\n",
    "    print(f'Test Loss: {average_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    return average_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Batch: 0/391 |  Training Loss: 2.3016 | \n",
      "Batch: 10/391 |  Training Loss: 2.2439 | \n",
      "Batch: 20/391 |  Training Loss: 1.9507 | \n",
      "Batch: 30/391 |  Training Loss: 1.7621 | \n",
      "Batch: 40/391 |  Training Loss: 1.9841 | \n",
      "Batch: 50/391 |  Training Loss: 1.8093 | \n",
      "Batch: 60/391 |  Training Loss: 1.7754 | \n",
      "Batch: 70/391 |  Training Loss: 1.6590 | \n",
      "Batch: 80/391 |  Training Loss: 1.5577 | \n",
      "Batch: 90/391 |  Training Loss: 1.6702 | \n",
      "Batch: 100/391 |  Training Loss: 1.6271 | \n",
      "Batch: 110/391 |  Training Loss: 1.6363 | \n",
      "Batch: 120/391 |  Training Loss: 1.5842 | \n",
      "Batch: 130/391 |  Training Loss: 1.4840 | \n",
      "Batch: 140/391 |  Training Loss: 1.5377 | \n",
      "Batch: 150/391 |  Training Loss: 1.3859 | \n",
      "Batch: 160/391 |  Training Loss: 1.6857 | \n",
      "Batch: 170/391 |  Training Loss: 1.3919 | \n",
      "Batch: 180/391 |  Training Loss: 1.6371 | \n",
      "Batch: 190/391 |  Training Loss: 1.2828 | \n",
      "Batch: 200/391 |  Training Loss: 1.4675 | \n",
      "Batch: 210/391 |  Training Loss: 1.2946 | \n",
      "Batch: 220/391 |  Training Loss: 1.4024 | \n",
      "Batch: 230/391 |  Training Loss: 1.4967 | \n",
      "Batch: 240/391 |  Training Loss: 1.5422 | \n",
      "Batch: 250/391 |  Training Loss: 1.2561 | \n",
      "Batch: 260/391 |  Training Loss: 1.2874 | \n",
      "Batch: 270/391 |  Training Loss: 1.3300 | \n",
      "Batch: 280/391 |  Training Loss: 1.4411 | \n",
      "Batch: 290/391 |  Training Loss: 1.2659 | \n",
      "Batch: 300/391 |  Training Loss: 1.1122 | \n",
      "Batch: 310/391 |  Training Loss: 1.2781 | \n",
      "Batch: 320/391 |  Training Loss: 1.1692 | \n",
      "Batch: 330/391 |  Training Loss: 1.2873 | \n",
      "Batch: 340/391 |  Training Loss: 1.1496 | \n",
      "Batch: 350/391 |  Training Loss: 1.3494 | \n",
      "Batch: 360/391 |  Training Loss: 1.2217 | \n",
      "Batch: 370/391 |  Training Loss: 1.2090 | \n",
      "Batch: 380/391 |  Training Loss: 1.1265 | \n",
      "Batch: 390/391 |  Training Loss: 0.9498 | \n",
      "Epoch: 2\n",
      "Batch: 0/391 |  Training Loss: 1.1568 | \n",
      "Batch: 10/391 |  Training Loss: 1.3380 | \n",
      "Batch: 20/391 |  Training Loss: 1.0114 | \n",
      "Batch: 30/391 |  Training Loss: 1.1374 | \n",
      "Batch: 40/391 |  Training Loss: 1.0732 | \n",
      "Batch: 50/391 |  Training Loss: 0.8794 | \n",
      "Batch: 60/391 |  Training Loss: 1.1634 | \n",
      "Batch: 70/391 |  Training Loss: 1.3107 | \n",
      "Batch: 80/391 |  Training Loss: 1.1849 | \n",
      "Batch: 90/391 |  Training Loss: 1.0779 | \n",
      "Batch: 100/391 |  Training Loss: 0.9152 | \n",
      "Batch: 110/391 |  Training Loss: 1.0087 | \n",
      "Batch: 120/391 |  Training Loss: 1.0694 | \n",
      "Batch: 130/391 |  Training Loss: 0.9175 | \n",
      "Batch: 140/391 |  Training Loss: 1.1220 | \n",
      "Batch: 150/391 |  Training Loss: 1.2748 | \n",
      "Batch: 160/391 |  Training Loss: 0.9856 | \n",
      "Batch: 170/391 |  Training Loss: 1.2021 | \n",
      "Batch: 180/391 |  Training Loss: 1.3062 | \n",
      "Batch: 190/391 |  Training Loss: 0.9704 | \n",
      "Batch: 200/391 |  Training Loss: 0.9654 | \n",
      "Batch: 210/391 |  Training Loss: 1.1063 | \n",
      "Batch: 220/391 |  Training Loss: 0.9703 | \n",
      "Batch: 230/391 |  Training Loss: 0.9959 | \n",
      "Batch: 240/391 |  Training Loss: 1.0210 | \n",
      "Batch: 250/391 |  Training Loss: 1.0605 | \n",
      "Batch: 260/391 |  Training Loss: 1.0333 | \n",
      "Batch: 270/391 |  Training Loss: 1.0151 | \n",
      "Batch: 280/391 |  Training Loss: 0.9393 | \n",
      "Batch: 290/391 |  Training Loss: 0.8965 | \n",
      "Batch: 300/391 |  Training Loss: 0.9508 | \n",
      "Batch: 310/391 |  Training Loss: 0.9536 | \n",
      "Batch: 320/391 |  Training Loss: 0.8470 | \n",
      "Batch: 330/391 |  Training Loss: 0.9245 | \n",
      "Batch: 340/391 |  Training Loss: 0.9759 | \n",
      "Batch: 350/391 |  Training Loss: 1.0754 | \n",
      "Batch: 360/391 |  Training Loss: 0.9423 | \n",
      "Batch: 370/391 |  Training Loss: 0.8501 | \n",
      "Batch: 380/391 |  Training Loss: 0.9550 | \n",
      "Batch: 390/391 |  Training Loss: 0.8654 | \n",
      "Epoch: 3\n",
      "Batch: 0/391 |  Training Loss: 0.9079 | \n",
      "Batch: 10/391 |  Training Loss: 0.8575 | \n",
      "Batch: 20/391 |  Training Loss: 0.7607 | \n",
      "Batch: 30/391 |  Training Loss: 1.0346 | \n",
      "Batch: 40/391 |  Training Loss: 0.8585 | \n",
      "Batch: 50/391 |  Training Loss: 0.9474 | \n",
      "Batch: 60/391 |  Training Loss: 0.8762 | \n",
      "Batch: 70/391 |  Training Loss: 0.8267 | \n",
      "Batch: 80/391 |  Training Loss: 0.8571 | \n",
      "Batch: 90/391 |  Training Loss: 0.6968 | \n",
      "Batch: 100/391 |  Training Loss: 0.7782 | \n",
      "Batch: 110/391 |  Training Loss: 0.7109 | \n",
      "Batch: 120/391 |  Training Loss: 0.9630 | \n",
      "Batch: 130/391 |  Training Loss: 0.7758 | \n",
      "Batch: 140/391 |  Training Loss: 0.8014 | \n",
      "Batch: 150/391 |  Training Loss: 0.8886 | \n",
      "Batch: 160/391 |  Training Loss: 0.7569 | \n",
      "Batch: 170/391 |  Training Loss: 0.7032 | \n",
      "Batch: 180/391 |  Training Loss: 0.7579 | \n",
      "Batch: 190/391 |  Training Loss: 0.6815 | \n",
      "Batch: 200/391 |  Training Loss: 1.0110 | \n",
      "Batch: 210/391 |  Training Loss: 0.8685 | \n",
      "Batch: 220/391 |  Training Loss: 0.7959 | \n",
      "Batch: 230/391 |  Training Loss: 0.8470 | \n",
      "Batch: 240/391 |  Training Loss: 0.8861 | \n",
      "Batch: 250/391 |  Training Loss: 0.7774 | \n",
      "Batch: 260/391 |  Training Loss: 0.8519 | \n",
      "Batch: 270/391 |  Training Loss: 0.7315 | \n",
      "Batch: 280/391 |  Training Loss: 0.8877 | \n",
      "Batch: 290/391 |  Training Loss: 0.8375 | \n",
      "Batch: 300/391 |  Training Loss: 0.6863 | \n",
      "Batch: 310/391 |  Training Loss: 0.7345 | \n",
      "Batch: 320/391 |  Training Loss: 0.8640 | \n",
      "Batch: 330/391 |  Training Loss: 0.8496 | \n",
      "Batch: 340/391 |  Training Loss: 0.8328 | \n",
      "Batch: 350/391 |  Training Loss: 0.9061 | \n",
      "Batch: 360/391 |  Training Loss: 0.6891 | \n",
      "Batch: 370/391 |  Training Loss: 0.6518 | \n",
      "Batch: 380/391 |  Training Loss: 0.9821 | \n",
      "Batch: 390/391 |  Training Loss: 0.7636 | \n",
      "Epoch: 4\n",
      "Batch: 0/391 |  Training Loss: 0.5494 | \n",
      "Batch: 10/391 |  Training Loss: 0.6646 | \n",
      "Batch: 20/391 |  Training Loss: 0.8103 | \n",
      "Batch: 30/391 |  Training Loss: 0.6455 | \n",
      "Batch: 40/391 |  Training Loss: 0.8085 | \n",
      "Batch: 50/391 |  Training Loss: 0.6476 | \n",
      "Batch: 60/391 |  Training Loss: 0.6606 | \n",
      "Batch: 70/391 |  Training Loss: 0.6787 | \n",
      "Batch: 80/391 |  Training Loss: 0.7011 | \n",
      "Batch: 90/391 |  Training Loss: 0.8871 | \n",
      "Batch: 100/391 |  Training Loss: 0.7791 | \n",
      "Batch: 110/391 |  Training Loss: 0.8574 | \n",
      "Batch: 120/391 |  Training Loss: 0.7244 | \n",
      "Batch: 130/391 |  Training Loss: 0.7265 | \n",
      "Batch: 140/391 |  Training Loss: 0.8124 | \n",
      "Batch: 150/391 |  Training Loss: 0.6568 | \n",
      "Batch: 160/391 |  Training Loss: 0.5961 | \n",
      "Batch: 170/391 |  Training Loss: 0.5905 | \n",
      "Batch: 180/391 |  Training Loss: 0.6877 | \n",
      "Batch: 190/391 |  Training Loss: 0.7754 | \n",
      "Batch: 200/391 |  Training Loss: 0.8403 | \n",
      "Batch: 210/391 |  Training Loss: 0.7535 | \n",
      "Batch: 220/391 |  Training Loss: 0.7588 | \n",
      "Batch: 230/391 |  Training Loss: 0.7401 | \n",
      "Batch: 240/391 |  Training Loss: 0.7225 | \n",
      "Batch: 250/391 |  Training Loss: 0.6635 | \n",
      "Batch: 260/391 |  Training Loss: 0.6839 | \n",
      "Batch: 270/391 |  Training Loss: 0.6282 | \n",
      "Batch: 280/391 |  Training Loss: 0.7010 | \n",
      "Batch: 290/391 |  Training Loss: 0.6479 | \n",
      "Batch: 300/391 |  Training Loss: 0.7031 | \n",
      "Batch: 310/391 |  Training Loss: 0.6068 | \n",
      "Batch: 320/391 |  Training Loss: 0.9209 | \n",
      "Batch: 330/391 |  Training Loss: 0.5741 | \n",
      "Batch: 340/391 |  Training Loss: 0.5093 | \n",
      "Batch: 350/391 |  Training Loss: 0.4824 | \n",
      "Batch: 360/391 |  Training Loss: 0.6030 | \n",
      "Batch: 370/391 |  Training Loss: 0.5486 | \n",
      "Batch: 380/391 |  Training Loss: 0.7876 | \n",
      "Batch: 390/391 |  Training Loss: 0.6800 | \n",
      "Epoch: 5\n",
      "Batch: 0/391 |  Training Loss: 0.4815 | \n",
      "Batch: 10/391 |  Training Loss: 0.4893 | \n",
      "Batch: 20/391 |  Training Loss: 0.5197 | \n",
      "Batch: 30/391 |  Training Loss: 0.7018 | \n",
      "Batch: 40/391 |  Training Loss: 0.7225 | \n",
      "Batch: 50/391 |  Training Loss: 0.5912 | \n",
      "Batch: 60/391 |  Training Loss: 0.6514 | \n",
      "Batch: 70/391 |  Training Loss: 0.4981 | \n",
      "Batch: 80/391 |  Training Loss: 0.6882 | \n",
      "Batch: 90/391 |  Training Loss: 0.4805 | \n",
      "Batch: 100/391 |  Training Loss: 0.5919 | \n",
      "Batch: 110/391 |  Training Loss: 0.5305 | \n",
      "Batch: 120/391 |  Training Loss: 0.6493 | \n",
      "Batch: 130/391 |  Training Loss: 0.7057 | \n",
      "Batch: 140/391 |  Training Loss: 0.5002 | \n",
      "Batch: 150/391 |  Training Loss: 0.6661 | \n",
      "Batch: 160/391 |  Training Loss: 0.6113 | \n",
      "Batch: 170/391 |  Training Loss: 0.6029 | \n",
      "Batch: 180/391 |  Training Loss: 0.5956 | \n",
      "Batch: 190/391 |  Training Loss: 0.5647 | \n",
      "Batch: 200/391 |  Training Loss: 0.5992 | \n",
      "Batch: 210/391 |  Training Loss: 0.6082 | \n",
      "Batch: 220/391 |  Training Loss: 0.7716 | \n",
      "Batch: 230/391 |  Training Loss: 0.5508 | \n",
      "Batch: 240/391 |  Training Loss: 0.5768 | \n",
      "Batch: 250/391 |  Training Loss: 0.6777 | \n",
      "Batch: 260/391 |  Training Loss: 0.4331 | \n",
      "Batch: 270/391 |  Training Loss: 0.7789 | \n",
      "Batch: 280/391 |  Training Loss: 0.6564 | \n",
      "Batch: 290/391 |  Training Loss: 0.6425 | \n",
      "Batch: 300/391 |  Training Loss: 0.6910 | \n",
      "Batch: 310/391 |  Training Loss: 0.5665 | \n",
      "Batch: 320/391 |  Training Loss: 0.5302 | \n",
      "Batch: 330/391 |  Training Loss: 0.5908 | \n",
      "Batch: 340/391 |  Training Loss: 0.5715 | \n",
      "Batch: 350/391 |  Training Loss: 0.6365 | \n",
      "Batch: 360/391 |  Training Loss: 0.5809 | \n",
      "Batch: 370/391 |  Training Loss: 0.6509 | \n",
      "Batch: 380/391 |  Training Loss: 0.5600 | \n",
      "Batch: 390/391 |  Training Loss: 0.9280 | \n",
      "Epoch: 6\n",
      "Batch: 0/391 |  Training Loss: 0.7204 | \n",
      "Batch: 10/391 |  Training Loss: 0.5493 | \n",
      "Batch: 20/391 |  Training Loss: 0.7104 | \n",
      "Batch: 30/391 |  Training Loss: 0.5871 | \n",
      "Batch: 40/391 |  Training Loss: 0.5925 | \n",
      "Batch: 50/391 |  Training Loss: 0.3367 | \n",
      "Batch: 60/391 |  Training Loss: 0.3728 | \n",
      "Batch: 70/391 |  Training Loss: 0.5477 | \n",
      "Batch: 80/391 |  Training Loss: 0.5847 | \n",
      "Batch: 90/391 |  Training Loss: 0.5373 | \n",
      "Batch: 100/391 |  Training Loss: 0.5022 | \n",
      "Batch: 110/391 |  Training Loss: 0.4949 | \n",
      "Batch: 120/391 |  Training Loss: 0.5494 | \n",
      "Batch: 130/391 |  Training Loss: 0.5770 | \n",
      "Batch: 140/391 |  Training Loss: 0.5924 | \n",
      "Batch: 150/391 |  Training Loss: 0.3303 | \n",
      "Batch: 160/391 |  Training Loss: 0.4530 | \n",
      "Batch: 170/391 |  Training Loss: 0.4950 | \n",
      "Batch: 180/391 |  Training Loss: 0.5864 | \n",
      "Batch: 190/391 |  Training Loss: 0.5846 | \n",
      "Batch: 200/391 |  Training Loss: 0.6167 | \n",
      "Batch: 210/391 |  Training Loss: 0.5127 | \n",
      "Batch: 220/391 |  Training Loss: 0.5039 | \n",
      "Batch: 230/391 |  Training Loss: 0.5618 | \n",
      "Batch: 240/391 |  Training Loss: 0.3302 | \n",
      "Batch: 250/391 |  Training Loss: 0.5108 | \n",
      "Batch: 260/391 |  Training Loss: 0.4852 | \n",
      "Batch: 270/391 |  Training Loss: 0.5515 | \n",
      "Batch: 280/391 |  Training Loss: 0.5195 | \n",
      "Batch: 290/391 |  Training Loss: 0.5728 | \n",
      "Batch: 300/391 |  Training Loss: 0.4911 | \n",
      "Batch: 310/391 |  Training Loss: 0.6424 | \n",
      "Batch: 320/391 |  Training Loss: 0.5675 | \n",
      "Batch: 330/391 |  Training Loss: 0.6417 | \n",
      "Batch: 340/391 |  Training Loss: 0.5051 | \n",
      "Batch: 350/391 |  Training Loss: 0.5348 | \n",
      "Batch: 360/391 |  Training Loss: 0.4823 | \n",
      "Batch: 370/391 |  Training Loss: 0.6201 | \n",
      "Batch: 380/391 |  Training Loss: 0.6017 | \n",
      "Batch: 390/391 |  Training Loss: 0.4984 | \n",
      "Epoch: 7\n",
      "Batch: 0/391 |  Training Loss: 0.4944 | \n",
      "Batch: 10/391 |  Training Loss: 0.4049 | \n",
      "Batch: 20/391 |  Training Loss: 0.3964 | \n",
      "Batch: 30/391 |  Training Loss: 0.4237 | \n",
      "Batch: 40/391 |  Training Loss: 0.4509 | \n",
      "Batch: 50/391 |  Training Loss: 0.3675 | \n",
      "Batch: 60/391 |  Training Loss: 0.4153 | \n",
      "Batch: 70/391 |  Training Loss: 0.5952 | \n",
      "Batch: 80/391 |  Training Loss: 0.6636 | \n",
      "Batch: 90/391 |  Training Loss: 0.5384 | \n",
      "Batch: 100/391 |  Training Loss: 0.4498 | \n",
      "Batch: 110/391 |  Training Loss: 0.5594 | \n",
      "Batch: 120/391 |  Training Loss: 0.5265 | \n",
      "Batch: 130/391 |  Training Loss: 0.3381 | \n",
      "Batch: 140/391 |  Training Loss: 0.3619 | \n",
      "Batch: 150/391 |  Training Loss: 0.5563 | \n",
      "Batch: 160/391 |  Training Loss: 0.4162 | \n",
      "Batch: 170/391 |  Training Loss: 0.3789 | \n",
      "Batch: 180/391 |  Training Loss: 0.4500 | \n",
      "Batch: 190/391 |  Training Loss: 0.4273 | \n",
      "Batch: 200/391 |  Training Loss: 0.4743 | \n",
      "Batch: 210/391 |  Training Loss: 0.3955 | \n",
      "Batch: 220/391 |  Training Loss: 0.4449 | \n",
      "Batch: 230/391 |  Training Loss: 0.4585 | \n",
      "Batch: 240/391 |  Training Loss: 0.4925 | \n",
      "Batch: 250/391 |  Training Loss: 0.3115 | \n",
      "Batch: 260/391 |  Training Loss: 0.5054 | \n",
      "Batch: 270/391 |  Training Loss: 0.3586 | \n",
      "Batch: 280/391 |  Training Loss: 0.5127 | \n",
      "Batch: 290/391 |  Training Loss: 0.5922 | \n",
      "Batch: 300/391 |  Training Loss: 0.4542 | \n",
      "Batch: 310/391 |  Training Loss: 0.3345 | \n",
      "Batch: 320/391 |  Training Loss: 0.5573 | \n",
      "Batch: 330/391 |  Training Loss: 0.6811 | \n",
      "Batch: 340/391 |  Training Loss: 0.4611 | \n",
      "Batch: 350/391 |  Training Loss: 0.4830 | \n",
      "Batch: 360/391 |  Training Loss: 0.5643 | \n",
      "Batch: 370/391 |  Training Loss: 0.4377 | \n",
      "Batch: 380/391 |  Training Loss: 0.5888 | \n",
      "Batch: 390/391 |  Training Loss: 0.5073 | \n",
      "Epoch: 8\n",
      "Batch: 0/391 |  Training Loss: 0.4587 | \n",
      "Batch: 10/391 |  Training Loss: 0.3782 | \n",
      "Batch: 20/391 |  Training Loss: 0.4639 | \n",
      "Batch: 30/391 |  Training Loss: 0.3473 | \n",
      "Batch: 40/391 |  Training Loss: 0.3309 | \n",
      "Batch: 50/391 |  Training Loss: 0.3580 | \n",
      "Batch: 60/391 |  Training Loss: 0.5246 | \n",
      "Batch: 70/391 |  Training Loss: 0.3721 | \n",
      "Batch: 80/391 |  Training Loss: 0.3681 | \n",
      "Batch: 90/391 |  Training Loss: 0.4344 | \n",
      "Batch: 100/391 |  Training Loss: 0.4247 | \n",
      "Batch: 110/391 |  Training Loss: 0.4146 | \n",
      "Batch: 120/391 |  Training Loss: 0.5598 | \n",
      "Batch: 130/391 |  Training Loss: 0.4471 | \n",
      "Batch: 140/391 |  Training Loss: 0.4039 | \n",
      "Batch: 150/391 |  Training Loss: 0.4227 | \n",
      "Batch: 160/391 |  Training Loss: 0.4448 | \n",
      "Batch: 170/391 |  Training Loss: 0.4981 | \n",
      "Batch: 180/391 |  Training Loss: 0.4457 | \n",
      "Batch: 190/391 |  Training Loss: 0.3952 | \n",
      "Batch: 200/391 |  Training Loss: 0.4301 | \n",
      "Batch: 210/391 |  Training Loss: 0.4826 | \n",
      "Batch: 220/391 |  Training Loss: 0.4062 | \n",
      "Batch: 230/391 |  Training Loss: 0.4507 | \n",
      "Batch: 240/391 |  Training Loss: 0.2755 | \n",
      "Batch: 250/391 |  Training Loss: 0.4512 | \n",
      "Batch: 260/391 |  Training Loss: 0.4691 | \n",
      "Batch: 270/391 |  Training Loss: 0.4794 | \n",
      "Batch: 280/391 |  Training Loss: 0.4582 | \n",
      "Batch: 290/391 |  Training Loss: 0.4284 | \n",
      "Batch: 300/391 |  Training Loss: 0.4504 | \n",
      "Batch: 310/391 |  Training Loss: 0.4785 | \n",
      "Batch: 320/391 |  Training Loss: 0.4056 | \n",
      "Batch: 330/391 |  Training Loss: 0.4258 | \n",
      "Batch: 340/391 |  Training Loss: 0.4249 | \n",
      "Batch: 350/391 |  Training Loss: 0.4909 | \n",
      "Batch: 360/391 |  Training Loss: 0.4316 | \n",
      "Batch: 370/391 |  Training Loss: 0.5662 | \n",
      "Batch: 380/391 |  Training Loss: 0.4132 | \n",
      "Batch: 390/391 |  Training Loss: 0.3523 | \n",
      "Epoch: 9\n",
      "Batch: 0/391 |  Training Loss: 0.3201 | \n",
      "Batch: 10/391 |  Training Loss: 0.4333 | \n",
      "Batch: 20/391 |  Training Loss: 0.2469 | \n",
      "Batch: 30/391 |  Training Loss: 0.3393 | \n",
      "Batch: 40/391 |  Training Loss: 0.4451 | \n",
      "Batch: 50/391 |  Training Loss: 0.3166 | \n",
      "Batch: 60/391 |  Training Loss: 0.3227 | \n",
      "Batch: 70/391 |  Training Loss: 0.3737 | \n",
      "Batch: 80/391 |  Training Loss: 0.3756 | \n",
      "Batch: 90/391 |  Training Loss: 0.3597 | \n",
      "Batch: 100/391 |  Training Loss: 0.4012 | \n",
      "Batch: 110/391 |  Training Loss: 0.2570 | \n",
      "Batch: 120/391 |  Training Loss: 0.5177 | \n",
      "Batch: 130/391 |  Training Loss: 0.3273 | \n",
      "Batch: 140/391 |  Training Loss: 0.3084 | \n",
      "Batch: 150/391 |  Training Loss: 0.3343 | \n",
      "Batch: 160/391 |  Training Loss: 0.2630 | \n",
      "Batch: 170/391 |  Training Loss: 0.3035 | \n",
      "Batch: 180/391 |  Training Loss: 0.4793 | \n",
      "Batch: 190/391 |  Training Loss: 0.4568 | \n",
      "Batch: 200/391 |  Training Loss: 0.5451 | \n",
      "Batch: 210/391 |  Training Loss: 0.3505 | \n",
      "Batch: 220/391 |  Training Loss: 0.3357 | \n",
      "Batch: 230/391 |  Training Loss: 0.4307 | \n",
      "Batch: 240/391 |  Training Loss: 0.3749 | \n",
      "Batch: 250/391 |  Training Loss: 0.3679 | \n",
      "Batch: 260/391 |  Training Loss: 0.3252 | \n",
      "Batch: 270/391 |  Training Loss: 0.3499 | \n",
      "Batch: 280/391 |  Training Loss: 0.3049 | \n",
      "Batch: 290/391 |  Training Loss: 0.4306 | \n",
      "Batch: 300/391 |  Training Loss: 0.3883 | \n",
      "Batch: 310/391 |  Training Loss: 0.4678 | \n",
      "Batch: 320/391 |  Training Loss: 0.4267 | \n",
      "Batch: 330/391 |  Training Loss: 0.3490 | \n",
      "Batch: 340/391 |  Training Loss: 0.4269 | \n",
      "Batch: 350/391 |  Training Loss: 0.3511 | \n",
      "Batch: 360/391 |  Training Loss: 0.2951 | \n",
      "Batch: 370/391 |  Training Loss: 0.5113 | \n",
      "Batch: 380/391 |  Training Loss: 0.4208 | \n",
      "Batch: 390/391 |  Training Loss: 0.3722 | \n",
      "Epoch: 10\n",
      "Batch: 0/391 |  Training Loss: 0.3626 | \n",
      "Batch: 10/391 |  Training Loss: 0.3435 | \n",
      "Batch: 20/391 |  Training Loss: 0.2447 | \n",
      "Batch: 30/391 |  Training Loss: 0.2130 | \n",
      "Batch: 40/391 |  Training Loss: 0.3030 | \n",
      "Batch: 50/391 |  Training Loss: 0.2625 | \n",
      "Batch: 60/391 |  Training Loss: 0.4295 | \n",
      "Batch: 70/391 |  Training Loss: 0.2886 | \n",
      "Batch: 80/391 |  Training Loss: 0.2277 | \n",
      "Batch: 90/391 |  Training Loss: 0.3594 | \n",
      "Batch: 100/391 |  Training Loss: 0.3226 | \n",
      "Batch: 110/391 |  Training Loss: 0.3177 | \n",
      "Batch: 120/391 |  Training Loss: 0.2825 | \n",
      "Batch: 130/391 |  Training Loss: 0.3772 | \n",
      "Batch: 140/391 |  Training Loss: 0.3001 | \n",
      "Batch: 150/391 |  Training Loss: 0.3758 | \n",
      "Batch: 160/391 |  Training Loss: 0.3902 | \n",
      "Batch: 170/391 |  Training Loss: 0.2754 | \n",
      "Batch: 180/391 |  Training Loss: 0.4471 | \n",
      "Batch: 190/391 |  Training Loss: 0.3105 | \n",
      "Batch: 200/391 |  Training Loss: 0.2531 | \n",
      "Batch: 210/391 |  Training Loss: 0.2633 | \n",
      "Batch: 220/391 |  Training Loss: 0.2579 | \n",
      "Batch: 230/391 |  Training Loss: 0.4385 | \n",
      "Batch: 240/391 |  Training Loss: 0.3390 | \n",
      "Batch: 250/391 |  Training Loss: 0.3241 | \n",
      "Batch: 260/391 |  Training Loss: 0.2912 | \n",
      "Batch: 270/391 |  Training Loss: 0.4520 | \n",
      "Batch: 280/391 |  Training Loss: 0.3796 | \n",
      "Batch: 290/391 |  Training Loss: 0.3778 | \n",
      "Batch: 300/391 |  Training Loss: 0.4143 | \n",
      "Batch: 310/391 |  Training Loss: 0.3802 | \n",
      "Batch: 320/391 |  Training Loss: 0.3682 | \n",
      "Batch: 330/391 |  Training Loss: 0.4178 | \n",
      "Batch: 340/391 |  Training Loss: 0.4124 | \n",
      "Batch: 350/391 |  Training Loss: 0.3239 | \n",
      "Batch: 360/391 |  Training Loss: 0.3411 | \n",
      "Batch: 370/391 |  Training Loss: 0.2794 | \n",
      "Batch: 380/391 |  Training Loss: 0.3080 | \n",
      "Batch: 390/391 |  Training Loss: 0.2320 | \n",
      "Test Loss: 0.5795, Accuracy: 81.55%\n",
      "Final Training Loss: 0.3493, Final Test Loss: 0.5795, Test Accuracy: 81.55%\n"
     ]
    }
   ],
   "source": [
    "def train_test(model, train_dataloader, test_dataloader, optimizer, criterion, num_epochs, device):\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # train the model\n",
    "        train_loss = training(model, train_dataloader, optimizer, criterion, epoch, device)\n",
    "\n",
    "    # test the model\n",
    "    test_loss, test_accuracy = testing(model, test_dataloader, criterion, device)\n",
    "\n",
    "    return train_loss, test_loss, test_accuracy\n",
    "\n",
    "\n",
    "# init params\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "\n",
    "# init model\n",
    "model = ConvNeuralNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# load cifar-10 dataloaders\n",
    "\n",
    "# trainloader\n",
    "cifar_trainloader = DataLoader(\n",
    "    cifar10_training_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# testloader\n",
    "cifar_testloader = DataLoader(\n",
    "    cifar10_testing_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# init model, optimizer, criterion\n",
    "model = ConvNeuralNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# call train_test\n",
    "train_loss, test_loss, test_accuracy = train_test(\n",
    "    model, \n",
    "    cifar_trainloader, \n",
    "    cifar_testloader, \n",
    "    optimizer, \n",
    "    criterion, \n",
    "    epochs, \n",
    "    device\n",
    ")\n",
    "\n",
    "print(f\"Final Training Loss: {train_loss:.4f}, Final Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without re-implementing ResNet architectures and data augmentations:\n",
    "\n",
    "Run 1 Results:<br>\n",
    "(batch size = 64)<br>\n",
    "Final Training Loss: 0.2981, Final Test Loss: 0.6369, Test Accuracy: 80.62%\n",
    "\n",
    "Run 2 Results:<br>\n",
    "(batch size = 64)<br>\n",
    "Final Training Loss: 0.3409, Final Test Loss: 0.5919, Test Accuracy: 81.60%\n",
    "\n",
    "Run 3 Results:<br>\n",
    "(batch size = 128)<br>\n",
    "Final Training Loss: 0.2351, Final Test Loss: 0.6708, Test Accuracy: 81.02%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
