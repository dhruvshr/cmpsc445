{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M7 Bonus \n",
    "#### Training a Convolutional Neural Net on the CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# configuring machine to utilize GPU with cuda\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# loading cifar-10 training and testing with transforming it\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "# loading cifar-10 training data\n",
    "cifar10_training_data = datasets.CIFAR10(\n",
    "    './data/cifar10', \n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# loading cifar-10 test data\n",
    "cifar10_testing_data = datasets.CIFAR10(\n",
    "    './data/cifar10', \n",
    "    train=False, \n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining cnn model\n",
    "\n",
    "class ConvNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolution_layer = nn.Sequential(\n",
    "            # first block\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # second block\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.05),\n",
    "\n",
    "            # third block\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.fullyconnected_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # convolution\n",
    "        x = self.convolution_layer(x)\n",
    "\n",
    "        # flatten view\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # fully connected layer\n",
    "        x = self.fullyconnected_layer(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# model = ConvNeuralNet()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def training(model, train_dataloader, optimizer, criterion, epoch, device, print_freq=10):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    for batch_idx, (data, labels) in enumerate(train_dataloader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # propagation\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print stats\n",
    "        training_loss += loss.item() * data.shape[0]\n",
    "\n",
    "\n",
    "        if not (batch_idx % print_freq):\n",
    "            print(f\"Batch: {batch_idx}/{len(train_dataloader)} | \", \n",
    "                  f\"Training Loss: {loss.item():.4f} | \"\n",
    "            )\n",
    "    return training_loss / len(train_dataloader.dataset)\n",
    "\n",
    "# testing function\n",
    "def testing(model, test_dataloader, criterion, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            # move data and target to the specified device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # forward pass \n",
    "            output = model(data)\n",
    "\n",
    "            # calculate loss\n",
    "            test_loss += criterion(output, target).item() * data.size(0)\n",
    "\n",
    "            # get predictions\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    # average loss\n",
    "    average_loss = test_loss / total\n",
    "    # calculate accuracy\n",
    "    accuracy = correct / total * 100\n",
    "\n",
    "    print(f'Test Loss: {average_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    return average_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Batch: 0/782 |  Training Loss: 2.2971 | \n",
      "Batch: 10/782 |  Training Loss: 2.2938 | \n",
      "Batch: 20/782 |  Training Loss: 2.1153 | \n",
      "Batch: 30/782 |  Training Loss: 1.7905 | \n",
      "Batch: 40/782 |  Training Loss: 2.0506 | \n",
      "Batch: 50/782 |  Training Loss: 1.9280 | \n",
      "Batch: 60/782 |  Training Loss: 1.7751 | \n",
      "Batch: 70/782 |  Training Loss: 1.8771 | \n",
      "Batch: 80/782 |  Training Loss: 1.9234 | \n",
      "Batch: 90/782 |  Training Loss: 1.8266 | \n",
      "Batch: 100/782 |  Training Loss: 1.6117 | \n",
      "Batch: 110/782 |  Training Loss: 1.4725 | \n",
      "Batch: 120/782 |  Training Loss: 1.5832 | \n",
      "Batch: 130/782 |  Training Loss: 1.6139 | \n",
      "Batch: 140/782 |  Training Loss: 1.7820 | \n",
      "Batch: 150/782 |  Training Loss: 1.6205 | \n",
      "Batch: 160/782 |  Training Loss: 1.5618 | \n",
      "Batch: 170/782 |  Training Loss: 1.5914 | \n",
      "Batch: 180/782 |  Training Loss: 1.6321 | \n",
      "Batch: 190/782 |  Training Loss: 1.6264 | \n",
      "Batch: 200/782 |  Training Loss: 1.4951 | \n",
      "Batch: 210/782 |  Training Loss: 1.4787 | \n",
      "Batch: 220/782 |  Training Loss: 1.4724 | \n",
      "Batch: 230/782 |  Training Loss: 1.4735 | \n",
      "Batch: 240/782 |  Training Loss: 1.6727 | \n",
      "Batch: 250/782 |  Training Loss: 1.2402 | \n",
      "Batch: 260/782 |  Training Loss: 1.4791 | \n",
      "Batch: 270/782 |  Training Loss: 1.3396 | \n",
      "Batch: 280/782 |  Training Loss: 1.3088 | \n",
      "Batch: 290/782 |  Training Loss: 1.2870 | \n",
      "Batch: 300/782 |  Training Loss: 1.4912 | \n",
      "Batch: 310/782 |  Training Loss: 1.4701 | \n",
      "Batch: 320/782 |  Training Loss: 1.5556 | \n",
      "Batch: 330/782 |  Training Loss: 1.2640 | \n",
      "Batch: 340/782 |  Training Loss: 1.3126 | \n",
      "Batch: 350/782 |  Training Loss: 1.2146 | \n",
      "Batch: 360/782 |  Training Loss: 1.4830 | \n",
      "Batch: 370/782 |  Training Loss: 1.3769 | \n",
      "Batch: 380/782 |  Training Loss: 1.3044 | \n",
      "Batch: 390/782 |  Training Loss: 1.5104 | \n",
      "Batch: 400/782 |  Training Loss: 1.2461 | \n",
      "Batch: 410/782 |  Training Loss: 1.2637 | \n",
      "Batch: 420/782 |  Training Loss: 1.5317 | \n",
      "Batch: 430/782 |  Training Loss: 1.3382 | \n",
      "Batch: 440/782 |  Training Loss: 1.2907 | \n",
      "Batch: 450/782 |  Training Loss: 1.1439 | \n",
      "Batch: 460/782 |  Training Loss: 1.0622 | \n",
      "Batch: 470/782 |  Training Loss: 1.3606 | \n",
      "Batch: 480/782 |  Training Loss: 1.3251 | \n",
      "Batch: 490/782 |  Training Loss: 1.0448 | \n",
      "Batch: 500/782 |  Training Loss: 1.2707 | \n",
      "Batch: 510/782 |  Training Loss: 1.3769 | \n",
      "Batch: 520/782 |  Training Loss: 1.2623 | \n",
      "Batch: 530/782 |  Training Loss: 1.0903 | \n",
      "Batch: 540/782 |  Training Loss: 1.1839 | \n",
      "Batch: 550/782 |  Training Loss: 1.2587 | \n",
      "Batch: 560/782 |  Training Loss: 1.3107 | \n",
      "Batch: 570/782 |  Training Loss: 1.0921 | \n",
      "Batch: 580/782 |  Training Loss: 1.4370 | \n",
      "Batch: 590/782 |  Training Loss: 0.9874 | \n",
      "Batch: 600/782 |  Training Loss: 1.2282 | \n",
      "Batch: 610/782 |  Training Loss: 1.2259 | \n",
      "Batch: 620/782 |  Training Loss: 1.0136 | \n",
      "Batch: 630/782 |  Training Loss: 1.1064 | \n",
      "Batch: 640/782 |  Training Loss: 1.0410 | \n",
      "Batch: 650/782 |  Training Loss: 1.2670 | \n",
      "Batch: 660/782 |  Training Loss: 1.2129 | \n",
      "Batch: 670/782 |  Training Loss: 0.9128 | \n",
      "Batch: 680/782 |  Training Loss: 1.3501 | \n",
      "Batch: 690/782 |  Training Loss: 1.1770 | \n",
      "Batch: 700/782 |  Training Loss: 1.1887 | \n",
      "Batch: 710/782 |  Training Loss: 0.9523 | \n",
      "Batch: 720/782 |  Training Loss: 1.1226 | \n",
      "Batch: 730/782 |  Training Loss: 1.1020 | \n",
      "Batch: 740/782 |  Training Loss: 1.2200 | \n",
      "Batch: 750/782 |  Training Loss: 1.3238 | \n",
      "Batch: 760/782 |  Training Loss: 1.1728 | \n",
      "Batch: 770/782 |  Training Loss: 1.0569 | \n",
      "Batch: 780/782 |  Training Loss: 1.1452 | \n",
      "Epoch: 2\n",
      "Batch: 0/782 |  Training Loss: 1.0364 | \n",
      "Batch: 10/782 |  Training Loss: 1.0977 | \n",
      "Batch: 20/782 |  Training Loss: 1.0877 | \n",
      "Batch: 30/782 |  Training Loss: 1.0635 | \n",
      "Batch: 40/782 |  Training Loss: 1.3415 | \n",
      "Batch: 50/782 |  Training Loss: 0.9782 | \n",
      "Batch: 60/782 |  Training Loss: 0.8037 | \n",
      "Batch: 70/782 |  Training Loss: 1.0859 | \n",
      "Batch: 80/782 |  Training Loss: 1.0528 | \n",
      "Batch: 90/782 |  Training Loss: 0.8439 | \n",
      "Batch: 100/782 |  Training Loss: 0.9718 | \n",
      "Batch: 110/782 |  Training Loss: 1.0667 | \n",
      "Batch: 120/782 |  Training Loss: 1.2678 | \n",
      "Batch: 130/782 |  Training Loss: 1.1059 | \n",
      "Batch: 140/782 |  Training Loss: 1.2273 | \n",
      "Batch: 150/782 |  Training Loss: 1.0675 | \n",
      "Batch: 160/782 |  Training Loss: 0.6836 | \n",
      "Batch: 170/782 |  Training Loss: 1.0108 | \n",
      "Batch: 180/782 |  Training Loss: 0.8255 | \n",
      "Batch: 190/782 |  Training Loss: 1.1779 | \n",
      "Batch: 200/782 |  Training Loss: 1.0172 | \n",
      "Batch: 210/782 |  Training Loss: 0.9036 | \n",
      "Batch: 220/782 |  Training Loss: 1.0407 | \n",
      "Batch: 230/782 |  Training Loss: 1.3226 | \n",
      "Batch: 240/782 |  Training Loss: 0.7088 | \n",
      "Batch: 250/782 |  Training Loss: 1.0397 | \n",
      "Batch: 260/782 |  Training Loss: 0.8725 | \n",
      "Batch: 270/782 |  Training Loss: 1.0386 | \n",
      "Batch: 280/782 |  Training Loss: 1.1360 | \n",
      "Batch: 290/782 |  Training Loss: 1.0026 | \n",
      "Batch: 300/782 |  Training Loss: 1.0744 | \n",
      "Batch: 310/782 |  Training Loss: 0.6688 | \n",
      "Batch: 320/782 |  Training Loss: 0.7798 | \n",
      "Batch: 330/782 |  Training Loss: 0.9039 | \n",
      "Batch: 340/782 |  Training Loss: 1.0205 | \n",
      "Batch: 350/782 |  Training Loss: 0.9072 | \n",
      "Batch: 360/782 |  Training Loss: 0.9520 | \n",
      "Batch: 370/782 |  Training Loss: 1.0867 | \n",
      "Batch: 380/782 |  Training Loss: 0.7521 | \n",
      "Batch: 390/782 |  Training Loss: 1.0462 | \n",
      "Batch: 400/782 |  Training Loss: 0.9926 | \n",
      "Batch: 410/782 |  Training Loss: 0.9654 | \n",
      "Batch: 420/782 |  Training Loss: 0.9470 | \n",
      "Batch: 430/782 |  Training Loss: 0.8583 | \n",
      "Batch: 440/782 |  Training Loss: 1.1253 | \n",
      "Batch: 450/782 |  Training Loss: 0.8278 | \n",
      "Batch: 460/782 |  Training Loss: 0.7116 | \n",
      "Batch: 470/782 |  Training Loss: 1.1074 | \n",
      "Batch: 480/782 |  Training Loss: 0.7325 | \n",
      "Batch: 490/782 |  Training Loss: 0.8789 | \n",
      "Batch: 500/782 |  Training Loss: 0.7075 | \n",
      "Batch: 510/782 |  Training Loss: 0.9511 | \n",
      "Batch: 520/782 |  Training Loss: 0.6886 | \n",
      "Batch: 530/782 |  Training Loss: 0.9227 | \n",
      "Batch: 540/782 |  Training Loss: 1.0228 | \n",
      "Batch: 550/782 |  Training Loss: 0.8605 | \n",
      "Batch: 560/782 |  Training Loss: 0.7254 | \n",
      "Batch: 570/782 |  Training Loss: 0.9507 | \n",
      "Batch: 580/782 |  Training Loss: 1.0333 | \n",
      "Batch: 590/782 |  Training Loss: 0.7842 | \n",
      "Batch: 600/782 |  Training Loss: 0.8572 | \n",
      "Batch: 610/782 |  Training Loss: 1.0396 | \n",
      "Batch: 620/782 |  Training Loss: 0.8013 | \n",
      "Batch: 630/782 |  Training Loss: 0.9726 | \n",
      "Batch: 640/782 |  Training Loss: 0.9560 | \n",
      "Batch: 650/782 |  Training Loss: 1.0352 | \n",
      "Batch: 660/782 |  Training Loss: 0.8245 | \n",
      "Batch: 670/782 |  Training Loss: 0.8632 | \n",
      "Batch: 680/782 |  Training Loss: 0.9393 | \n",
      "Batch: 690/782 |  Training Loss: 0.8193 | \n",
      "Batch: 700/782 |  Training Loss: 0.8421 | \n",
      "Batch: 710/782 |  Training Loss: 0.6720 | \n",
      "Batch: 720/782 |  Training Loss: 0.7298 | \n",
      "Batch: 730/782 |  Training Loss: 0.9018 | \n",
      "Batch: 740/782 |  Training Loss: 0.8056 | \n",
      "Batch: 750/782 |  Training Loss: 1.1370 | \n",
      "Batch: 760/782 |  Training Loss: 0.9246 | \n",
      "Batch: 770/782 |  Training Loss: 0.8419 | \n",
      "Batch: 780/782 |  Training Loss: 0.9175 | \n",
      "Epoch: 3\n",
      "Batch: 0/782 |  Training Loss: 0.8049 | \n",
      "Batch: 10/782 |  Training Loss: 0.7861 | \n",
      "Batch: 20/782 |  Training Loss: 0.7710 | \n",
      "Batch: 30/782 |  Training Loss: 0.9102 | \n",
      "Batch: 40/782 |  Training Loss: 0.9133 | \n",
      "Batch: 50/782 |  Training Loss: 0.7159 | \n",
      "Batch: 60/782 |  Training Loss: 0.9284 | \n",
      "Batch: 70/782 |  Training Loss: 0.8439 | \n",
      "Batch: 80/782 |  Training Loss: 0.9657 | \n",
      "Batch: 90/782 |  Training Loss: 1.0454 | \n",
      "Batch: 100/782 |  Training Loss: 0.7892 | \n",
      "Batch: 110/782 |  Training Loss: 0.9258 | \n",
      "Batch: 120/782 |  Training Loss: 0.6968 | \n",
      "Batch: 130/782 |  Training Loss: 0.7330 | \n",
      "Batch: 140/782 |  Training Loss: 0.8353 | \n",
      "Batch: 150/782 |  Training Loss: 0.5732 | \n",
      "Batch: 160/782 |  Training Loss: 0.5575 | \n",
      "Batch: 170/782 |  Training Loss: 0.7430 | \n",
      "Batch: 180/782 |  Training Loss: 0.8232 | \n",
      "Batch: 190/782 |  Training Loss: 0.6307 | \n",
      "Batch: 200/782 |  Training Loss: 0.7449 | \n",
      "Batch: 210/782 |  Training Loss: 0.7207 | \n",
      "Batch: 220/782 |  Training Loss: 1.1249 | \n",
      "Batch: 230/782 |  Training Loss: 1.0352 | \n",
      "Batch: 240/782 |  Training Loss: 0.8021 | \n",
      "Batch: 250/782 |  Training Loss: 0.8391 | \n",
      "Batch: 260/782 |  Training Loss: 0.9984 | \n",
      "Batch: 270/782 |  Training Loss: 0.8878 | \n",
      "Batch: 280/782 |  Training Loss: 0.8101 | \n",
      "Batch: 290/782 |  Training Loss: 0.9558 | \n",
      "Batch: 300/782 |  Training Loss: 0.6573 | \n",
      "Batch: 310/782 |  Training Loss: 0.8337 | \n",
      "Batch: 320/782 |  Training Loss: 0.8956 | \n",
      "Batch: 330/782 |  Training Loss: 0.8647 | \n",
      "Batch: 340/782 |  Training Loss: 0.7068 | \n",
      "Batch: 350/782 |  Training Loss: 0.8674 | \n",
      "Batch: 360/782 |  Training Loss: 0.8538 | \n",
      "Batch: 370/782 |  Training Loss: 0.9996 | \n",
      "Batch: 380/782 |  Training Loss: 0.8002 | \n",
      "Batch: 390/782 |  Training Loss: 0.6397 | \n",
      "Batch: 400/782 |  Training Loss: 0.8933 | \n",
      "Batch: 410/782 |  Training Loss: 0.7869 | \n",
      "Batch: 420/782 |  Training Loss: 0.7897 | \n",
      "Batch: 430/782 |  Training Loss: 0.7846 | \n",
      "Batch: 440/782 |  Training Loss: 0.9426 | \n",
      "Batch: 450/782 |  Training Loss: 0.9293 | \n",
      "Batch: 460/782 |  Training Loss: 0.5420 | \n",
      "Batch: 470/782 |  Training Loss: 0.7634 | \n",
      "Batch: 480/782 |  Training Loss: 0.9071 | \n",
      "Batch: 490/782 |  Training Loss: 0.7712 | \n",
      "Batch: 500/782 |  Training Loss: 0.8246 | \n",
      "Batch: 510/782 |  Training Loss: 0.6961 | \n",
      "Batch: 520/782 |  Training Loss: 0.7169 | \n",
      "Batch: 530/782 |  Training Loss: 0.8054 | \n",
      "Batch: 540/782 |  Training Loss: 0.5991 | \n",
      "Batch: 550/782 |  Training Loss: 0.9531 | \n",
      "Batch: 560/782 |  Training Loss: 0.7667 | \n",
      "Batch: 570/782 |  Training Loss: 0.7621 | \n",
      "Batch: 580/782 |  Training Loss: 0.9316 | \n",
      "Batch: 590/782 |  Training Loss: 0.5534 | \n",
      "Batch: 600/782 |  Training Loss: 0.8172 | \n",
      "Batch: 610/782 |  Training Loss: 0.6600 | \n",
      "Batch: 620/782 |  Training Loss: 0.9215 | \n",
      "Batch: 630/782 |  Training Loss: 0.6825 | \n",
      "Batch: 640/782 |  Training Loss: 0.7766 | \n",
      "Batch: 650/782 |  Training Loss: 0.8800 | \n",
      "Batch: 660/782 |  Training Loss: 0.5440 | \n",
      "Batch: 670/782 |  Training Loss: 0.8148 | \n",
      "Batch: 680/782 |  Training Loss: 0.9280 | \n",
      "Batch: 690/782 |  Training Loss: 0.6868 | \n",
      "Batch: 700/782 |  Training Loss: 0.8452 | \n",
      "Batch: 710/782 |  Training Loss: 0.7970 | \n",
      "Batch: 720/782 |  Training Loss: 0.8070 | \n",
      "Batch: 730/782 |  Training Loss: 0.7460 | \n",
      "Batch: 740/782 |  Training Loss: 0.6960 | \n",
      "Batch: 750/782 |  Training Loss: 0.7776 | \n",
      "Batch: 760/782 |  Training Loss: 0.7272 | \n",
      "Batch: 770/782 |  Training Loss: 0.8608 | \n",
      "Batch: 780/782 |  Training Loss: 0.3862 | \n",
      "Epoch: 4\n",
      "Batch: 0/782 |  Training Loss: 0.5216 | \n",
      "Batch: 10/782 |  Training Loss: 0.6296 | \n",
      "Batch: 20/782 |  Training Loss: 0.9171 | \n",
      "Batch: 30/782 |  Training Loss: 0.7931 | \n",
      "Batch: 40/782 |  Training Loss: 0.8093 | \n",
      "Batch: 50/782 |  Training Loss: 0.6926 | \n",
      "Batch: 60/782 |  Training Loss: 0.7517 | \n",
      "Batch: 70/782 |  Training Loss: 0.6926 | \n",
      "Batch: 80/782 |  Training Loss: 0.5121 | \n",
      "Batch: 90/782 |  Training Loss: 0.4959 | \n",
      "Batch: 100/782 |  Training Loss: 0.8745 | \n",
      "Batch: 110/782 |  Training Loss: 0.7093 | \n",
      "Batch: 120/782 |  Training Loss: 0.6708 | \n",
      "Batch: 130/782 |  Training Loss: 0.7524 | \n",
      "Batch: 140/782 |  Training Loss: 0.7375 | \n",
      "Batch: 150/782 |  Training Loss: 0.6455 | \n",
      "Batch: 160/782 |  Training Loss: 0.5759 | \n",
      "Batch: 170/782 |  Training Loss: 1.0499 | \n",
      "Batch: 180/782 |  Training Loss: 0.5703 | \n",
      "Batch: 190/782 |  Training Loss: 0.6510 | \n",
      "Batch: 200/782 |  Training Loss: 0.6972 | \n",
      "Batch: 210/782 |  Training Loss: 0.6075 | \n",
      "Batch: 220/782 |  Training Loss: 0.7122 | \n",
      "Batch: 230/782 |  Training Loss: 0.6278 | \n",
      "Batch: 240/782 |  Training Loss: 0.5333 | \n",
      "Batch: 250/782 |  Training Loss: 0.5037 | \n",
      "Batch: 260/782 |  Training Loss: 0.5742 | \n",
      "Batch: 270/782 |  Training Loss: 0.8637 | \n",
      "Batch: 280/782 |  Training Loss: 0.7715 | \n",
      "Batch: 290/782 |  Training Loss: 0.9589 | \n",
      "Batch: 300/782 |  Training Loss: 0.6478 | \n",
      "Batch: 310/782 |  Training Loss: 0.7664 | \n",
      "Batch: 320/782 |  Training Loss: 0.8885 | \n",
      "Batch: 330/782 |  Training Loss: 0.5458 | \n",
      "Batch: 340/782 |  Training Loss: 0.4755 | \n",
      "Batch: 350/782 |  Training Loss: 0.6615 | \n",
      "Batch: 360/782 |  Training Loss: 0.7597 | \n",
      "Batch: 370/782 |  Training Loss: 0.8924 | \n",
      "Batch: 380/782 |  Training Loss: 0.5975 | \n",
      "Batch: 390/782 |  Training Loss: 0.5538 | \n",
      "Batch: 400/782 |  Training Loss: 0.5042 | \n",
      "Batch: 410/782 |  Training Loss: 0.6311 | \n",
      "Batch: 420/782 |  Training Loss: 0.8232 | \n",
      "Batch: 430/782 |  Training Loss: 0.7742 | \n",
      "Batch: 440/782 |  Training Loss: 0.6680 | \n",
      "Batch: 450/782 |  Training Loss: 0.6464 | \n",
      "Batch: 460/782 |  Training Loss: 0.4623 | \n",
      "Batch: 470/782 |  Training Loss: 0.6797 | \n",
      "Batch: 480/782 |  Training Loss: 0.6111 | \n",
      "Batch: 490/782 |  Training Loss: 0.8517 | \n",
      "Batch: 500/782 |  Training Loss: 0.6486 | \n",
      "Batch: 510/782 |  Training Loss: 0.5391 | \n",
      "Batch: 520/782 |  Training Loss: 0.5772 | \n",
      "Batch: 530/782 |  Training Loss: 0.5005 | \n",
      "Batch: 540/782 |  Training Loss: 0.8327 | \n",
      "Batch: 550/782 |  Training Loss: 0.6014 | \n",
      "Batch: 560/782 |  Training Loss: 0.7903 | \n",
      "Batch: 570/782 |  Training Loss: 0.8480 | \n",
      "Batch: 580/782 |  Training Loss: 0.5987 | \n",
      "Batch: 590/782 |  Training Loss: 0.5827 | \n",
      "Batch: 600/782 |  Training Loss: 0.5625 | \n",
      "Batch: 610/782 |  Training Loss: 0.7542 | \n",
      "Batch: 620/782 |  Training Loss: 0.5298 | \n",
      "Batch: 630/782 |  Training Loss: 0.5810 | \n",
      "Batch: 640/782 |  Training Loss: 0.5999 | \n",
      "Batch: 650/782 |  Training Loss: 0.8327 | \n",
      "Batch: 660/782 |  Training Loss: 0.8671 | \n",
      "Batch: 670/782 |  Training Loss: 0.4541 | \n",
      "Batch: 680/782 |  Training Loss: 0.6003 | \n",
      "Batch: 690/782 |  Training Loss: 0.8102 | \n",
      "Batch: 700/782 |  Training Loss: 0.7001 | \n",
      "Batch: 710/782 |  Training Loss: 0.5094 | \n",
      "Batch: 720/782 |  Training Loss: 0.4193 | \n",
      "Batch: 730/782 |  Training Loss: 0.5103 | \n",
      "Batch: 740/782 |  Training Loss: 0.7275 | \n",
      "Batch: 750/782 |  Training Loss: 0.5189 | \n",
      "Batch: 760/782 |  Training Loss: 0.5995 | \n",
      "Batch: 770/782 |  Training Loss: 0.4680 | \n",
      "Batch: 780/782 |  Training Loss: 0.6157 | \n",
      "Epoch: 5\n",
      "Batch: 0/782 |  Training Loss: 0.5749 | \n",
      "Batch: 10/782 |  Training Loss: 0.5595 | \n",
      "Batch: 20/782 |  Training Loss: 0.4024 | \n",
      "Batch: 30/782 |  Training Loss: 0.4884 | \n",
      "Batch: 40/782 |  Training Loss: 0.4681 | \n",
      "Batch: 50/782 |  Training Loss: 0.5136 | \n",
      "Batch: 60/782 |  Training Loss: 0.7521 | \n",
      "Batch: 70/782 |  Training Loss: 0.6358 | \n",
      "Batch: 80/782 |  Training Loss: 0.6726 | \n",
      "Batch: 90/782 |  Training Loss: 0.4544 | \n",
      "Batch: 100/782 |  Training Loss: 0.4671 | \n",
      "Batch: 110/782 |  Training Loss: 0.3084 | \n",
      "Batch: 120/782 |  Training Loss: 0.7202 | \n",
      "Batch: 130/782 |  Training Loss: 0.7042 | \n",
      "Batch: 140/782 |  Training Loss: 0.6283 | \n",
      "Batch: 150/782 |  Training Loss: 0.6710 | \n",
      "Batch: 160/782 |  Training Loss: 0.6995 | \n",
      "Batch: 170/782 |  Training Loss: 0.6251 | \n",
      "Batch: 180/782 |  Training Loss: 0.7965 | \n",
      "Batch: 190/782 |  Training Loss: 0.5339 | \n",
      "Batch: 200/782 |  Training Loss: 0.3658 | \n",
      "Batch: 210/782 |  Training Loss: 0.6856 | \n",
      "Batch: 220/782 |  Training Loss: 0.4839 | \n",
      "Batch: 230/782 |  Training Loss: 0.3633 | \n",
      "Batch: 240/782 |  Training Loss: 0.5620 | \n",
      "Batch: 250/782 |  Training Loss: 0.4155 | \n",
      "Batch: 260/782 |  Training Loss: 0.5359 | \n",
      "Batch: 270/782 |  Training Loss: 0.5601 | \n",
      "Batch: 280/782 |  Training Loss: 0.6192 | \n",
      "Batch: 290/782 |  Training Loss: 0.6439 | \n",
      "Batch: 300/782 |  Training Loss: 0.5085 | \n",
      "Batch: 310/782 |  Training Loss: 0.8638 | \n",
      "Batch: 320/782 |  Training Loss: 0.7486 | \n",
      "Batch: 330/782 |  Training Loss: 0.5993 | \n",
      "Batch: 340/782 |  Training Loss: 0.6537 | \n",
      "Batch: 350/782 |  Training Loss: 0.5028 | \n",
      "Batch: 360/782 |  Training Loss: 0.3083 | \n",
      "Batch: 370/782 |  Training Loss: 0.5623 | \n",
      "Batch: 380/782 |  Training Loss: 0.6857 | \n",
      "Batch: 390/782 |  Training Loss: 0.4890 | \n",
      "Batch: 400/782 |  Training Loss: 0.5438 | \n",
      "Batch: 410/782 |  Training Loss: 0.7121 | \n",
      "Batch: 420/782 |  Training Loss: 0.4446 | \n",
      "Batch: 430/782 |  Training Loss: 0.5843 | \n",
      "Batch: 440/782 |  Training Loss: 0.7863 | \n",
      "Batch: 450/782 |  Training Loss: 0.5800 | \n",
      "Batch: 460/782 |  Training Loss: 0.5214 | \n",
      "Batch: 470/782 |  Training Loss: 0.6803 | \n",
      "Batch: 480/782 |  Training Loss: 0.6138 | \n",
      "Batch: 490/782 |  Training Loss: 0.5820 | \n",
      "Batch: 500/782 |  Training Loss: 0.3929 | \n",
      "Batch: 510/782 |  Training Loss: 0.6570 | \n",
      "Batch: 520/782 |  Training Loss: 0.5754 | \n",
      "Batch: 530/782 |  Training Loss: 0.5517 | \n",
      "Batch: 540/782 |  Training Loss: 0.6319 | \n",
      "Batch: 550/782 |  Training Loss: 0.6842 | \n",
      "Batch: 560/782 |  Training Loss: 0.8120 | \n",
      "Batch: 570/782 |  Training Loss: 0.8665 | \n",
      "Batch: 580/782 |  Training Loss: 0.6664 | \n",
      "Batch: 590/782 |  Training Loss: 0.6125 | \n",
      "Batch: 600/782 |  Training Loss: 0.6480 | \n",
      "Batch: 610/782 |  Training Loss: 0.5747 | \n",
      "Batch: 620/782 |  Training Loss: 0.4991 | \n",
      "Batch: 630/782 |  Training Loss: 0.8534 | \n",
      "Batch: 640/782 |  Training Loss: 0.4740 | \n",
      "Batch: 650/782 |  Training Loss: 0.5647 | \n",
      "Batch: 660/782 |  Training Loss: 0.6257 | \n",
      "Batch: 670/782 |  Training Loss: 0.6459 | \n",
      "Batch: 680/782 |  Training Loss: 0.7681 | \n",
      "Batch: 690/782 |  Training Loss: 0.7171 | \n",
      "Batch: 700/782 |  Training Loss: 0.5322 | \n",
      "Batch: 710/782 |  Training Loss: 0.4784 | \n",
      "Batch: 720/782 |  Training Loss: 0.6106 | \n",
      "Batch: 730/782 |  Training Loss: 0.7539 | \n",
      "Batch: 740/782 |  Training Loss: 0.6537 | \n",
      "Batch: 750/782 |  Training Loss: 0.3618 | \n",
      "Batch: 760/782 |  Training Loss: 0.5119 | \n",
      "Batch: 770/782 |  Training Loss: 0.5169 | \n",
      "Batch: 780/782 |  Training Loss: 0.4950 | \n",
      "Epoch: 6\n",
      "Batch: 0/782 |  Training Loss: 0.4568 | \n",
      "Batch: 10/782 |  Training Loss: 0.5365 | \n",
      "Batch: 20/782 |  Training Loss: 0.5328 | \n",
      "Batch: 30/782 |  Training Loss: 0.4440 | \n",
      "Batch: 40/782 |  Training Loss: 0.6221 | \n",
      "Batch: 50/782 |  Training Loss: 0.6290 | \n",
      "Batch: 60/782 |  Training Loss: 0.4066 | \n",
      "Batch: 70/782 |  Training Loss: 0.4221 | \n",
      "Batch: 80/782 |  Training Loss: 0.4896 | \n",
      "Batch: 90/782 |  Training Loss: 0.5444 | \n",
      "Batch: 100/782 |  Training Loss: 0.5142 | \n",
      "Batch: 110/782 |  Training Loss: 0.4688 | \n",
      "Batch: 120/782 |  Training Loss: 0.5692 | \n",
      "Batch: 130/782 |  Training Loss: 0.6683 | \n",
      "Batch: 140/782 |  Training Loss: 0.5608 | \n",
      "Batch: 150/782 |  Training Loss: 0.6422 | \n",
      "Batch: 160/782 |  Training Loss: 0.4700 | \n",
      "Batch: 170/782 |  Training Loss: 0.6519 | \n",
      "Batch: 180/782 |  Training Loss: 0.6082 | \n",
      "Batch: 190/782 |  Training Loss: 0.6444 | \n",
      "Batch: 200/782 |  Training Loss: 0.5342 | \n",
      "Batch: 210/782 |  Training Loss: 0.5274 | \n",
      "Batch: 220/782 |  Training Loss: 0.4422 | \n",
      "Batch: 230/782 |  Training Loss: 0.5742 | \n",
      "Batch: 240/782 |  Training Loss: 0.4623 | \n",
      "Batch: 250/782 |  Training Loss: 0.6093 | \n",
      "Batch: 260/782 |  Training Loss: 0.4520 | \n",
      "Batch: 270/782 |  Training Loss: 0.5067 | \n",
      "Batch: 280/782 |  Training Loss: 0.2975 | \n",
      "Batch: 290/782 |  Training Loss: 0.3684 | \n",
      "Batch: 300/782 |  Training Loss: 0.5244 | \n",
      "Batch: 310/782 |  Training Loss: 0.6554 | \n",
      "Batch: 320/782 |  Training Loss: 0.5927 | \n",
      "Batch: 330/782 |  Training Loss: 0.5311 | \n",
      "Batch: 340/782 |  Training Loss: 0.4947 | \n",
      "Batch: 350/782 |  Training Loss: 0.6389 | \n",
      "Batch: 360/782 |  Training Loss: 0.4699 | \n",
      "Batch: 370/782 |  Training Loss: 0.5337 | \n",
      "Batch: 380/782 |  Training Loss: 0.6577 | \n",
      "Batch: 390/782 |  Training Loss: 0.5849 | \n",
      "Batch: 400/782 |  Training Loss: 0.4740 | \n",
      "Batch: 410/782 |  Training Loss: 0.7085 | \n",
      "Batch: 420/782 |  Training Loss: 0.3910 | \n",
      "Batch: 430/782 |  Training Loss: 0.4061 | \n",
      "Batch: 440/782 |  Training Loss: 0.7619 | \n",
      "Batch: 450/782 |  Training Loss: 0.4323 | \n",
      "Batch: 460/782 |  Training Loss: 0.6229 | \n",
      "Batch: 470/782 |  Training Loss: 0.4348 | \n",
      "Batch: 480/782 |  Training Loss: 0.7881 | \n",
      "Batch: 490/782 |  Training Loss: 0.8588 | \n",
      "Batch: 500/782 |  Training Loss: 0.4728 | \n",
      "Batch: 510/782 |  Training Loss: 0.9311 | \n",
      "Batch: 520/782 |  Training Loss: 0.4032 | \n",
      "Batch: 530/782 |  Training Loss: 0.4044 | \n",
      "Batch: 540/782 |  Training Loss: 0.5765 | \n",
      "Batch: 550/782 |  Training Loss: 0.2580 | \n",
      "Batch: 560/782 |  Training Loss: 0.4083 | \n",
      "Batch: 570/782 |  Training Loss: 0.8732 | \n",
      "Batch: 580/782 |  Training Loss: 0.5138 | \n",
      "Batch: 590/782 |  Training Loss: 0.6583 | \n",
      "Batch: 600/782 |  Training Loss: 0.6733 | \n",
      "Batch: 610/782 |  Training Loss: 0.4342 | \n",
      "Batch: 620/782 |  Training Loss: 0.4180 | \n",
      "Batch: 630/782 |  Training Loss: 0.7706 | \n",
      "Batch: 640/782 |  Training Loss: 0.5989 | \n",
      "Batch: 650/782 |  Training Loss: 0.6625 | \n",
      "Batch: 660/782 |  Training Loss: 0.6373 | \n",
      "Batch: 670/782 |  Training Loss: 0.6326 | \n",
      "Batch: 680/782 |  Training Loss: 0.6726 | \n",
      "Batch: 690/782 |  Training Loss: 0.5040 | \n",
      "Batch: 700/782 |  Training Loss: 0.7954 | \n",
      "Batch: 710/782 |  Training Loss: 0.5298 | \n",
      "Batch: 720/782 |  Training Loss: 0.4075 | \n",
      "Batch: 730/782 |  Training Loss: 0.7736 | \n",
      "Batch: 740/782 |  Training Loss: 0.5743 | \n",
      "Batch: 750/782 |  Training Loss: 0.7529 | \n",
      "Batch: 760/782 |  Training Loss: 0.4992 | \n",
      "Batch: 770/782 |  Training Loss: 0.5537 | \n",
      "Batch: 780/782 |  Training Loss: 0.5804 | \n",
      "Epoch: 7\n",
      "Batch: 0/782 |  Training Loss: 0.6166 | \n",
      "Batch: 10/782 |  Training Loss: 0.3301 | \n",
      "Batch: 20/782 |  Training Loss: 0.4902 | \n",
      "Batch: 30/782 |  Training Loss: 0.3942 | \n",
      "Batch: 40/782 |  Training Loss: 0.3093 | \n",
      "Batch: 50/782 |  Training Loss: 0.5081 | \n",
      "Batch: 60/782 |  Training Loss: 0.4578 | \n",
      "Batch: 70/782 |  Training Loss: 0.3615 | \n",
      "Batch: 80/782 |  Training Loss: 0.3555 | \n",
      "Batch: 90/782 |  Training Loss: 0.4233 | \n",
      "Batch: 100/782 |  Training Loss: 0.4417 | \n",
      "Batch: 110/782 |  Training Loss: 0.3554 | \n",
      "Batch: 120/782 |  Training Loss: 0.4621 | \n",
      "Batch: 130/782 |  Training Loss: 0.4210 | \n",
      "Batch: 140/782 |  Training Loss: 0.4354 | \n",
      "Batch: 150/782 |  Training Loss: 0.4443 | \n",
      "Batch: 160/782 |  Training Loss: 0.4186 | \n",
      "Batch: 170/782 |  Training Loss: 0.5665 | \n",
      "Batch: 180/782 |  Training Loss: 0.6139 | \n",
      "Batch: 190/782 |  Training Loss: 0.3581 | \n",
      "Batch: 200/782 |  Training Loss: 0.5630 | \n",
      "Batch: 210/782 |  Training Loss: 0.5254 | \n",
      "Batch: 220/782 |  Training Loss: 0.4500 | \n",
      "Batch: 230/782 |  Training Loss: 0.3347 | \n",
      "Batch: 240/782 |  Training Loss: 0.4148 | \n",
      "Batch: 250/782 |  Training Loss: 0.4607 | \n",
      "Batch: 260/782 |  Training Loss: 0.4359 | \n",
      "Batch: 270/782 |  Training Loss: 0.5207 | \n",
      "Batch: 280/782 |  Training Loss: 0.5752 | \n",
      "Batch: 290/782 |  Training Loss: 0.4560 | \n",
      "Batch: 300/782 |  Training Loss: 0.3627 | \n",
      "Batch: 310/782 |  Training Loss: 0.5934 | \n",
      "Batch: 320/782 |  Training Loss: 0.3050 | \n",
      "Batch: 330/782 |  Training Loss: 0.5572 | \n",
      "Batch: 340/782 |  Training Loss: 0.2995 | \n",
      "Batch: 350/782 |  Training Loss: 0.4124 | \n",
      "Batch: 360/782 |  Training Loss: 0.6248 | \n",
      "Batch: 370/782 |  Training Loss: 0.4499 | \n",
      "Batch: 380/782 |  Training Loss: 0.3417 | \n",
      "Batch: 390/782 |  Training Loss: 0.4327 | \n",
      "Batch: 400/782 |  Training Loss: 0.5823 | \n",
      "Batch: 410/782 |  Training Loss: 0.5241 | \n",
      "Batch: 420/782 |  Training Loss: 0.3677 | \n",
      "Batch: 430/782 |  Training Loss: 0.6572 | \n",
      "Batch: 440/782 |  Training Loss: 0.3253 | \n",
      "Batch: 450/782 |  Training Loss: 0.5185 | \n",
      "Batch: 460/782 |  Training Loss: 0.4878 | \n",
      "Batch: 470/782 |  Training Loss: 0.4789 | \n",
      "Batch: 480/782 |  Training Loss: 0.3652 | \n",
      "Batch: 490/782 |  Training Loss: 0.4556 | \n",
      "Batch: 500/782 |  Training Loss: 0.6284 | \n",
      "Batch: 510/782 |  Training Loss: 0.6093 | \n",
      "Batch: 520/782 |  Training Loss: 0.3035 | \n",
      "Batch: 530/782 |  Training Loss: 0.3659 | \n",
      "Batch: 540/782 |  Training Loss: 0.4243 | \n",
      "Batch: 550/782 |  Training Loss: 0.5127 | \n",
      "Batch: 560/782 |  Training Loss: 0.6575 | \n",
      "Batch: 570/782 |  Training Loss: 0.5896 | \n",
      "Batch: 580/782 |  Training Loss: 0.3980 | \n",
      "Batch: 590/782 |  Training Loss: 0.4095 | \n",
      "Batch: 600/782 |  Training Loss: 0.4790 | \n",
      "Batch: 610/782 |  Training Loss: 0.2710 | \n",
      "Batch: 620/782 |  Training Loss: 0.4261 | \n",
      "Batch: 630/782 |  Training Loss: 0.3302 | \n",
      "Batch: 640/782 |  Training Loss: 0.5058 | \n",
      "Batch: 650/782 |  Training Loss: 0.4682 | \n",
      "Batch: 660/782 |  Training Loss: 0.6063 | \n",
      "Batch: 670/782 |  Training Loss: 0.4853 | \n",
      "Batch: 680/782 |  Training Loss: 0.6142 | \n",
      "Batch: 690/782 |  Training Loss: 0.3453 | \n",
      "Batch: 700/782 |  Training Loss: 0.5820 | \n",
      "Batch: 710/782 |  Training Loss: 0.5186 | \n",
      "Batch: 720/782 |  Training Loss: 0.4012 | \n",
      "Batch: 730/782 |  Training Loss: 0.4818 | \n",
      "Batch: 740/782 |  Training Loss: 0.5435 | \n",
      "Batch: 750/782 |  Training Loss: 0.4761 | \n",
      "Batch: 760/782 |  Training Loss: 0.4077 | \n",
      "Batch: 770/782 |  Training Loss: 0.4787 | \n",
      "Batch: 780/782 |  Training Loss: 0.3894 | \n",
      "Epoch: 8\n",
      "Batch: 0/782 |  Training Loss: 0.5262 | \n",
      "Batch: 10/782 |  Training Loss: 0.2969 | \n",
      "Batch: 20/782 |  Training Loss: 0.4161 | \n",
      "Batch: 30/782 |  Training Loss: 0.3480 | \n",
      "Batch: 40/782 |  Training Loss: 0.4062 | \n",
      "Batch: 50/782 |  Training Loss: 0.4958 | \n",
      "Batch: 60/782 |  Training Loss: 0.3918 | \n",
      "Batch: 70/782 |  Training Loss: 0.3884 | \n",
      "Batch: 80/782 |  Training Loss: 0.5304 | \n",
      "Batch: 90/782 |  Training Loss: 0.2786 | \n",
      "Batch: 100/782 |  Training Loss: 0.4492 | \n",
      "Batch: 110/782 |  Training Loss: 0.4368 | \n",
      "Batch: 120/782 |  Training Loss: 0.3337 | \n",
      "Batch: 130/782 |  Training Loss: 0.4789 | \n",
      "Batch: 140/782 |  Training Loss: 0.6017 | \n",
      "Batch: 150/782 |  Training Loss: 0.3457 | \n",
      "Batch: 160/782 |  Training Loss: 0.3982 | \n",
      "Batch: 170/782 |  Training Loss: 0.4279 | \n",
      "Batch: 180/782 |  Training Loss: 0.4512 | \n",
      "Batch: 190/782 |  Training Loss: 0.4330 | \n",
      "Batch: 200/782 |  Training Loss: 0.3016 | \n",
      "Batch: 210/782 |  Training Loss: 0.4135 | \n",
      "Batch: 220/782 |  Training Loss: 0.3384 | \n",
      "Batch: 230/782 |  Training Loss: 0.4966 | \n",
      "Batch: 240/782 |  Training Loss: 0.5618 | \n",
      "Batch: 250/782 |  Training Loss: 0.3343 | \n",
      "Batch: 260/782 |  Training Loss: 0.4970 | \n",
      "Batch: 270/782 |  Training Loss: 0.3582 | \n",
      "Batch: 280/782 |  Training Loss: 0.4100 | \n",
      "Batch: 290/782 |  Training Loss: 0.4110 | \n",
      "Batch: 300/782 |  Training Loss: 0.4264 | \n",
      "Batch: 310/782 |  Training Loss: 0.4111 | \n",
      "Batch: 320/782 |  Training Loss: 0.3364 | \n",
      "Batch: 330/782 |  Training Loss: 0.4061 | \n",
      "Batch: 340/782 |  Training Loss: 0.4982 | \n",
      "Batch: 350/782 |  Training Loss: 0.3957 | \n",
      "Batch: 360/782 |  Training Loss: 0.4008 | \n",
      "Batch: 370/782 |  Training Loss: 0.5846 | \n",
      "Batch: 380/782 |  Training Loss: 0.5532 | \n",
      "Batch: 390/782 |  Training Loss: 0.4855 | \n",
      "Batch: 400/782 |  Training Loss: 0.5095 | \n",
      "Batch: 410/782 |  Training Loss: 0.4400 | \n",
      "Batch: 420/782 |  Training Loss: 0.2773 | \n",
      "Batch: 430/782 |  Training Loss: 0.3843 | \n",
      "Batch: 440/782 |  Training Loss: 0.3767 | \n",
      "Batch: 450/782 |  Training Loss: 0.5132 | \n",
      "Batch: 460/782 |  Training Loss: 0.5428 | \n",
      "Batch: 470/782 |  Training Loss: 0.5275 | \n",
      "Batch: 480/782 |  Training Loss: 0.5042 | \n",
      "Batch: 490/782 |  Training Loss: 0.3497 | \n",
      "Batch: 500/782 |  Training Loss: 0.2778 | \n",
      "Batch: 510/782 |  Training Loss: 0.3418 | \n",
      "Batch: 520/782 |  Training Loss: 0.4094 | \n",
      "Batch: 530/782 |  Training Loss: 0.3369 | \n",
      "Batch: 540/782 |  Training Loss: 0.4036 | \n",
      "Batch: 550/782 |  Training Loss: 0.5668 | \n",
      "Batch: 560/782 |  Training Loss: 0.5088 | \n",
      "Batch: 570/782 |  Training Loss: 0.3602 | \n",
      "Batch: 580/782 |  Training Loss: 0.5020 | \n",
      "Batch: 590/782 |  Training Loss: 0.4584 | \n",
      "Batch: 600/782 |  Training Loss: 0.3668 | \n",
      "Batch: 610/782 |  Training Loss: 0.5716 | \n",
      "Batch: 620/782 |  Training Loss: 0.5486 | \n",
      "Batch: 630/782 |  Training Loss: 0.3089 | \n",
      "Batch: 640/782 |  Training Loss: 0.4797 | \n",
      "Batch: 650/782 |  Training Loss: 0.4497 | \n",
      "Batch: 660/782 |  Training Loss: 0.5511 | \n",
      "Batch: 670/782 |  Training Loss: 0.4945 | \n",
      "Batch: 680/782 |  Training Loss: 0.5740 | \n",
      "Batch: 690/782 |  Training Loss: 0.3456 | \n",
      "Batch: 700/782 |  Training Loss: 0.4841 | \n",
      "Batch: 710/782 |  Training Loss: 0.7003 | \n",
      "Batch: 720/782 |  Training Loss: 0.5025 | \n",
      "Batch: 730/782 |  Training Loss: 0.4907 | \n",
      "Batch: 740/782 |  Training Loss: 0.5169 | \n",
      "Batch: 750/782 |  Training Loss: 0.4376 | \n",
      "Batch: 760/782 |  Training Loss: 0.5215 | \n",
      "Batch: 770/782 |  Training Loss: 0.4406 | \n",
      "Batch: 780/782 |  Training Loss: 0.5292 | \n",
      "Epoch: 9\n",
      "Batch: 0/782 |  Training Loss: 0.4191 | \n",
      "Batch: 10/782 |  Training Loss: 0.3536 | \n",
      "Batch: 20/782 |  Training Loss: 0.3264 | \n",
      "Batch: 30/782 |  Training Loss: 0.3511 | \n",
      "Batch: 40/782 |  Training Loss: 0.5376 | \n",
      "Batch: 50/782 |  Training Loss: 0.2269 | \n",
      "Batch: 60/782 |  Training Loss: 0.2240 | \n",
      "Batch: 70/782 |  Training Loss: 0.3645 | \n",
      "Batch: 80/782 |  Training Loss: 0.2558 | \n",
      "Batch: 90/782 |  Training Loss: 0.4091 | \n",
      "Batch: 100/782 |  Training Loss: 0.3317 | \n",
      "Batch: 110/782 |  Training Loss: 0.2852 | \n",
      "Batch: 120/782 |  Training Loss: 0.3784 | \n",
      "Batch: 130/782 |  Training Loss: 0.3932 | \n",
      "Batch: 140/782 |  Training Loss: 0.2795 | \n",
      "Batch: 150/782 |  Training Loss: 0.3487 | \n",
      "Batch: 160/782 |  Training Loss: 0.3668 | \n",
      "Batch: 170/782 |  Training Loss: 0.3203 | \n",
      "Batch: 180/782 |  Training Loss: 0.2882 | \n",
      "Batch: 190/782 |  Training Loss: 0.5877 | \n",
      "Batch: 200/782 |  Training Loss: 0.3488 | \n",
      "Batch: 210/782 |  Training Loss: 0.5474 | \n",
      "Batch: 220/782 |  Training Loss: 0.3337 | \n",
      "Batch: 230/782 |  Training Loss: 0.5346 | \n",
      "Batch: 240/782 |  Training Loss: 0.3584 | \n",
      "Batch: 250/782 |  Training Loss: 0.1547 | \n",
      "Batch: 260/782 |  Training Loss: 0.2969 | \n",
      "Batch: 270/782 |  Training Loss: 0.4128 | \n",
      "Batch: 280/782 |  Training Loss: 0.4182 | \n",
      "Batch: 290/782 |  Training Loss: 0.4829 | \n",
      "Batch: 300/782 |  Training Loss: 0.4238 | \n",
      "Batch: 310/782 |  Training Loss: 0.3257 | \n",
      "Batch: 320/782 |  Training Loss: 0.2107 | \n",
      "Batch: 330/782 |  Training Loss: 0.4427 | \n",
      "Batch: 340/782 |  Training Loss: 0.2633 | \n",
      "Batch: 350/782 |  Training Loss: 0.2766 | \n",
      "Batch: 360/782 |  Training Loss: 0.4651 | \n",
      "Batch: 370/782 |  Training Loss: 0.4008 | \n",
      "Batch: 380/782 |  Training Loss: 0.4939 | \n",
      "Batch: 390/782 |  Training Loss: 0.4124 | \n",
      "Batch: 400/782 |  Training Loss: 0.4758 | \n",
      "Batch: 410/782 |  Training Loss: 0.4260 | \n",
      "Batch: 420/782 |  Training Loss: 0.4127 | \n",
      "Batch: 430/782 |  Training Loss: 0.3663 | \n",
      "Batch: 440/782 |  Training Loss: 0.4320 | \n",
      "Batch: 450/782 |  Training Loss: 0.6034 | \n",
      "Batch: 460/782 |  Training Loss: 0.2746 | \n",
      "Batch: 470/782 |  Training Loss: 0.2735 | \n",
      "Batch: 480/782 |  Training Loss: 0.2747 | \n",
      "Batch: 490/782 |  Training Loss: 0.2354 | \n",
      "Batch: 500/782 |  Training Loss: 0.4131 | \n",
      "Batch: 510/782 |  Training Loss: 0.4438 | \n",
      "Batch: 520/782 |  Training Loss: 0.5344 | \n",
      "Batch: 530/782 |  Training Loss: 0.4242 | \n",
      "Batch: 540/782 |  Training Loss: 0.4082 | \n",
      "Batch: 550/782 |  Training Loss: 0.4477 | \n",
      "Batch: 560/782 |  Training Loss: 0.7175 | \n",
      "Batch: 570/782 |  Training Loss: 0.3245 | \n",
      "Batch: 580/782 |  Training Loss: 0.4208 | \n",
      "Batch: 590/782 |  Training Loss: 0.3607 | \n",
      "Batch: 600/782 |  Training Loss: 0.4094 | \n",
      "Batch: 610/782 |  Training Loss: 0.2026 | \n",
      "Batch: 620/782 |  Training Loss: 0.5086 | \n",
      "Batch: 630/782 |  Training Loss: 0.4162 | \n",
      "Batch: 640/782 |  Training Loss: 0.2771 | \n",
      "Batch: 650/782 |  Training Loss: 0.4466 | \n",
      "Batch: 660/782 |  Training Loss: 0.3699 | \n",
      "Batch: 670/782 |  Training Loss: 0.2147 | \n",
      "Batch: 680/782 |  Training Loss: 0.2736 | \n",
      "Batch: 690/782 |  Training Loss: 0.3109 | \n",
      "Batch: 700/782 |  Training Loss: 0.3359 | \n",
      "Batch: 710/782 |  Training Loss: 0.3631 | \n",
      "Batch: 720/782 |  Training Loss: 0.4489 | \n",
      "Batch: 730/782 |  Training Loss: 0.5749 | \n",
      "Batch: 740/782 |  Training Loss: 0.4052 | \n",
      "Batch: 750/782 |  Training Loss: 0.4206 | \n",
      "Batch: 760/782 |  Training Loss: 0.4251 | \n",
      "Batch: 770/782 |  Training Loss: 0.3304 | \n",
      "Batch: 780/782 |  Training Loss: 0.3807 | \n",
      "Epoch: 10\n",
      "Batch: 0/782 |  Training Loss: 0.3177 | \n",
      "Batch: 10/782 |  Training Loss: 0.3152 | \n",
      "Batch: 20/782 |  Training Loss: 0.4625 | \n",
      "Batch: 30/782 |  Training Loss: 0.3922 | \n",
      "Batch: 40/782 |  Training Loss: 0.3257 | \n",
      "Batch: 50/782 |  Training Loss: 0.2342 | \n",
      "Batch: 60/782 |  Training Loss: 0.3609 | \n",
      "Batch: 70/782 |  Training Loss: 0.1737 | \n",
      "Batch: 80/782 |  Training Loss: 0.1682 | \n",
      "Batch: 90/782 |  Training Loss: 0.2833 | \n",
      "Batch: 100/782 |  Training Loss: 0.5611 | \n",
      "Batch: 110/782 |  Training Loss: 0.1730 | \n",
      "Batch: 120/782 |  Training Loss: 0.3195 | \n",
      "Batch: 130/782 |  Training Loss: 0.2088 | \n",
      "Batch: 140/782 |  Training Loss: 0.3991 | \n",
      "Batch: 150/782 |  Training Loss: 0.1597 | \n",
      "Batch: 160/782 |  Training Loss: 0.2994 | \n",
      "Batch: 170/782 |  Training Loss: 0.4193 | \n",
      "Batch: 180/782 |  Training Loss: 0.4932 | \n",
      "Batch: 190/782 |  Training Loss: 0.3403 | \n",
      "Batch: 200/782 |  Training Loss: 0.4374 | \n",
      "Batch: 210/782 |  Training Loss: 0.2861 | \n",
      "Batch: 220/782 |  Training Loss: 0.3530 | \n",
      "Batch: 230/782 |  Training Loss: 0.2012 | \n",
      "Batch: 240/782 |  Training Loss: 0.4003 | \n",
      "Batch: 250/782 |  Training Loss: 0.3478 | \n",
      "Batch: 260/782 |  Training Loss: 0.2380 | \n",
      "Batch: 270/782 |  Training Loss: 0.3007 | \n",
      "Batch: 280/782 |  Training Loss: 0.3843 | \n",
      "Batch: 290/782 |  Training Loss: 0.3922 | \n",
      "Batch: 300/782 |  Training Loss: 0.2199 | \n",
      "Batch: 310/782 |  Training Loss: 0.3450 | \n",
      "Batch: 320/782 |  Training Loss: 0.3712 | \n",
      "Batch: 330/782 |  Training Loss: 0.2910 | \n",
      "Batch: 340/782 |  Training Loss: 0.5043 | \n",
      "Batch: 350/782 |  Training Loss: 0.3714 | \n",
      "Batch: 360/782 |  Training Loss: 0.3156 | \n",
      "Batch: 370/782 |  Training Loss: 0.2810 | \n",
      "Batch: 380/782 |  Training Loss: 0.2886 | \n",
      "Batch: 390/782 |  Training Loss: 0.3493 | \n",
      "Batch: 400/782 |  Training Loss: 0.2874 | \n",
      "Batch: 410/782 |  Training Loss: 0.4235 | \n",
      "Batch: 420/782 |  Training Loss: 0.2910 | \n",
      "Batch: 430/782 |  Training Loss: 0.3374 | \n",
      "Batch: 440/782 |  Training Loss: 0.3988 | \n",
      "Batch: 450/782 |  Training Loss: 0.3090 | \n",
      "Batch: 460/782 |  Training Loss: 0.3180 | \n",
      "Batch: 470/782 |  Training Loss: 0.3095 | \n",
      "Batch: 480/782 |  Training Loss: 0.4453 | \n",
      "Batch: 490/782 |  Training Loss: 0.5456 | \n",
      "Batch: 500/782 |  Training Loss: 0.3864 | \n",
      "Batch: 510/782 |  Training Loss: 0.3298 | \n",
      "Batch: 520/782 |  Training Loss: 0.1865 | \n",
      "Batch: 530/782 |  Training Loss: 0.4994 | \n",
      "Batch: 540/782 |  Training Loss: 0.3005 | \n",
      "Batch: 550/782 |  Training Loss: 0.2975 | \n",
      "Batch: 560/782 |  Training Loss: 0.2599 | \n",
      "Batch: 570/782 |  Training Loss: 0.5589 | \n",
      "Batch: 580/782 |  Training Loss: 0.2543 | \n",
      "Batch: 590/782 |  Training Loss: 0.4860 | \n",
      "Batch: 600/782 |  Training Loss: 0.2868 | \n",
      "Batch: 610/782 |  Training Loss: 0.3880 | \n",
      "Batch: 620/782 |  Training Loss: 0.4496 | \n",
      "Batch: 630/782 |  Training Loss: 0.3674 | \n",
      "Batch: 640/782 |  Training Loss: 0.4422 | \n",
      "Batch: 650/782 |  Training Loss: 0.3352 | \n",
      "Batch: 660/782 |  Training Loss: 0.4748 | \n",
      "Batch: 670/782 |  Training Loss: 0.4409 | \n",
      "Batch: 680/782 |  Training Loss: 0.4110 | \n",
      "Batch: 690/782 |  Training Loss: 0.4703 | \n",
      "Batch: 700/782 |  Training Loss: 0.4413 | \n",
      "Batch: 710/782 |  Training Loss: 0.4113 | \n",
      "Batch: 720/782 |  Training Loss: 0.4173 | \n",
      "Batch: 730/782 |  Training Loss: 0.4401 | \n",
      "Batch: 740/782 |  Training Loss: 0.5840 | \n",
      "Batch: 750/782 |  Training Loss: 0.2518 | \n",
      "Batch: 760/782 |  Training Loss: 0.3360 | \n",
      "Batch: 770/782 |  Training Loss: 0.2188 | \n",
      "Batch: 780/782 |  Training Loss: 0.4368 | \n",
      "Test Loss: 0.5919, Accuracy: 81.60%\n",
      "Final Training Loss: 0.3409, Final Test Loss: 0.5919, Test Accuracy: 81.60%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nRun 1 Results:\\nTest Loss: 0.6369, Accuracy: 80.62%\\nFinal Training Loss: 0.2981, Final Test Loss: 0.6369, Test Accuracy: 80.62%\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_test(model, train_dataloader, test_dataloader, optimizer, criterion, num_epochs, device):\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # train the model\n",
    "        train_loss = training(model, train_dataloader, optimizer, criterion, epoch, device)\n",
    "\n",
    "    # Test the model\n",
    "    test_loss, test_accuracy = testing(model, test_dataloader, criterion, device)\n",
    "\n",
    "    return train_loss, test_loss, test_accuracy\n",
    "\n",
    "\n",
    "# init params\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "\n",
    "# init model\n",
    "model = ConvNeuralNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# load cifar-10 dataloaders\n",
    "\n",
    "# trainloader\n",
    "cifar_trainloader = DataLoader(\n",
    "    cifar10_training_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# testloader\n",
    "cifar_testloader = DataLoader(\n",
    "    cifar10_testing_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Initialize your model, criterion, optimizer, etc.\n",
    "model = ConvNeuralNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Call the train_test function\n",
    "train_loss, test_loss, test_accuracy = train_test(\n",
    "    model, \n",
    "    cifar_trainloader, \n",
    "    cifar_testloader, \n",
    "    optimizer, \n",
    "    criterion, \n",
    "    epochs, \n",
    "    device\n",
    ")\n",
    "\n",
    "# Print final results\n",
    "print(f\"Final Training Loss: {train_loss:.4f}, Final Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 1 Results:<br>\n",
    "Final Training Loss: 0.2981, Final Test Loss: 0.6369, Test Accuracy: 80.62%\n",
    "\n",
    "Run 2 Results:<br>\n",
    "Final Training Loss: 0.3409, Final Test Loss: 0.5919, Test Accuracy: 81.60%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
