{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M7 Bonus \n",
    "#### Training a Convolutional Neural Net on the CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# configuring machine to utilize GPU with cuda\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# loading cifar-10 training and testing with transforming it\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "# loading cifar-10 training data\n",
    "cifar10_training_data = datasets.CIFAR10(\n",
    "    './data/cifar10', \n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# loading cifar-10 test data\n",
    "cifar10_testing_data = datasets.CIFAR10(\n",
    "    './data/cifar10', \n",
    "    train=False, \n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining cnn model\n",
    "\n",
    "class ConvNeuralNet(nn.Module):\n",
    "    \"\"\"    \n",
    "    Convolutional Neural Network for classifying CIFAR-10 dataset.\n",
    "\n",
    "    Attributes:\n",
    "        convolution_layer (nn.Sequential): A sequential container of convolutional layers\n",
    "            that includes convolution, batch normalization, ReLU activations, and pooling.\n",
    "        fullyconnected_layer (nn.Sequential): A sequential container of fully connected\n",
    "            layers with dropout for regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolution_layer = nn.Sequential(\n",
    "            # first block : (input: 3 channels, output: 32 channels)\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # second block : (input: 64 channels, output: 128 channels)\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.05),\n",
    "\n",
    "            # third block : (input: 128 channels, output: 256 channels)\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.fullyconnected_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(4096, 1024),  # (input: 4096 features, output: 1024 features)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),   # (input: 1024 features, output: 512 features)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 10)  # final layer (input: 512 features, output: 10 classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # convolution\n",
    "        x = self.convolution_layer(x)\n",
    "\n",
    "        # flatten view\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # fully connected layer\n",
    "        x = self.fullyconnected_layer(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# model = ConvNeuralNet()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def training(model, train_dataloader, optimizer, criterion, epoch, device, print_freq=10):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    for batch_idx, (data, labels) in enumerate(train_dataloader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # propagation\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print stats\n",
    "        training_loss += loss.item() * data.shape[0]\n",
    "\n",
    "\n",
    "        if not (batch_idx % print_freq):\n",
    "            print(f\"Batch: {batch_idx}/{len(train_dataloader)} | \", \n",
    "                  f\"Training Loss: {loss.item():.4f} | \"\n",
    "            )\n",
    "    return training_loss / len(train_dataloader.dataset)\n",
    "\n",
    "# testing function\n",
    "def testing(model, test_dataloader, criterion, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            # move data and target to the specified device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # forward pass \n",
    "            output = model(data)\n",
    "\n",
    "            # calculate loss\n",
    "            test_loss += criterion(output, target).item() * data.size(0)\n",
    "\n",
    "            # get predictions\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    # average loss\n",
    "    average_loss = test_loss / total\n",
    "    # calculate accuracy\n",
    "    accuracy = correct / total * 100\n",
    "\n",
    "    print(f'Test Loss: {average_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    return average_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Batch: 0/391 |  Training Loss: 2.3016 | \n",
      "Batch: 10/391 |  Training Loss: 2.0081 | \n",
      "Batch: 20/391 |  Training Loss: 1.9946 | \n",
      "Batch: 30/391 |  Training Loss: 1.7033 | \n",
      "Batch: 40/391 |  Training Loss: 1.8636 | \n",
      "Batch: 50/391 |  Training Loss: 1.6799 | \n",
      "Batch: 60/391 |  Training Loss: 1.5870 | \n",
      "Batch: 70/391 |  Training Loss: 1.6619 | \n",
      "Batch: 80/391 |  Training Loss: 1.5527 | \n",
      "Batch: 90/391 |  Training Loss: 1.3482 | \n",
      "Batch: 100/391 |  Training Loss: 1.5115 | \n",
      "Batch: 110/391 |  Training Loss: 1.5210 | \n",
      "Batch: 120/391 |  Training Loss: 1.3608 | \n",
      "Batch: 130/391 |  Training Loss: 1.4018 | \n",
      "Batch: 140/391 |  Training Loss: 1.3567 | \n",
      "Batch: 150/391 |  Training Loss: 1.4488 | \n",
      "Batch: 160/391 |  Training Loss: 1.3592 | \n",
      "Batch: 170/391 |  Training Loss: 1.3173 | \n",
      "Batch: 180/391 |  Training Loss: 1.2713 | \n",
      "Batch: 190/391 |  Training Loss: 1.3970 | \n",
      "Batch: 200/391 |  Training Loss: 1.1959 | \n",
      "Batch: 210/391 |  Training Loss: 1.3212 | \n",
      "Batch: 220/391 |  Training Loss: 1.4122 | \n",
      "Batch: 230/391 |  Training Loss: 1.1373 | \n",
      "Batch: 240/391 |  Training Loss: 1.2044 | \n",
      "Batch: 250/391 |  Training Loss: 1.2013 | \n",
      "Batch: 260/391 |  Training Loss: 1.2190 | \n",
      "Batch: 270/391 |  Training Loss: 1.1103 | \n",
      "Batch: 280/391 |  Training Loss: 1.2796 | \n",
      "Batch: 290/391 |  Training Loss: 1.1040 | \n",
      "Batch: 300/391 |  Training Loss: 1.1675 | \n",
      "Batch: 310/391 |  Training Loss: 1.1457 | \n",
      "Batch: 320/391 |  Training Loss: 1.1327 | \n",
      "Batch: 330/391 |  Training Loss: 1.0533 | \n",
      "Batch: 340/391 |  Training Loss: 1.1846 | \n",
      "Batch: 350/391 |  Training Loss: 1.1519 | \n",
      "Batch: 360/391 |  Training Loss: 1.0531 | \n",
      "Batch: 370/391 |  Training Loss: 1.0184 | \n",
      "Batch: 380/391 |  Training Loss: 1.2173 | \n",
      "Batch: 390/391 |  Training Loss: 1.1829 | \n",
      "Epoch: 2\n",
      "Batch: 0/391 |  Training Loss: 0.9784 | \n",
      "Batch: 10/391 |  Training Loss: 1.0469 | \n",
      "Batch: 20/391 |  Training Loss: 1.1830 | \n",
      "Batch: 30/391 |  Training Loss: 1.0391 | \n",
      "Batch: 40/391 |  Training Loss: 0.8738 | \n",
      "Batch: 50/391 |  Training Loss: 0.9985 | \n",
      "Batch: 60/391 |  Training Loss: 1.1613 | \n",
      "Batch: 70/391 |  Training Loss: 0.9572 | \n",
      "Batch: 80/391 |  Training Loss: 0.9361 | \n",
      "Batch: 90/391 |  Training Loss: 0.8939 | \n",
      "Batch: 100/391 |  Training Loss: 1.0849 | \n",
      "Batch: 110/391 |  Training Loss: 1.1143 | \n",
      "Batch: 120/391 |  Training Loss: 0.8764 | \n",
      "Batch: 130/391 |  Training Loss: 1.0858 | \n",
      "Batch: 140/391 |  Training Loss: 0.9862 | \n",
      "Batch: 150/391 |  Training Loss: 0.8319 | \n",
      "Batch: 160/391 |  Training Loss: 0.8374 | \n",
      "Batch: 170/391 |  Training Loss: 0.8912 | \n",
      "Batch: 180/391 |  Training Loss: 0.7813 | \n",
      "Batch: 190/391 |  Training Loss: 0.9253 | \n",
      "Batch: 200/391 |  Training Loss: 0.8272 | \n",
      "Batch: 210/391 |  Training Loss: 0.7526 | \n",
      "Batch: 220/391 |  Training Loss: 0.9817 | \n",
      "Batch: 230/391 |  Training Loss: 1.0378 | \n",
      "Batch: 240/391 |  Training Loss: 0.8508 | \n",
      "Batch: 250/391 |  Training Loss: 1.0132 | \n",
      "Batch: 260/391 |  Training Loss: 0.8993 | \n",
      "Batch: 270/391 |  Training Loss: 0.7595 | \n",
      "Batch: 280/391 |  Training Loss: 0.9277 | \n",
      "Batch: 290/391 |  Training Loss: 0.9308 | \n",
      "Batch: 300/391 |  Training Loss: 0.9675 | \n",
      "Batch: 310/391 |  Training Loss: 0.6919 | \n",
      "Batch: 320/391 |  Training Loss: 0.7457 | \n",
      "Batch: 330/391 |  Training Loss: 0.8846 | \n",
      "Batch: 340/391 |  Training Loss: 0.7006 | \n",
      "Batch: 350/391 |  Training Loss: 0.9192 | \n",
      "Batch: 360/391 |  Training Loss: 0.8147 | \n",
      "Batch: 370/391 |  Training Loss: 0.8633 | \n",
      "Batch: 380/391 |  Training Loss: 0.6814 | \n",
      "Batch: 390/391 |  Training Loss: 0.8099 | \n",
      "Epoch: 3\n",
      "Batch: 0/391 |  Training Loss: 0.8398 | \n",
      "Batch: 10/391 |  Training Loss: 0.6588 | \n",
      "Batch: 20/391 |  Training Loss: 0.8907 | \n",
      "Batch: 30/391 |  Training Loss: 0.8654 | \n",
      "Batch: 40/391 |  Training Loss: 0.7531 | \n",
      "Batch: 50/391 |  Training Loss: 0.6913 | \n",
      "Batch: 60/391 |  Training Loss: 0.6055 | \n",
      "Batch: 70/391 |  Training Loss: 0.7901 | \n",
      "Batch: 80/391 |  Training Loss: 0.9048 | \n",
      "Batch: 90/391 |  Training Loss: 0.5995 | \n",
      "Batch: 100/391 |  Training Loss: 0.9613 | \n",
      "Batch: 110/391 |  Training Loss: 0.6175 | \n",
      "Batch: 120/391 |  Training Loss: 0.6497 | \n",
      "Batch: 130/391 |  Training Loss: 0.7435 | \n",
      "Batch: 140/391 |  Training Loss: 0.6322 | \n",
      "Batch: 150/391 |  Training Loss: 0.8058 | \n",
      "Batch: 160/391 |  Training Loss: 0.8256 | \n",
      "Batch: 170/391 |  Training Loss: 0.7377 | \n",
      "Batch: 180/391 |  Training Loss: 0.7251 | \n",
      "Batch: 190/391 |  Training Loss: 0.6604 | \n",
      "Batch: 200/391 |  Training Loss: 0.8174 | \n",
      "Batch: 210/391 |  Training Loss: 0.6685 | \n",
      "Batch: 220/391 |  Training Loss: 0.9417 | \n",
      "Batch: 230/391 |  Training Loss: 0.6057 | \n",
      "Batch: 240/391 |  Training Loss: 0.7511 | \n",
      "Batch: 250/391 |  Training Loss: 0.7048 | \n",
      "Batch: 260/391 |  Training Loss: 0.6831 | \n",
      "Batch: 270/391 |  Training Loss: 0.5997 | \n",
      "Batch: 280/391 |  Training Loss: 0.7764 | \n",
      "Batch: 290/391 |  Training Loss: 0.5914 | \n",
      "Batch: 300/391 |  Training Loss: 0.6085 | \n",
      "Batch: 310/391 |  Training Loss: 0.5803 | \n",
      "Batch: 320/391 |  Training Loss: 0.7562 | \n",
      "Batch: 330/391 |  Training Loss: 0.4689 | \n",
      "Batch: 340/391 |  Training Loss: 0.7289 | \n",
      "Batch: 350/391 |  Training Loss: 0.7834 | \n",
      "Batch: 360/391 |  Training Loss: 0.7610 | \n",
      "Batch: 370/391 |  Training Loss: 0.6200 | \n",
      "Batch: 380/391 |  Training Loss: 0.6660 | \n",
      "Batch: 390/391 |  Training Loss: 0.5000 | \n",
      "Epoch: 4\n",
      "Batch: 0/391 |  Training Loss: 0.5037 | \n",
      "Batch: 10/391 |  Training Loss: 0.4821 | \n",
      "Batch: 20/391 |  Training Loss: 0.4755 | \n",
      "Batch: 30/391 |  Training Loss: 0.5184 | \n",
      "Batch: 40/391 |  Training Loss: 0.5802 | \n",
      "Batch: 50/391 |  Training Loss: 0.5012 | \n",
      "Batch: 60/391 |  Training Loss: 0.6774 | \n",
      "Batch: 70/391 |  Training Loss: 0.4897 | \n",
      "Batch: 80/391 |  Training Loss: 0.5929 | \n",
      "Batch: 90/391 |  Training Loss: 0.5124 | \n",
      "Batch: 100/391 |  Training Loss: 0.4356 | \n",
      "Batch: 110/391 |  Training Loss: 0.5740 | \n",
      "Batch: 120/391 |  Training Loss: 0.6748 | \n",
      "Batch: 130/391 |  Training Loss: 0.5798 | \n",
      "Batch: 140/391 |  Training Loss: 0.5914 | \n",
      "Batch: 150/391 |  Training Loss: 0.5578 | \n",
      "Batch: 160/391 |  Training Loss: 0.5298 | \n",
      "Batch: 170/391 |  Training Loss: 0.6477 | \n",
      "Batch: 180/391 |  Training Loss: 0.5141 | \n",
      "Batch: 190/391 |  Training Loss: 0.8118 | \n",
      "Batch: 200/391 |  Training Loss: 0.5718 | \n",
      "Batch: 210/391 |  Training Loss: 0.6024 | \n",
      "Batch: 220/391 |  Training Loss: 0.5617 | \n",
      "Batch: 230/391 |  Training Loss: 0.5953 | \n",
      "Batch: 240/391 |  Training Loss: 0.6084 | \n",
      "Batch: 250/391 |  Training Loss: 0.5136 | \n",
      "Batch: 260/391 |  Training Loss: 0.7152 | \n",
      "Batch: 270/391 |  Training Loss: 0.5556 | \n",
      "Batch: 280/391 |  Training Loss: 0.5512 | \n",
      "Batch: 290/391 |  Training Loss: 0.5519 | \n",
      "Batch: 300/391 |  Training Loss: 0.4452 | \n",
      "Batch: 310/391 |  Training Loss: 0.5737 | \n",
      "Batch: 320/391 |  Training Loss: 0.5278 | \n",
      "Batch: 330/391 |  Training Loss: 0.4785 | \n",
      "Batch: 340/391 |  Training Loss: 0.6424 | \n",
      "Batch: 350/391 |  Training Loss: 0.6183 | \n",
      "Batch: 360/391 |  Training Loss: 0.5818 | \n",
      "Batch: 370/391 |  Training Loss: 0.6118 | \n",
      "Batch: 380/391 |  Training Loss: 0.5055 | \n",
      "Batch: 390/391 |  Training Loss: 0.5772 | \n",
      "Epoch: 5\n",
      "Batch: 0/391 |  Training Loss: 0.4969 | \n",
      "Batch: 10/391 |  Training Loss: 0.7228 | \n",
      "Batch: 20/391 |  Training Loss: 0.4185 | \n",
      "Batch: 30/391 |  Training Loss: 0.4712 | \n",
      "Batch: 40/391 |  Training Loss: 0.3980 | \n",
      "Batch: 50/391 |  Training Loss: 0.4927 | \n",
      "Batch: 60/391 |  Training Loss: 0.5366 | \n",
      "Batch: 70/391 |  Training Loss: 0.4092 | \n",
      "Batch: 80/391 |  Training Loss: 0.7661 | \n",
      "Batch: 90/391 |  Training Loss: 0.5071 | \n",
      "Batch: 100/391 |  Training Loss: 0.4137 | \n",
      "Batch: 110/391 |  Training Loss: 0.4549 | \n",
      "Batch: 120/391 |  Training Loss: 0.6075 | \n",
      "Batch: 130/391 |  Training Loss: 0.4991 | \n",
      "Batch: 140/391 |  Training Loss: 0.3896 | \n",
      "Batch: 150/391 |  Training Loss: 0.4327 | \n",
      "Batch: 160/391 |  Training Loss: 0.5119 | \n",
      "Batch: 170/391 |  Training Loss: 0.5449 | \n",
      "Batch: 180/391 |  Training Loss: 0.6642 | \n",
      "Batch: 190/391 |  Training Loss: 0.5245 | \n",
      "Batch: 200/391 |  Training Loss: 0.5920 | \n",
      "Batch: 210/391 |  Training Loss: 0.4876 | \n",
      "Batch: 220/391 |  Training Loss: 0.3694 | \n",
      "Batch: 230/391 |  Training Loss: 0.4424 | \n",
      "Batch: 240/391 |  Training Loss: 0.4264 | \n",
      "Batch: 250/391 |  Training Loss: 0.4453 | \n",
      "Batch: 260/391 |  Training Loss: 0.3697 | \n",
      "Batch: 270/391 |  Training Loss: 0.4177 | \n",
      "Batch: 280/391 |  Training Loss: 0.4348 | \n",
      "Batch: 290/391 |  Training Loss: 0.6347 | \n",
      "Batch: 300/391 |  Training Loss: 0.4785 | \n",
      "Batch: 310/391 |  Training Loss: 0.6014 | \n",
      "Batch: 320/391 |  Training Loss: 0.6189 | \n",
      "Batch: 330/391 |  Training Loss: 0.4852 | \n",
      "Batch: 340/391 |  Training Loss: 0.4971 | \n",
      "Batch: 350/391 |  Training Loss: 0.4580 | \n",
      "Batch: 360/391 |  Training Loss: 0.6497 | \n",
      "Batch: 370/391 |  Training Loss: 0.4697 | \n",
      "Batch: 380/391 |  Training Loss: 0.4708 | \n",
      "Batch: 390/391 |  Training Loss: 0.5427 | \n",
      "Epoch: 6\n",
      "Batch: 0/391 |  Training Loss: 0.5954 | \n",
      "Batch: 10/391 |  Training Loss: 0.4970 | \n",
      "Batch: 20/391 |  Training Loss: 0.4105 | \n",
      "Batch: 30/391 |  Training Loss: 0.2560 | \n",
      "Batch: 40/391 |  Training Loss: 0.2963 | \n",
      "Batch: 50/391 |  Training Loss: 0.4650 | \n",
      "Batch: 60/391 |  Training Loss: 0.3965 | \n",
      "Batch: 70/391 |  Training Loss: 0.2622 | \n",
      "Batch: 80/391 |  Training Loss: 0.4645 | \n",
      "Batch: 90/391 |  Training Loss: 0.4003 | \n",
      "Batch: 100/391 |  Training Loss: 0.2763 | \n",
      "Batch: 110/391 |  Training Loss: 0.4169 | \n",
      "Batch: 120/391 |  Training Loss: 0.3406 | \n",
      "Batch: 130/391 |  Training Loss: 0.2736 | \n",
      "Batch: 140/391 |  Training Loss: 0.2869 | \n",
      "Batch: 150/391 |  Training Loss: 0.5171 | \n",
      "Batch: 160/391 |  Training Loss: 0.4827 | \n",
      "Batch: 170/391 |  Training Loss: 0.4595 | \n",
      "Batch: 180/391 |  Training Loss: 0.2817 | \n",
      "Batch: 190/391 |  Training Loss: 0.4712 | \n",
      "Batch: 200/391 |  Training Loss: 0.5206 | \n",
      "Batch: 210/391 |  Training Loss: 0.3921 | \n",
      "Batch: 220/391 |  Training Loss: 0.4211 | \n",
      "Batch: 230/391 |  Training Loss: 0.2490 | \n",
      "Batch: 240/391 |  Training Loss: 0.2427 | \n",
      "Batch: 250/391 |  Training Loss: 0.3994 | \n",
      "Batch: 260/391 |  Training Loss: 0.3406 | \n",
      "Batch: 270/391 |  Training Loss: 0.4379 | \n",
      "Batch: 280/391 |  Training Loss: 0.3272 | \n",
      "Batch: 290/391 |  Training Loss: 0.4517 | \n",
      "Batch: 300/391 |  Training Loss: 0.4329 | \n",
      "Batch: 310/391 |  Training Loss: 0.4109 | \n",
      "Batch: 320/391 |  Training Loss: 0.4177 | \n",
      "Batch: 330/391 |  Training Loss: 0.2986 | \n",
      "Batch: 340/391 |  Training Loss: 0.3988 | \n",
      "Batch: 350/391 |  Training Loss: 0.3828 | \n",
      "Batch: 360/391 |  Training Loss: 0.4131 | \n",
      "Batch: 370/391 |  Training Loss: 0.3450 | \n",
      "Batch: 380/391 |  Training Loss: 0.5706 | \n",
      "Batch: 390/391 |  Training Loss: 0.4188 | \n",
      "Epoch: 7\n",
      "Batch: 0/391 |  Training Loss: 0.3541 | \n",
      "Batch: 10/391 |  Training Loss: 0.4253 | \n",
      "Batch: 20/391 |  Training Loss: 0.2302 | \n",
      "Batch: 30/391 |  Training Loss: 0.2447 | \n",
      "Batch: 40/391 |  Training Loss: 0.2684 | \n",
      "Batch: 50/391 |  Training Loss: 0.2901 | \n",
      "Batch: 60/391 |  Training Loss: 0.3096 | \n",
      "Batch: 70/391 |  Training Loss: 0.4158 | \n",
      "Batch: 80/391 |  Training Loss: 0.3289 | \n",
      "Batch: 90/391 |  Training Loss: 0.3708 | \n",
      "Batch: 100/391 |  Training Loss: 0.3534 | \n",
      "Batch: 110/391 |  Training Loss: 0.4616 | \n",
      "Batch: 120/391 |  Training Loss: 0.2995 | \n",
      "Batch: 130/391 |  Training Loss: 0.3207 | \n",
      "Batch: 140/391 |  Training Loss: 0.2907 | \n",
      "Batch: 150/391 |  Training Loss: 0.3432 | \n",
      "Batch: 160/391 |  Training Loss: 0.2854 | \n",
      "Batch: 170/391 |  Training Loss: 0.3744 | \n",
      "Batch: 180/391 |  Training Loss: 0.3288 | \n",
      "Batch: 190/391 |  Training Loss: 0.2596 | \n",
      "Batch: 200/391 |  Training Loss: 0.3550 | \n",
      "Batch: 210/391 |  Training Loss: 0.4000 | \n",
      "Batch: 220/391 |  Training Loss: 0.4371 | \n",
      "Batch: 230/391 |  Training Loss: 0.3381 | \n",
      "Batch: 240/391 |  Training Loss: 0.3594 | \n",
      "Batch: 250/391 |  Training Loss: 0.4020 | \n",
      "Batch: 260/391 |  Training Loss: 0.4811 | \n",
      "Batch: 270/391 |  Training Loss: 0.4097 | \n",
      "Batch: 280/391 |  Training Loss: 0.3068 | \n",
      "Batch: 290/391 |  Training Loss: 0.3783 | \n",
      "Batch: 300/391 |  Training Loss: 0.2416 | \n",
      "Batch: 310/391 |  Training Loss: 0.3262 | \n",
      "Batch: 320/391 |  Training Loss: 0.2829 | \n",
      "Batch: 330/391 |  Training Loss: 0.3268 | \n",
      "Batch: 340/391 |  Training Loss: 0.5111 | \n",
      "Batch: 350/391 |  Training Loss: 0.4296 | \n",
      "Batch: 360/391 |  Training Loss: 0.4788 | \n",
      "Batch: 370/391 |  Training Loss: 0.3588 | \n",
      "Batch: 380/391 |  Training Loss: 0.3163 | \n",
      "Batch: 390/391 |  Training Loss: 0.3560 | \n",
      "Epoch: 8\n",
      "Batch: 0/391 |  Training Loss: 0.2480 | \n",
      "Batch: 10/391 |  Training Loss: 0.2072 | \n",
      "Batch: 20/391 |  Training Loss: 0.3064 | \n",
      "Batch: 30/391 |  Training Loss: 0.2898 | \n",
      "Batch: 40/391 |  Training Loss: 0.2426 | \n",
      "Batch: 50/391 |  Training Loss: 0.2250 | \n",
      "Batch: 60/391 |  Training Loss: 0.2510 | \n",
      "Batch: 70/391 |  Training Loss: 0.2346 | \n",
      "Batch: 80/391 |  Training Loss: 0.2265 | \n",
      "Batch: 90/391 |  Training Loss: 0.2294 | \n",
      "Batch: 100/391 |  Training Loss: 0.2330 | \n",
      "Batch: 110/391 |  Training Loss: 0.2168 | \n",
      "Batch: 120/391 |  Training Loss: 0.1819 | \n",
      "Batch: 130/391 |  Training Loss: 0.3248 | \n",
      "Batch: 140/391 |  Training Loss: 0.2508 | \n",
      "Batch: 150/391 |  Training Loss: 0.3233 | \n",
      "Batch: 160/391 |  Training Loss: 0.1744 | \n",
      "Batch: 170/391 |  Training Loss: 0.2652 | \n",
      "Batch: 180/391 |  Training Loss: 0.1796 | \n",
      "Batch: 190/391 |  Training Loss: 0.2160 | \n",
      "Batch: 200/391 |  Training Loss: 0.2593 | \n",
      "Batch: 210/391 |  Training Loss: 0.2087 | \n",
      "Batch: 220/391 |  Training Loss: 0.2764 | \n",
      "Batch: 230/391 |  Training Loss: 0.2645 | \n",
      "Batch: 240/391 |  Training Loss: 0.2756 | \n",
      "Batch: 250/391 |  Training Loss: 0.2632 | \n",
      "Batch: 260/391 |  Training Loss: 0.3735 | \n",
      "Batch: 270/391 |  Training Loss: 0.3956 | \n",
      "Batch: 280/391 |  Training Loss: 0.2404 | \n",
      "Batch: 290/391 |  Training Loss: 0.2411 | \n",
      "Batch: 300/391 |  Training Loss: 0.2824 | \n",
      "Batch: 310/391 |  Training Loss: 0.2652 | \n",
      "Batch: 320/391 |  Training Loss: 0.3076 | \n",
      "Batch: 330/391 |  Training Loss: 0.4106 | \n",
      "Batch: 340/391 |  Training Loss: 0.2496 | \n",
      "Batch: 350/391 |  Training Loss: 0.3474 | \n",
      "Batch: 360/391 |  Training Loss: 0.4542 | \n",
      "Batch: 370/391 |  Training Loss: 0.2995 | \n",
      "Batch: 380/391 |  Training Loss: 0.2119 | \n",
      "Batch: 390/391 |  Training Loss: 0.4732 | \n",
      "Epoch: 9\n",
      "Batch: 0/391 |  Training Loss: 0.1425 | \n",
      "Batch: 10/391 |  Training Loss: 0.2740 | \n",
      "Batch: 20/391 |  Training Loss: 0.2390 | \n",
      "Batch: 30/391 |  Training Loss: 0.1625 | \n",
      "Batch: 40/391 |  Training Loss: 0.2113 | \n",
      "Batch: 50/391 |  Training Loss: 0.1760 | \n",
      "Batch: 60/391 |  Training Loss: 0.1801 | \n",
      "Batch: 70/391 |  Training Loss: 0.1587 | \n",
      "Batch: 80/391 |  Training Loss: 0.1628 | \n",
      "Batch: 90/391 |  Training Loss: 0.3194 | \n",
      "Batch: 100/391 |  Training Loss: 0.2229 | \n",
      "Batch: 110/391 |  Training Loss: 0.1867 | \n",
      "Batch: 120/391 |  Training Loss: 0.2786 | \n",
      "Batch: 130/391 |  Training Loss: 0.3353 | \n",
      "Batch: 140/391 |  Training Loss: 0.3291 | \n",
      "Batch: 150/391 |  Training Loss: 0.2700 | \n",
      "Batch: 160/391 |  Training Loss: 0.2174 | \n",
      "Batch: 170/391 |  Training Loss: 0.3625 | \n",
      "Batch: 180/391 |  Training Loss: 0.3160 | \n",
      "Batch: 190/391 |  Training Loss: 0.2684 | \n",
      "Batch: 200/391 |  Training Loss: 0.1425 | \n",
      "Batch: 210/391 |  Training Loss: 0.2209 | \n",
      "Batch: 220/391 |  Training Loss: 0.1502 | \n",
      "Batch: 230/391 |  Training Loss: 0.2475 | \n",
      "Batch: 240/391 |  Training Loss: 0.3332 | \n",
      "Batch: 250/391 |  Training Loss: 0.2587 | \n",
      "Batch: 260/391 |  Training Loss: 0.2848 | \n",
      "Batch: 270/391 |  Training Loss: 0.2139 | \n",
      "Batch: 280/391 |  Training Loss: 0.3541 | \n",
      "Batch: 290/391 |  Training Loss: 0.2128 | \n",
      "Batch: 300/391 |  Training Loss: 0.1863 | \n",
      "Batch: 310/391 |  Training Loss: 0.3383 | \n",
      "Batch: 320/391 |  Training Loss: 0.1497 | \n",
      "Batch: 330/391 |  Training Loss: 0.2127 | \n",
      "Batch: 340/391 |  Training Loss: 0.2141 | \n",
      "Batch: 350/391 |  Training Loss: 0.2135 | \n",
      "Batch: 360/391 |  Training Loss: 0.1814 | \n",
      "Batch: 370/391 |  Training Loss: 0.2717 | \n",
      "Batch: 380/391 |  Training Loss: 0.3138 | \n",
      "Batch: 390/391 |  Training Loss: 0.2790 | \n",
      "Epoch: 10\n",
      "Batch: 0/391 |  Training Loss: 0.1529 | \n",
      "Batch: 10/391 |  Training Loss: 0.1976 | \n",
      "Batch: 20/391 |  Training Loss: 0.1440 | \n",
      "Batch: 30/391 |  Training Loss: 0.1996 | \n",
      "Batch: 40/391 |  Training Loss: 0.1908 | \n",
      "Batch: 50/391 |  Training Loss: 0.1109 | \n",
      "Batch: 60/391 |  Training Loss: 0.0788 | \n",
      "Batch: 70/391 |  Training Loss: 0.1615 | \n",
      "Batch: 80/391 |  Training Loss: 0.1141 | \n",
      "Batch: 90/391 |  Training Loss: 0.2840 | \n",
      "Batch: 100/391 |  Training Loss: 0.1851 | \n",
      "Batch: 110/391 |  Training Loss: 0.2455 | \n",
      "Batch: 120/391 |  Training Loss: 0.1347 | \n",
      "Batch: 130/391 |  Training Loss: 0.2750 | \n",
      "Batch: 140/391 |  Training Loss: 0.2012 | \n",
      "Batch: 150/391 |  Training Loss: 0.1925 | \n",
      "Batch: 160/391 |  Training Loss: 0.1725 | \n",
      "Batch: 170/391 |  Training Loss: 0.1189 | \n",
      "Batch: 180/391 |  Training Loss: 0.1230 | \n",
      "Batch: 190/391 |  Training Loss: 0.2038 | \n",
      "Batch: 200/391 |  Training Loss: 0.1871 | \n",
      "Batch: 210/391 |  Training Loss: 0.1515 | \n",
      "Batch: 220/391 |  Training Loss: 0.1844 | \n",
      "Batch: 230/391 |  Training Loss: 0.2481 | \n",
      "Batch: 240/391 |  Training Loss: 0.1074 | \n",
      "Batch: 250/391 |  Training Loss: 0.1555 | \n",
      "Batch: 260/391 |  Training Loss: 0.3043 | \n",
      "Batch: 270/391 |  Training Loss: 0.2295 | \n",
      "Batch: 280/391 |  Training Loss: 0.0928 | \n",
      "Batch: 290/391 |  Training Loss: 0.1442 | \n",
      "Batch: 300/391 |  Training Loss: 0.1735 | \n",
      "Batch: 310/391 |  Training Loss: 0.2538 | \n",
      "Batch: 320/391 |  Training Loss: 0.2244 | \n",
      "Batch: 330/391 |  Training Loss: 0.3555 | \n",
      "Batch: 340/391 |  Training Loss: 0.2306 | \n",
      "Batch: 350/391 |  Training Loss: 0.1242 | \n",
      "Batch: 360/391 |  Training Loss: 0.2282 | \n",
      "Batch: 370/391 |  Training Loss: 0.2079 | \n",
      "Batch: 380/391 |  Training Loss: 0.2034 | \n",
      "Batch: 390/391 |  Training Loss: 0.1760 | \n",
      "Test Loss: 0.6487, Accuracy: 82.43%\n",
      "Final Training Loss: 0.1971, Final Test Loss: 0.6487, Test Accuracy: 82.43%\n"
     ]
    }
   ],
   "source": [
    "def train_test(model, train_dataloader, test_dataloader, optimizer, criterion, num_epochs, device):\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # train the model\n",
    "        train_loss = training(model, train_dataloader, optimizer, criterion, epoch, device)\n",
    "\n",
    "    # test the model\n",
    "    test_loss, test_accuracy = testing(model, test_dataloader, criterion, device)\n",
    "\n",
    "    return train_loss, test_loss, test_accuracy\n",
    "\n",
    "\n",
    "# init params\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "\n",
    "# init model\n",
    "model = ConvNeuralNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# load cifar-10 dataloaders\n",
    "\n",
    "# trainloader\n",
    "cifar_trainloader = DataLoader(\n",
    "    cifar10_training_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# testloader\n",
    "cifar_testloader = DataLoader(\n",
    "    cifar10_testing_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# init model, optimizer, criterion\n",
    "model = ConvNeuralNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# call train_test\n",
    "train_loss, test_loss, test_accuracy = train_test(\n",
    "    model, \n",
    "    cifar_trainloader, \n",
    "    cifar_testloader, \n",
    "    optimizer, \n",
    "    criterion, \n",
    "    epochs, \n",
    "    device\n",
    ")\n",
    "\n",
    "print(f\"Final Training Loss: {train_loss:.4f}, Final Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without re-implementing ResNet architectures and data augmentations:\n",
    "\n",
    "Run 1 Results:<br>\n",
    "(batch size = 64)<br>\n",
    "Final Training Loss: 0.2981, Final Test Loss: 0.6369, Test Accuracy: 80.62%\n",
    "\n",
    "Run 2 Results:<br>\n",
    "(batch size = 64)<br>\n",
    "Final Training Loss: 0.3409, Final Test Loss: 0.5919, Test Accuracy: 81.60%\n",
    "\n",
    "Run 3 Results:<br>\n",
    "(batch size = 128)<br>\n",
    "Final Training Loss: 0.2351, Final Test Loss: 0.6708, Test Accuracy: 81.02%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
